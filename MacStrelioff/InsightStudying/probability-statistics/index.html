<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.2.5">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Mac Strelioff">

  
  
  
    
  
  <meta name="description" content="Probability Foundations Probability Rules Probability Functions Transformations of Random Variables Expectation Variance Information Theory Causal Graphs  Causal Inference Framework Key Assumptions Common Methods Diff in Diff Causal Impact Synthetic Controls IV Analysis Regression Discontinuity Fixed Effects Regression First differences Random Effects Propensity Score Matching Microsoft DoWhy   Efficient Inference Efficient Sampling Efficient Designs Efficient Models  Experiment Design Foundations Concerns Treatment Units Generative Model and Adjustment Variables Common Designs Assignment Mechanism Duration Integrety Checks Maximum Likelihood versus Bayes’ Estimators  GLMs Common Tests t-test ANOVA Linear Model Z test for proportions Chi-Square Logistic Regression Likelihood Ratio and Bayes Factors always valid p-values (ck optimizely white paper)  Different Estimands For CI more on variance Emperical Variance Estimation Mixed Effects Models Effiencicy or Variance Reduction GLMs  Causal Inference Methods Difference in Differences Causal Impact Synthetic Controls Propensity Score Matching Fixed Effects Regression Instrumental variables Regression Discontinuity    Probability Foundations A random variable random variable is a variable with an unknown value, but known possible values.">

  
  <link rel="alternate" hreflang="en-us" href="/MacStrelioff/insightstudying/probability-statistics/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="rgba(0,130,160,1)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/MacStrelioff/css/academic.min.9c3a903cc870878595d69f08d98aa322.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-140153670-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/MacStrelioff/site.webmanifest">
  <link rel="icon" type="image/png" href="/MacStrelioff/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/MacStrelioff/img/icon-192.png">

  <link rel="canonical" href="/MacStrelioff/insightstudying/probability-statistics/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@macstrelioff">
  <meta property="twitter:creator" content="@macstrelioff">
  
  <meta property="og:site_name" content="Mac Strelioff">
  <meta property="og:url" content="/MacStrelioff/insightstudying/probability-statistics/">
  <meta property="og:title" content="Probability and Statistics | Mac Strelioff">
  <meta property="og:description" content="Probability Foundations Probability Rules Probability Functions Transformations of Random Variables Expectation Variance Information Theory Causal Graphs  Causal Inference Framework Key Assumptions Common Methods Diff in Diff Causal Impact Synthetic Controls IV Analysis Regression Discontinuity Fixed Effects Regression First differences Random Effects Propensity Score Matching Microsoft DoWhy   Efficient Inference Efficient Sampling Efficient Designs Efficient Models  Experiment Design Foundations Concerns Treatment Units Generative Model and Adjustment Variables Common Designs Assignment Mechanism Duration Integrety Checks Maximum Likelihood versus Bayes’ Estimators  GLMs Common Tests t-test ANOVA Linear Model Z test for proportions Chi-Square Logistic Regression Likelihood Ratio and Bayes Factors always valid p-values (ck optimizely white paper)  Different Estimands For CI more on variance Emperical Variance Estimation Mixed Effects Models Effiencicy or Variance Reduction GLMs  Causal Inference Methods Difference in Differences Causal Impact Synthetic Controls Propensity Score Matching Fixed Effects Regression Instrumental variables Regression Discontinuity    Probability Foundations A random variable random variable is a variable with an unknown value, but known possible values."><meta property="og:image" content="/MacStrelioff/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-10-17T17:35:02&#43;00:00">
  
  <meta property="article:modified_time" content="2019-10-17T17:35:02&#43;00:00">
  

  

  

  <title>Probability and Statistics | Mac Strelioff</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/MacStrelioff/">Mac Strelioff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/data-science/">
            
            <span>Data Science</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/video-lectures/">
            
            <span>Videos</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/cv/">
            
            <span>Resume</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Probability and Statistics</h1>

  

  
    



<meta content="2019-10-17 17:35:02 &#43;0000 UTC" itemprop="datePublished">
<meta content="2019-10-17 17:35:02 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Oct 17, 2019</time>
  </span>
  

  

  

  
  
  <span class="middot-divider"></span>
  <a href="/MacStrelioff/insightstudying/probability-statistics/#disqus_thread"></a>
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=&amp;url="
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u="
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=&amp;body=">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    
















  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<div id="TOC">
<ul>
<li><a href="#probability-foundations">Probability Foundations</a><ul>
<li><a href="#probability-rules">Probability Rules</a></li>
<li><a href="#probability-functions">Probability Functions</a></li>
<li><a href="#transformations-of-random-variables">Transformations of Random Variables</a></li>
<li><a href="#expectation">Expectation</a></li>
<li><a href="#variance">Variance</a></li>
<li><a href="#information-theory">Information Theory</a></li>
<li><a href="#causal-graphs">Causal Graphs</a></li>
</ul></li>
<li><a href="#causal-inference">Causal Inference</a><ul>
<li><a href="#framework">Framework</a></li>
<li><a href="#key-assumptions">Key Assumptions</a></li>
<li><a href="#common-methods">Common Methods</a><ul>
<li><a href="#diff-in-diff">Diff in Diff</a></li>
<li><a href="#causal-impact">Causal Impact</a></li>
<li><a href="#synthetic-controls">Synthetic Controls</a></li>
<li><a href="#iv-analysis">IV Analysis</a></li>
<li><a href="#regression-discontinuity">Regression Discontinuity</a></li>
<li><a href="#fixed-effects-regression">Fixed Effects Regression</a></li>
<li><a href="#first-differences">First differences</a></li>
<li><a href="#random-effects">Random Effects</a></li>
<li><a href="#propensity-score-matching">Propensity Score Matching</a></li>
<li><a href="#microsoft-dowhy">Microsoft DoWhy</a></li>
</ul></li>
</ul></li>
<li><a href="#efficient-inference">Efficient Inference</a><ul>
<li><a href="#efficient-sampling">Efficient Sampling</a></li>
<li><a href="#efficient-designs">Efficient Designs</a></li>
<li><a href="#efficient-models">Efficient Models</a></li>
</ul></li>
<li><a href="#experiment-design">Experiment Design</a><ul>
<li><a href="#foundations">Foundations</a></li>
<li><a href="#concerns">Concerns</a></li>
<li><a href="#treatment-units">Treatment Units</a></li>
<li><a href="#generative-model-and-adjustment-variables">Generative Model and Adjustment Variables</a></li>
<li><a href="#common-designs">Common Designs</a></li>
<li><a href="#assignment-mechanism">Assignment Mechanism</a></li>
<li><a href="#duration">Duration</a></li>
<li><a href="#integrety-checks">Integrety Checks</a></li>
<li><a href="#maximum-likelihood-versus-bayes-estimators">Maximum Likelihood versus Bayes’ Estimators</a></li>
</ul></li>
<li><a href="#glms">GLMs</a></li>
<li><a href="#common-tests">Common Tests</a><ul>
<li><a href="#t-test">t-test</a></li>
<li><a href="#anova">ANOVA</a></li>
<li><a href="#linear-model">Linear Model</a></li>
<li><a href="#z-test-for-proportions">Z test for proportions</a></li>
<li><a href="#chi-square">Chi-Square</a></li>
<li><a href="#logistic-regression">Logistic Regression</a></li>
<li><a href="#likelihood-ratio-and-bayes-factors">Likelihood Ratio and Bayes Factors</a></li>
<li><a href="#always-valid-p-values-ck-optimizely-white-paper">always valid p-values (ck optimizely white paper)</a></li>
</ul></li>
<li><a href="#different-estimands-for-ci">Different Estimands For CI</a></li>
<li><a href="#more-on-variance">more on variance</a><ul>
<li><a href="#emperical-variance-estimation">Emperical Variance Estimation</a></li>
<li><a href="#mixed-effects-models">Mixed Effects Models</a></li>
<li><a href="#effiencicy-or-variance-reduction">Effiencicy or Variance Reduction</a></li>
<li><a href="#glms-1">GLMs</a></li>
</ul></li>
<li><a href="#causal-inference-methods">Causal Inference Methods</a><ul>
<li><a href="#difference-in-differences">Difference in Differences</a></li>
<li><a href="#causal-impact-1">Causal Impact</a></li>
<li><a href="#synthetic-controls-1">Synthetic Controls</a></li>
<li><a href="#propensity-score-matching-1">Propensity Score Matching</a></li>
<li><a href="#fixed-effects-regression-1">Fixed Effects Regression</a></li>
<li><a href="#instrumental-variables">Instrumental variables</a></li>
<li><a href="#regression-discontinuity-1">Regression Discontinuity</a></li>
</ul></li>
</ul>
</div>

<!--
Fermi problems (order of magnitude)
-->
<!--
- Make YouTube videos on each!
-->
<!--
# What is a probability, and how does that relate to statistics? 

- Probabilities are commonly thought of in three ways; 

1. Purely mathematical
2. Magnitudes of belief
3. Relative frequencies of infinate events

I use probability theory to quantify beliefs and make decisions. 

Logic, about truths of the universe. 
By there is uncertainty in what we know, and uncertainty in our measurements and observations. Statistics as a field is concerned with specifying our knowledge of the laws of the universe and the uncertainty around them. They do this by applying probability theory. 

Probability theory generalizes logic to situations where we aren't certain. And logic emerges from probability theory when we know things with certainty. 

Hypothesis testing

Statisticians use probability theory to decide on what is and isn't true
-->
<div id="probability-foundations" class="section level1">
<h1>Probability Foundations</h1>
<!--
Events, event spaces, 
combinatorics (permutations and combinations)
-->
<p>A random variable random variable is a variable with an unknown value, but known possible values. An event is an observed value of a random variable. An event space contains all possible values of the random variable. Probabilities are values assigned to the events in an event space (i.e. the possible values of a random variable) and represent how likely each event is relative to other events in the event space.</p>
<p>Here <span class="math inline">\(p(e)\)</span> will be used to represent the probability of an event. Some special probabilities include the probability of any event in the event space, which is 1. And the probability of any event other than <span class="math inline">\(p(e)\)</span>, known as the compliment of <span class="math inline">\(p(e)\)</span>, denoted with <span class="math inline">\(p(\neg e)\)</span>, is found by;</p>
<p><span class="math display">\[
p(\neg e) = 1 - p(e)
\]</span></p>
<div id="probability-rules" class="section level2">
<h2>Probability Rules</h2>
<p>A joint event refers to two or more events occurring together. They are colloquially talked about as one event ‘and’ another event occurring together. More formally, joint events are called intersections (represented with the <span class="math inline">\(\cap\)</span> symbol) between events. For events <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span>, the probability of their joint event will be represented with <span class="math inline">\(p(e_1 \cap e_2)\)</span>. The probability of an intersection of events is the same regardless of which event is considered first;</p>
<p><span class="math display">\[
p(e_1 \cap e_2) = p(e_2 \cap e_1)
\]</span></p>
<p>For two events, <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span>, their intersection is found by;
<span class="math display">\[
p(e_1 \&amp; e_2) = p(e_1\cap e_2) = p(e_1|e_2)p(e_2)
\]</span>
where <span class="math inline">\(p(e_1|e_2)\)</span> is a conditional probability, discussed in the next section.</p>
<p>Conditional events refer to one event, <span class="math inline">\(e_1\)</span>, after another event, <span class="math inline">\(e_2\)</span>, is known. <span class="math inline">\(p(e_1|e_2)\)</span> represents the probability of <span class="math inline">\(e_1\)</span> given, or after knowing, <span class="math inline">\(e_2\)</span>. These can be defined by rearranging the multiplication rule as follows;
<span class="math display">\[
p(e_1|e_2)p(e_2)=p(e_1\cap e_2) \Rightarrow p(e_1|e_2)= \frac{p(e_1\cap e_2)}{p(e_2)}
\]</span></p>
<p>Noting that, by the reflexively of joint events and the definition of the multiplication rule, <span class="math inline">\(p(e_1 \cap e_2) = p(e_2 \cap e_1)= p(e_2 | e_1) p(e_1)\)</span>, and so the above equation becomes;
<span class="math display">\[
p(e_1|e_2)=\frac{p(e_2 | e_1) p(e_1)}{p(e_2)}
\]</span>
This equation is known as Bayes’ Theorem or Bayes’ Rule.</p>
<p>A union (represented with the <span class="math inline">\(\cup\)</span> symbol) of events refers to at least one of multiple events occurring. For events, <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span>, their union would include the probability that <span class="math inline">\(e_1\)</span> occurs, the probability that or <span class="math inline">\(e_2\)</span> occurs, and the probability that both <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> occur. Colloquially this is talked about the probability of <span class="math inline">\(e_1\)</span> ‘or’ <span class="math inline">\(e_2\)</span>. Formally this is expressed and computed as;
<span class="math display">\[
p(e_1\text{ or } e_2) = p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1 \cap e_2)
\]</span></p>
<p>Where <span class="math inline">\(p(e_1 \cap e_2)\)</span> is a joint probability.</p>
<p>Independence is a common assumption in many statistical techniques. Statisticians assume independence primarily because it simplifies the computation of certain probabilities. Events are said to be independent if knowing one event does not change the probability of the other event. Formally, if events <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> are independent, this would mean that;
<span class="math display">\[
\begin{aligned}
p(e_1|e_2) &amp;= p(e_1) \\
p(e_2|e_1) &amp;= p(e_2)
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> are independent, then their joint probability simplifies as so;
<span class="math display">\[
p(e_1 \cap e_2) = p(e_1|e_2)p(e_2) = p(e_1)p(e_2)
\]</span>
Where the last equality is only true if <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> are independent (i.e. <span class="math inline">\(p(e_1|e_2)=p(e_1)\)</span>).</p>
<p>Then the probability of their union simplifies to;
<span class="math display">\[
\begin{aligned}
p(e_1 \cup e_2) &amp;= p(e_1) + p(e_2) - p(e_1|e_2)p(e_2) \\
&amp;= p(e_1) + p(e_2) - p(e_1)p(e_2)
\end{aligned}
\]</span>
Where again, the last part of this equality is only true if <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> are independent.</p>
<p>Events <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> are said to be mutually exclusive if the occurrence of either event precludes the occurrence of the other event. Formally mutual exclusivity means that;
<span class="math display">\[
p(e_1 | e_2) = 0 \text{ and } p(e_2 | e_1) = 0
\]</span></p>
<p>If two events are mutually exclusive, then the probability of their joint event is;
<span class="math display">\[
\begin{aligned}
p(e_1 \cap e_2) &amp;= p(e_1|e_2)p(e_2) \\
&amp;= 0*p(e_2) \\
&amp;= 0
\end{aligned}
\]</span></p>
<p>And the probability of their union simplifies to;
<span class="math display">\[
\begin{aligned}
p(e_1 \cup e_2) &amp;= p(e_1) + p(e_2) - p(e_1 \cap e_2) \\
&amp;= p(e_1) + p(e_2) - 0 \\
&amp;=p(e_1) + p(e_2)
\end{aligned}
\]</span></p>
<!--
## Practice Problems and Walkthroughs

- link to my YouTube videos on these things

Practice problems on the amoeba [here](https://www.quora.com/Bobo-the-amoeba-has-a-25-25-and-50-chance-of-producing-0-1-or-2-offspring-respectively-Each-of-Bobos-descendants-also-have-the-same-probabilities-What-is-the-probability-that-Bobos-lineage-dies-out) and  [here](https://www.quora.com/An-amoeba-has-a-75-chance-of-splitting-in-two-and-25-chance-of-dying-Is-there-an-intuitive-reason-why-the-probability-of-extinction-is-not-1).
-->
</div>
<div id="probability-functions" class="section level2">
<h2>Probability Functions</h2>
<!--
Many machine learning algorithms are different ways of estimating f(x)
-->
<p>Here I’ll describe the properties and use cases for probability functions and cumulative probability functions. Conventionally, use <span class="math inline">\(f(x)\)</span> is used represent a probability function, and <span class="math inline">\(F(x)\)</span> to represent a cumulative probability function. <span class="math inline">\(f(x)\)</span> is commonly called a probability mass function (pmf) if <span class="math inline">\(x\)</span> is discrete, and a probability density function (pdf) if <span class="math inline">\(x\)</span> is continuous. <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(F(x)\)</span> are related through integration:</p>
<p><span class="math display">\[
\begin{aligned}
f(x) &amp;= \frac{d}{dx}F(x) \\
F(x) &amp;= \int_{-\infty}^{\infty} f(x) d_{x}
\end{aligned}
\]</span></p>
<p>For discrete variables, the probability function represents the probability that <span class="math inline">\(X\)</span> takes a specific value <span class="math inline">\(x\)</span>: <span class="math inline">\(f(x) = p(X=x)\)</span>. For continuous variables, the probability that the variable <span class="math inline">\(X\)</span> takes any particular value is technically <span class="math inline">\(0\)</span>; <span class="math inline">\(p(X=x)=0\)</span>. However, <span class="math inline">\(p(x)\)</span> is still related to probability functions <span class="math inline">\(f(x)\)</span> through areas. For an arbitrarily small <span class="math inline">\(\epsilon\)</span> the probability that <span class="math inline">\(X\)</span> takes a value between <span class="math inline">\(x-\epsilon\)</span> and <span class="math inline">\(x+\epsilon\)</span> is the area of the distribution over that range;</p>
<p><span class="math display">\[
\begin{aligned}
p( x - \epsilon&lt; x &lt; x+\epsilon) &amp;= F(x+\epsilon) - F(x-\epsilon) \\
&amp;= \int_{-\infty}^{(x-+\epsilon)}f(x)d_{x} - \int_{-\infty}^{(x-\epsilon)}f(x)d_{x}
\end{aligned}
\]</span></p>
<p>Probability functions have to satisfy a few conditions:</p>
<ol style="list-style-type: decimal">
<li></li>
<li></li>
<li></li>
</ol>
</div>
<div id="transformations-of-random-variables" class="section level2">
<h2>Transformations of Random Variables</h2>
<p>Given <span class="math inline">\(x\sim F_X(x)\)</span> and <span class="math inline">\(y=g(x)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
F_Y(y) = \begin{cases}
F_X(g^{-1}(y)), &amp; \text{ if g is increasing wrt x}\\
1-F_X(g^{-1}(y)), &amp; \text{ if g is decreasing wrt x}
\end{cases}
\end{aligned}
\]</span></p>
<p>By the chain rule, the pdf is;</p>
<p><span class="math display">\[
\begin{aligned}
f_Y(y)=\frac{d}{dy}F_y(y) = f_X(g^{-1}(y))\left| \frac{d}{dy} g^{-1}(y) \right|
\end{aligned}
\]</span></p>
<p>A special case of this is the probability integral transformation;</p>
<p><span class="math display">\[
\begin{aligned}
x&amp;\sim F_X(x) \\
Y&amp;=F_X(x) \\
P(Y\leq y) &amp;= P(F_X(X)\leq y) \\
&amp;= P(F^{-1}_X(F_X^{-1}(X))\leq F_X^{-1}(y)) \\
&amp;= P(X \leq F_X^{-1}(y)) \\
&amp;= F_X(F_X^{-1}(y)) \\
&amp;= y
\end{aligned}
\]</span></p>
<p>Which shows the relationship between a uniform random variable and the CDF of any other random variable.</p>
<!--
Link to YouTube video on the circle problem
- circle problem
-->
<!--
### Probability Distributions
Make YouTube videos on each. 
- 1) Walk through properties from Wikipedia!
- 2) Walk through some simulations in Python!
-->
<!--
## Central limit theorem

Sum of any random variable converges to normal distribution. 
- show for binomial case, to justify Z test
-->
</div>
<div id="expectation" class="section level2">
<h2>Expectation</h2>
<!--
expectation, variance, conditional or iterated expectations, law of total variance, variance of multiple variables
-->
<p>For any functions of a random variable, <span class="math inline">\(g(x)\)</span>, the expectation of that function is;</p>
<p><span class="math display">\[
E_{x}(g(x))=\int_{-\infty}^{\infty}g(x)f(x)d_{x}
\]</span></p>
<p>For discrete random variables, <span class="math inline">\(E(x)=\sum_xxp(x)\)</span>. For continuous random variables, <span class="math inline">\(f(x)dx\)</span> is conceptually <span class="math inline">\(p(x)\)</span> since it is the area under an infinately small segment of the probability function if <span class="math inline">\(f(x)\)</span> represents the height and <span class="math inline">\(dx\)</span> represents an infinately small width.</p>
<p>Expectation is a linear operator, so for constants <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> and random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>; <span class="math inline">\(E(ax+by+c) = aE(x)+bE(y)+c\)</span>.</p>
<p>Expectation for joint random variables</p>
<p><span class="math display">\[
E_{x,y}(g(x,y))=\int_{s=x}\int_{t=y}g(s,t)f(s,t)d_{s}d_{t}
\]</span></p>
<p>Marginalizing a distribution from a joint probability function (<span class="math inline">\(f(x,y)\)</span>) or a conditional probability function (<span class="math inline">\(f(x|y)\)</span>):
<span class="math display">\[
\begin{aligned}
f_x(x)&amp;=\int_yf_{x,y}(x,y)d_{y}\\
&amp;=\int_y f_{x|y}(x|y)f_y(y)d_{y}\\
&amp;=E_y(f(x|y))
\end{aligned}
\]</span></p>
<p><a href="https://en.wikipedia.org/wiki/Law_of_total_expectation">Law of total expectation, and rough proof</a></p>
<p><span class="math display">\[
\begin{aligned}
E_{x}(x)&amp;=E_{y}(E_{x|y}(x|y))\\
&amp;=\int_y\int_{x}xf_{x|y}(x|y)d_{x}f(y)d_{y} \\
&amp;=\int_{x}x\int_{y}f_{x|y}(x|y)f(y)d_{y}d_{x} \\
&amp;=\int_{x}xf(x)d_{x}\\
&amp;=E_{x}(x)
\end{aligned}
\]</span></p>
<p>The last steps reflect that the internal integral is just marginalizing across y; <span class="math inline">\(f_x(x)=\int_y f_{x|y}(x|y)f_y(y)d_{y}\)</span>.</p>
</div>
<div id="variance" class="section level2">
<h2>Variance</h2>
<p>Covariance and variance</p>
<p><span class="math display">\[
\begin{aligned}
Cov(X,Y)&amp;=E_{x,y}((X-E_{x}(X))(Y-E_{y}(Y)))\\
Var(X)&amp;=Cov(X,X)=E_{x}((X-E_{x}(X))^2) \\
&amp;= E(X^2)-E(X)^2
\end{aligned}
\]</span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias-Variance tradeoff</a> decomposes the theoretical variance obtained when estimating a function <span class="math inline">\(g(x)\)</span> with a model <span class="math inline">\(\hat{g}(x)\)</span> into three components that can be used to motivated changes to the model. Specifically, the Bias-Variance tradeoff is a decomposition of the expected prediction error; <span class="math inline">\(g(x)-\hat{g}(x)\)</span>. To see it’s relation to variance, first rearrange the variance equation above:</p>
<p><span class="math display">\[
\begin{aligned}
Var(X)= E(X^2)-E(X)^2 \\
\Rightarrow E(X^2) = Var(X)+E(X)^2 \\
\end{aligned}
\]</span></p>
<p>Then define the random variable <span class="math inline">\(X\)</span> to be the errors of the model;</p>
<p><span class="math display">\[
\begin{aligned}
E((g(x)-\hat{g}(x))^2) &amp;= Var((g(x)-\hat{g}(x))^2)+E((g(x)-\hat{g}(x)))^2 \\
&amp;= (g(x)-E(\hat{g}(x)))^2 + E((E(\hat{g}(x))-\hat{g}(x))^2) + Var(g(x))\\
&amp;= Bias(\hat{g}(x))^2 + Var(\hat{g}(x)) + Var(g(x))
\end{aligned}
\]</span></p>
<p>Here bias represents the squared error between the true function <span class="math inline">\(g(x)\)</span> and the expected model <span class="math inline">\(E(\hat{g}(x))\)</span>. Variance represents the squared error between the expected model and the obtained model. And <span class="math inline">\(Var(g(x))\)</span> represents the true noise or irreducible error in the process <span class="math inline">\(g(x)\)</span>.</p>
<p>Variance of linear combinations of random variables:</p>
<p><span class="math display">\[
Var(aX+bY+c)=a^2Var(X)+b^2Var(Y)+2abCov(X,Y)
\]</span></p>
<p>Variance is also crucial for designing experiments, for example if <span class="math inline">\(a\)</span> or <span class="math inline">\(b\)</span> have different signs and <span class="math inline">\(Cov(X,Y)&gt;0\)</span>, then the variance between is reduced because the covariance is subtracted off. This is a justification for within-participant designs, where <span class="math inline">\(X\)</span> represents the first measurement, <span class="math inline">\(Y\)</span> represents a future measurement, <span class="math inline">\(a=-1\)</span> and <span class="math inline">\(b=1\)</span> to reflect a difference comparing the second and first measurements (<span class="math inline">\(Y-X\)</span>), and since the measurements come from the same person here, it is reasonable to assume they are correlated (<span class="math inline">\(Cov(X,Y)&gt;0\)</span>). In this case, <span class="math inline">\(Var(X,Y)=Var(X)+Var(Y)-Cov(X,Y)\)</span>, which is not larger than the variance that would be obtained if the measurements were from independent samples.</p>
<!--
Confidence intervals, prediction intervals
-->
<!--
Variance of a function:
$$
\begin{aligned}
Var(g(x)) &= E((g(x)-E(g(x)))^2)\\
&\approx \left(\frac{d}{dx}g(E(x))\right)^2var(x) 
\end{aligned}
$$
-->
<p><a href="https://en.wikipedia.org/wiki/Law_of_total_variance">Law of total variance</a>: The variance of a variable <span class="math inline">\(X\)</span> can be decomposed into the variation in the variable that remains when <span class="math inline">\(Y\)</span> is known or irreducible error (<span class="math inline">\(E(Var(X|Y))\)</span>), and the variation that carries through from the uncertainty in the model estimation <span class="math inline">\(Y\)</span> (<span class="math inline">\(Var(E(X|Y)\)</span>).
<span class="math display">\[
\begin{aligned}
Var(X)&amp;=E(Var(X|Y))+Var(E(X|Y))
\end{aligned}
\]</span></p>
<p>This arises in hierarchical models and prediction intervals.</p>
<p><span class="math display">\[
\begin{aligned}
x               &amp;\sim N(\mu,\sigma)\\
E(x)            &amp;=\mu\\
Var(x)          &amp;=\sigma^2\\
\hat{\mu}_n     &amp;\sim N\left(\mu,\frac{\sigma}{\sqrt{n}}\right)\\
E(\hat{\mu}_n)  &amp;=\mu\\
Var(\hat{\mu}_n)&amp;=\frac{\sigma^2}{n}\\
E(x_{n+1})      &amp;= E(E(x_{n+1}|\hat{\mu}_n))= E(\hat{\mu}_n)=\mu \\
Var(x_{n+1})    &amp;=E(Var(x_{n+1})) + Var(E(x_{n+1}|\mu_{n})\\
                &amp;=\sigma^2+\frac{\sigma^2}{n}\\
(100-\alpha)PI: &amp; E(x_{n+1}|\hat{\mu}_n) \pm T_\alpha \sqrt{Var(x_{n+1})}\\
\end{aligned}
\]</span></p>
<!--
Expectation and variance are the foundation for decision making
- value function v(x) that assigns values to the outcomes x
- If you know the distribution and value function, then you can directly compare choices based on their expected value
-->
<!--
## Models as conditional expectations

Models output an expectation that depends on the model, its structure, its features, and in Bayesian settings it's priors
-->
</div>
<div id="information-theory" class="section level2">
<h2>Information Theory</h2>
<p>Information or Surprise is a measure of how unexpected an event was:
<span class="math display">\[
I(x)=-log(f(x))=log\left(\frac{1}{f(x)} \right)
\]</span></p>
<p>Entropy is the expected surprise – higher entropy relates to more uncertainty:
<span class="math display">\[
H(X)=E(I(x)) = E(-log(f(x)))  = \int_X f(x)I(x)dx = -\int_X f(x)log(f(x))d_x
\]</span></p>
<p>K-L Divergance, or relative entropy, is the expected distance between distributions with respect to one of the distributions:
<span class="math display">\[
\begin{aligned}
D_{KL}(f_{x}(x);f_{y}(x)) &amp;= \int_{X} f_{x}(x)log\left(\frac{f_{y}(x)}{f_{x}(x)} \right)d_x\\
&amp;= \int_X f_{x}(x)(log(f_{y}(x))-log(f_{x}(x))d_x \\
\end{aligned}
\]</span></p>
<p>Divergance is an important concept in machine learning, because it can be a loss function when we are estimating a distribution <span class="math inline">\(f(x)\)</span> with a model <span class="math inline">\(\hat{f}(x)\)</span>. From a Bayesian perspective, Divergance between a prior and posterior measures the information gained by observing the data. Bayesian experimental design focuses on collecting data that maximizes the divergance between the posterior and prior, i.e. the most informative data.</p>
<p>Mutual Information is the divergance from the joint distribution <span class="math inline">\(f_{x,y}(x,y)\)</span> to the joint when independence is assumed <span class="math inline">\(f_x(x)f_y(y)\)</span> – i.e. a measure of non-independence;
<span class="math display">\[
I(x;y)=\int_{y} \int_{x} f_{x,y}(x,y)log\left(\frac{f(x,y)}{f(x)f(y)}  \right)d_yd_x
\]</span></p>
</div>
<div id="causal-graphs" class="section level2">
<h2>Causal Graphs</h2>
<ul>
<li>Add some info from Judeal Pearl papers relevant to experimentation and inference?</li>
</ul>
</div>
</div>
<div id="causal-inference" class="section level1">
<h1>Causal Inference</h1>
<p>The ultimate goal of most statistical work is to discover the causes of some outcome so that the outcome can be controlled. Hence, it is important to understand when causality can be inferred.</p>
<p>From this perspective, there is a treatment assignment <span class="math inline">\(T\)</span> and an outcome of interest that depends on the treatment <span class="math inline">\(Y(T)\)</span>. Ideally, we would be able to compute the expected treatment effeoct for any individual <span class="math inline">\(E(Y(T=1)-Y(T=0))\)</span>, but it is impossible to assign an individual to both variants of the treatment under the exact same conditions – e.g. one must be done at a later time than the other. Causal inference approaches specify the assumptions, models, and designs neeed to make causal statements.</p>
<p>Most methods like t-tests and ANOVAs compare the observed outcomes in the treatment group to the observed outcomes in the control group. Causal inference methods can give a more powerful comparison between observed outcomes and counterfactual outcomes <a href="https://www.youtube.com/watch?v=GTgZfCltMm8">CI video</a> at 10:00.</p>
<div id="framework" class="section level2">
<h2>Framework</h2>
<p>The general setting I’ll refer to a causal graph where individuals <span class="math inline">\(i\)</span> are assigned to a particular condition indicated with <span class="math inline">\(Z_i\)</span>, they experience a condition indicated by <span class="math inline">\(W_i\)</span>, and an observed outcome for a level of the assigned and experienced condition <span class="math inline">\(Y_i(Z_i,W_i)\)</span>.</p>
<p>The causal craph is;</p>
<p><span class="math display">\[
Z_i \rightarrow W_i \rightarrow Y_i(Z_i,W_i)
\]</span></p>
<p>Where the assignment and experience of a treatment or control condition are represented with;</p>
<p><span class="math display">\[
\begin{aligned}
Z_i &amp;= \begin{cases}
1, &amp; \text{Assigned to treatment} \\
0 &amp; \text{Assigned to control}
\end{cases} \\
W_i &amp;= \begin{cases}
1, &amp; \text{Experienced treatment} \\
0 &amp; \text{Experienced control}
\end{cases}
\end{aligned}
\]</span></p>
<p>And there are potential outcomes <span class="math inline">\(Y\)</span> under any combination of assigned and experienced conditions;</p>
<p><span class="math display">\[
\begin{aligned}
Y_i(Z_i,W_i) &amp;= \begin{cases}
Y_i(0,0), &amp; \text{Outcome when assigned C and experienced C}\\
Y_i(0,1), &amp; \text{Outcome when assigned C and experienced T} \\
Y_i(1,0), &amp; \text{Outcome when assigned T and experienced C} \\
Y_i(1,1)  &amp; \text{Outcome when assigned T and experienced T} 
\end{cases}
\end{aligned}
\]</span></p>
<p>In most lab studies this setup simplifies because <span class="math inline">\(Z_i=W_i\)</span> since there is no possibility that a lab participant does not experience the condition they are assigned to. However, in online experimentation users can often opt out of a treatment condition, and users assigned to a control condition could end up getting access to features of the treatment condition. Methods to address these complications are discussed below.</p>
</div>
<div id="key-assumptions" class="section level2">
<h2>Key Assumptions</h2>
<ul>
<li>Exclusion: <span class="math inline">\(Y(0,W_i)=Y(1,W_i)=Y(W_i)\)</span>, or probabilistically <span class="math inline">\(p(Y_i(Z_i,W_i)|Z_i,W_i)=p(Y_i(Z_i,W_i)|W_i)\)</span>. The treatment assignment <span class="math inline">\(Z_i\)</span> influences the outcome only through the experiences condition <span class="math inline">\(W_i\)</span>, i.e. conditional on an experienced condition, the observed outcome <span class="math inline">\(Y_i\)</span> is independent of the assigned condition.</li>
</ul>
<p>To simplify notation, I’ll assume exclusion past here and refer to observed outcomes as <span class="math inline">\(Y_i(W_i)\)</span>.</p>
<ul>
<li>Stable units:</li>
<li>units experience one version of the treatment</li>
<li><p>one unit’s assignment doesn’t influence another unit’s outcome</p></li>
<li><p>Endogeneity refers to a relationship between covariates and the error terms, e.g. through unobserved confounds.</p></li>
<li><p>Unconfoundedness: <span class="math inline">\(Y_i(0),Y_i(1) \perp Z_i | X_i\)</span>. Conditional on observed covariates (<span class="math inline">\(X_i\)</span>), assignments (<span class="math inline">\(Z_i\)</span>) and potential outcomes (<span class="math inline">\(Y(W_i)\)</span>) are independent.</p></li>
</ul>
<p>If unconfoundedness isn’t satisfied, i.e. the distributions of xi differ between levels of wi, then</p>
</div>
<div id="common-methods" class="section level2">
<h2>Common Methods</h2>
<p>Some summarized in this <a href="https://towardsdatascience.com/causal-inference-using-difference-in-differences-causal-impact-and-synthetic-control-f8639c408268">Medium Post</a></p>
<div id="diff-in-diff" class="section level3">
<h3>Diff in Diff</h3>
<p>Contexts and example ..</p>
<p>Assumptions and requirements:</p>
<ul>
<li>Requires observed data on a control group pre and post treatment</li>
<li>Assumes the trend (mean?) in the control group is an appropriate proxy for the trend (mean?) in the treatment group, had the treatment not occurred.</li>
<li>Assumes the temporal dimension isn’t informative.</li>
</ul>
<p>Model:</p>
<p><span class="math display">\[
\begin{aligned}
I_{T}&amp;:\text{Indicator for treatment group} \\
I_{Post}&amp;:\text{Indicator for post treatment} \\
Y &amp;= \beta_0 + \beta_1 I_{T} + \beta_2 I_{Post} + \beta_3 I_{T} I_{Post}
\end{aligned}
\]</span></p>
</div>
<div id="causal-impact" class="section level3">
<h3>Causal Impact</h3>
<p>Causal impact was developed by Google, docs <a href="https://google.github.io/CausalImpact/CausalImpact.html">here</a>.</p>
<p>Context:</p>
<ul>
<li>Want to estimate the counterfactual for an observed timeseries, pre and post treatment.</li>
</ul>
<p>Assumptions and requirements:</p>
<ul>
<li>Requires other time series related to the target time series</li>
<li>Assumes the other time series are not influenced by the treatment, generally good candidates include; google trends time series, the weather, other countries or markets where no action was taken, unemployment indecies, stock prices, … .</li>
</ul>
<p>Model:</p>
<ul>
<li>In pre period, train any kind of model to estimate the target time series as a function of the other time series. The model used in the Causal Impact package is a Bayesian structural time series model.</li>
<li>Google’s Causal Impact method uses Bayesian structural time series to construct a counterfactual group – an estimate of what the time series would have looked like if the treatment had not been assigned. The method is described in <a href="https://www.youtube.com/watch?v=GTgZfCltMm8">this video</a>. The method estimates a counterfactual using other related time series that were not influenced by the treatment.</li>
<li>BSTS uses spike and slab prior for feature selection.</li>
<li>In post period, use the model to estimate a counterfactual time series (i.e. synthetic control).</li>
<li>Independent Python implementation <a href="https://github.com/tcassou/causal_impact">here</a>.</li>
</ul>
<p>Benefits:</p>
<ul>
<li>Provides pointwise estimates of the causal effect over time, as well as a cumulative estimate (summing up the pointwise estimates)</li>
<li>Bayesian approach, provides credible intervals</li>
</ul>
</div>
<div id="synthetic-controls" class="section level3">
<h3>Synthetic Controls</h3>
<p>Context:</p>
<p>Assumptions and requirements:</p>
<ul>
<li>Requires multiple related time series</li>
<li>Assumes a linear combination of the related time sereis is a good proxy for the counterfactual</li>
</ul>
<p>Model:</p>
<ul>
<li>Regress target time series on related time series</li>
<li>Use estimates from this model as the counterfactual</li>
</ul>
<p>When to use vs causal impact:</p>
<ul>
<li>The other time series are more conceptually identical - e.g. observed outcomes in untreated market segments subject to the same plausiable confounds – e.g. different nearby counties in a state.</li>
<li>Causal impact might be better if the time series are thought to be components of the target – stock price, unemployment, page impressions, google trends, … .</li>
</ul>
</div>
<div id="iv-analysis" class="section level3">
<h3>IV Analysis</h3>
<p>More from <a href="https://www.youtube.com/watch?v=NLgB2WGGKUw">Ben Lambert</a>, good explanation <a href="https://www.youtube.com/watch?v=OWHCbEP56ms">here</a>, <a href="https://www.youtube.com/watch?v=cX5q_dKt6iU">part II</a>. Case study using quarter and years of education as <span class="math inline">\(Z\)</span> in place of education as the instrument [here](<a href="https://www.youtube.com/watch?v=pI9YGSJ2qPk" class="uri">https://www.youtube.com/watch?v=pI9YGSJ2qPk</a>, <a href="https://www.youtube.com/watch?v=WjcoHAJ4_Mc">relation to 2SLS</a>.</p>
<p>Mostly from Gelman chapter “Causal inference with more complicated observational designs” chapter from Gina.</p>
<p>Context:</p>
<ul>
<li>Goal is to find the causal effect of <span class="math inline">\(x\rightarrow y\)</span>.</li>
<li>Issue: <span class="math inline">\(Cov(x,\epsilon)\neq 0\)</span>, a feature is correlated to unobserved confounds, so <span class="math inline">\(\hat{\beta}_{OLS}\)</span> is unbiased and inconsistent.</li>
</ul>
<p>Assumptions and Requirements:</p>
<ul>
<li>An instrument <span class="math inline">\(z\)</span> is measured so it can be included in the model.</li>
<li><span class="math inline">\(Cov(z,x)\neq 0\)</span>, the instrumental variable is related to <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(Cov(z,\epsilon)=0\)</span>, the instrument is not correlated with unobserved confounds.</li>
</ul>
<p>Model, 2 stage least squares:</p>
<p>Done to address bias in <span class="math inline">\(\hat{\beta}_{OLS}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\hat{x}&amp;=\gamma z + u \\
\hat{y}&amp;=\beta_{IV} \hat{x} + \epsilon \\
\beta_{IV} &amp;= (Z^TX)^{-1}Z^TY = \frac{Cov(Z,Y)}{Cov(Z,X)}
\end{aligned}
\]</span></p>
<p>Limitation:</p>
<ul>
<li><span class="math inline">\(\beta_{IV}\)</span> can still be biased, but is at least consistent (<span class="math inline">\(\beta_{IV}\overset{p}{\rightarrow} \beta\)</span>).</li>
</ul>
<p>This relates to the intent to treat (ITT) estimate, where the effect of assignment to the treatment group is consided <span class="math inline">\(Z\)</span>, but the effect of actually being treated <span class="math inline">\(X\)</span> may not be known or estimated.</p>
</div>
<div id="regression-discontinuity" class="section level3">
<h3>Regression Discontinuity</h3>
<p>Context:</p>
<p>Some systematic mechanism forces a discontinuity in what would be expected to be a regression line.</p>
<ul>
<li>E.g. 1: Yelp rounds star ratings so that businesses close in underlying rating (4.49 and 4.5 stars) are assigned to different conditions (4 and 4.5 starts).</li>
<li>E.g. 2: Schools might filter on exam scores, so that students who are very similar in underlying ability or test score (1 point below the threshold vs at the threshold) might be assigned to different universities.</li>
</ul>
</div>
<div id="fixed-effects-regression" class="section level3">
<h3>Fixed Effects Regression</h3>
<p>Context:</p>
<ul>
<li>Unobserved heterogineity <span class="math inline">\(\alpha_i\)</span> that is related to features, <span class="math inline">\(Cov(\alpha_i,x_{i,t})\neq 0\)</span></li>
<li>Issue: <span class="math inline">\(Cov(\alpha_i,x_{i,t})\neq 0\)</span>, this implies that <span class="math inline">\(\hat{\beta}\)</span> isn’t consistent for <span class="math inline">\(\beta\)</span>, i.e. the sampling distribution of <span class="math inline">\(\hat{\beta}\)</span> doesn’t converge to <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p>Assumptions and Requirements:</p>
<ul>
<li><span class="math inline">\(Cov(x_{i,t},u_[i,t])=0\)</span>, i.e. weak exogeniety</li>
<li>No perfect correlation between <span class="math inline">\(x\)</span>.</li>
<li>These assumptions imply the fixed effects estimates (<span class="math inline">\(\beta_{FE}\)</span>) are consistent.</li>
</ul>
<p>Model, from <a href="https://www.youtube.com/watch?v=sFvV9b1cGFc">here</a>:</p>
<p>The procedure is to average over time to get <span class="math inline">\(\bar{y}_i\)</span>, and subtract this off, taking advantage of <span class="math inline">\(\alpha_i=\bar{\alpha}_i\)</span> where the later is averaged across time.</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_i &amp;: \text{unobserved hetergenity} \\
Cov(\alpha_i,x_{i,t}) &amp;\neq 0 \\
u_{i,t} &amp;: \text{error} \\
y_{i,t}&amp;=\beta x_{i,t} + \alpha_i + u_{i,t}\\
\bar{y}_{t}&amp;=\beta \bar{x}_{i} + \bar{\alpha}_i + \bar{u}_{i} \\
y_{i,t}-\bar{y}_{t}&amp;=\beta_{FE} (x_{i,t}-\bar{x}_{i}) + (\alpha_i - \bar{\alpha}_i)+ u_{i,t} - \bar{u}_{i} \\
&amp;= \beta_{FE} (x_{i,t}-\bar{x}_{i}) + u_{i,t} - \bar{u}_{i} \\
\end{aligned}
\]</span></p>
<p>Limitations:</p>
<ul>
<li>removes anything that is constant over time, meaning effects of any time-constant variables can’t be estimated</li>
</ul>
</div>
<div id="first-differences" class="section level3">
<h3>First differences</h3>
<p>Context:</p>
<ul>
<li>similar to fixed effects, but less asymptotically efficient if there are serially uncorrelated errors <a href="https://www.youtube.com/watch?v=G7WqK2o474Y">see this vid</a></li>
</ul>
</div>
<div id="random-effects" class="section level3">
<h3>Random Effects</h3>
<p><a href="https://www.youtube.com/watch?v=bQampZBzU9Q">description in this video</a>.</p>
<p>Context:</p>
<p>Assumptions and requirements:</p>
<p>Model:</p>
</div>
<div id="propensity-score-matching" class="section level3">
<h3>Propensity Score Matching</h3>
<p>Context:</p>
<p>Treatment assignment is non-random and might be related to the relationship between treatment and outcome. So you match people on their propensity to be assigned to treatment.</p>
</div>
<div id="microsoft-dowhy" class="section level3">
<h3>Microsoft DoWhy</h3>
<ul>
<li>see <a href="https://github.com/microsoft/dowhy">docs</a></li>
</ul>
</div>
</div>
</div>
<div id="efficient-inference" class="section level1">
<h1>Efficient Inference</h1>
<p>More efficient studies require fewer resources (time, data) to reach a conclusion.</p>
<ul>
<li>Methods to reduce variance,</li>
<li>Assessing the variance of estimators</li>
<li>Efficient models</li>
</ul>
<div id="efficient-sampling" class="section level2">
<h2>Efficient Sampling</h2>
<ul>
<li>Assigning at a point proximal to the experimental manipulation (see blogs in pinterest deep dive)</li>
<li>…</li>
</ul>
</div>
<div id="efficient-designs" class="section level2">
<h2>Efficient Designs</h2>
<ul>
<li>Independent samples</li>
<li>matching</li>
<li>repeated measures</li>
</ul>
</div>
<div id="efficient-models" class="section level2">
<h2>Efficient Models</h2>
<ul>
<li>Precision variables</li>
<li>Estimands</li>
<li>Wald test vs LRT vs Bayes vs Score test</li>
</ul>
</div>
</div>
<div id="experiment-design" class="section level1">
<h1>Experiment Design</h1>
<div id="foundations" class="section level2">
<h2>Foundations</h2>
<ul>
<li>Hypotheses</li>
<li>Treatment and control conditions</li>
<li>assumptions (including CI assumptions, SUTVA, …?)</li>
</ul>
</div>
<div id="concerns" class="section level2">
<h2>Concerns</h2>
<ul>
<li>network effects: front-end, it could be from different interactions, back-end it could be from changes in algorithm behavior for units in C based on different behavior from units in T – e.g. if a new recommender for units in T influences them to watch more of x, then x may also get recommended more to those in C.</li>
<li>learning effects (change aversion, novelty seeking)</li>
<li>early adopters</li>
<li>Non-compliance: Those assicned to treatment may not actually experience the treatment.</li>
<li>Crossover: Those assigned to control might gain access to the treatment.</li>
<li>Treatment inhomogenaity: Some user segments might respond differently than others.</li>
</ul>
</div>
<div id="treatment-units" class="section level2">
<h2>Treatment Units</h2>
<p>What is the unit at which we assign treatments?</p>
<p>Ideally independent individuals, but online it is hard to know who is visiting a webpage, or crucial to keep experiences comparable across devices. Also, in networks, individuals are not independent and it is important to keep user experience consistent across connected individuals.</p>
<p>Proxies for individuals include;</p>
<ul>
<li>User ID or account – most clearly tied to a user, but</li>
<li>cookies – device and browser specific, so these could differ across a user’s browsers or devices.</li>
<li>IP address (device request return address) – device specific, so a user’s experience might differ across devices.</li>
</ul>
<p>In networks, loosely connected clusters of individuals can be used as experimental units (see unofficial google data science blog post on this).</p>
<ul>
<li>a solution to SUTVA violations, but decreases sample size and power dramatically</li>
<li>cluster-based, stratified, serial, balanced, …</li>
<li>propensity scores and matching</li>
</ul>
</div>
<div id="generative-model-and-adjustment-variables" class="section level2">
<h2>Generative Model and Adjustment Variables</h2>
<ul>
<li>Specify the hypothesized generative process</li>
<li>Confounds</li>
<li>Precision</li>
<li>Neusance</li>
</ul>
</div>
<div id="common-designs" class="section level2">
<h2>Common Designs</h2>
<p>A/B testing</p>
<p>A/A testing to estimate variation</p>
<p>Variance reduction designs (paired designs, matching, …)</p>
<p>Bandits for limited data or maximizing an objective</p>
</div>
<div id="assignment-mechanism" class="section level2">
<h2>Assignment Mechanism</h2>
<ul>
<li>maps samples (xi,yi) into treatment or control conditions.</li>
<li>randomized control trials ideal, not always possible</li>
<li>other options, …?</li>
<li>test assignment validity, could use propensity scores or maybe chi square.</li>
</ul>
</div>
<div id="duration" class="section level2">
<h2>Duration</h2>
<ul>
<li>learning effects: initial exploration or novelty seeking</li>
<li>learning effects: initial change aversion</li>
<li>power analysis for sample size</li>
<li>time to run to mitigate</li>
</ul>
<!---
# Experiment Design

## Objective, demand, and value

Objective is based on a metric (increase clickthrough or revenue per user). 

Demand -- if adding a new user feature, how can demand for the feature be assessed? 

value = benefit - cost. 

Expected value helps with decisions about the size and duration of an experiment

## Constructs, metrics, and scoping

Once an objective is clear, the details of what can be measured need to be flushed out. 

Metrics (fill in from udamy course section on metrics)

Netflix metrics -- streaming hours, retention (users staying on platform), viewing for a title (e.g. effected by artwork -- but may be at the detrement of general viewing?) 

### User flow and target metrics

Think of the sequence of actions a user might take on the site. Experiments can target transtition probabilities between any user/platform states. 

- number of clicks
- time on page
- ... 

### Invariants

Metrics that shouldn't change or differ across groups. For example, demographic variables should be the same across groups if the randomization or balancing worked properly. Also, many application performance metrics and business metrics should be unchanged, or monitored just in case they change. 

### Confounding variables

Mitigated by randomization. 

### Precision variables

Can also highlight features that one would want to match treatment and control on.

Examples: 

- number of posts
- number of followers
- visibility / impressions

## Metric Validation

User expreience research, retrospective analyses of past data or log files, ... . 

## Conditions

Define the experimental manipulations. 

## Treatment Units

What is the unit at which we assign treatments? 

Ideally independent individuals, but online it is hard to know who is visiting a webpage, or crucial to keep experiences comparable across devices. Also, in networks, individuals are not independent and it is important to keep user experience consistent across connected individuals. 

Proxies for individuals include;

- User ID or account -- most clearly tied to a user, but 
- cookies -- device and browser specific, so these could differ across a user's browsers or devices.
- IP address (device request return address) -- device specific, so a user's experience might differ across devices.

In networks, loosely connected clusters of individuals can be used as experimental units (see unofficial google data science blog post on this).

## Assignment Mechanism 

The assignment mechanism samples members from a population and assignes them to conditions. 

How are units assigned to treatments?

Randomized control trials are the ideal, but many issues arise in online experimentation settings. 

(look up desirable properties form causal inference notes)

### Population 

What group is being sampled from?

### Cohorts

Random sample, cluster-based, stratified, serial, balanced, ... 

### Synthetic Control Groups

Propensity matching

### Limitations

- Non-compliance: Those assicned to treatment may not actually experience the treatment. 
- Crossover: Those assigned to control might gain access to the treatment. 
- Treatment inhomogenaity: Some user segments might respond differently than others.

# Implementation

Batched, or real-time

## Size

power, sample size

## Duration

- learning effects: initial exploration or novelty seeking
- learning effects: initial change aversion

Solution: Consider running experiment past any initial observed effect.

### Temporal variation

### Optional stopping

Why it's an issue for frequentists

Optamizely using a threshold on FDR from likelihood ratio tests in frequentist setting

Bayesian justifications

p-values versus likelihood ratios

## Estimands

- Average treatment effect
- Treatment on treated

## Effiencicy or Variance Reduction

- look up variance reduction in A/B tests, a common method might be to incorporate precision variables like demographic data or user data such as device or browser. 

e.g. with a two sample t-test, $Y_t - Y_{t-1} = \beta_0$, versus a model $Y_t = \beta_0 + \beta_1 Y_{t-1}$

- get table for different tests of means from soc sci 10 notes.

## Checking assignment integrety

- Check demographics across buckets, if randomization worked then demographics should be approximately equally represented in the buckets. 

--->
</div>
<div id="integrety-checks" class="section level2">
<h2>Integrety Checks</h2>
<ul>
<li>check that those assigned to treatment received it</li>
<li>check that stratifications were implemented correctly</li>
<li>Check demographics across buckets, if randomization worked then demographics should be approximately equally represented in the buckets.</li>
</ul>
</div>
<div id="maximum-likelihood-versus-bayes-estimators" class="section level2">
<h2>Maximum Likelihood versus Bayes’ Estimators</h2>
<p>likelihood is one term in Bayes’ Theorem</p>
</div>
</div>
<div id="glms" class="section level1">
<h1>GLMs</h1>
<p>Again about the variance – this time it takes a form other than normal.</p>
<p>likelihood, score, fisher information, robust veriance, …?</p>
</div>
<div id="common-tests" class="section level1">
<h1>Common Tests</h1>
<div id="t-test" class="section level2">
<h2>t-test</h2>
<ul>
<li>different variance formulations.</li>
<li>one-sample</li>
<li>two independent sampels</li>
<li>two dependent samples</li>
</ul>
</div>
<div id="anova" class="section level2">
<h2>ANOVA</h2>
</div>
<div id="linear-model" class="section level2">
<h2>Linear Model</h2>
</div>
<div id="z-test-for-proportions" class="section level2">
<h2>Z test for proportions</h2>
<p>Binary data</p>
</div>
<div id="chi-square" class="section level2">
<h2>Chi-Square</h2>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
</div>
<div id="likelihood-ratio-and-bayes-factors" class="section level2">
<h2>Likelihood Ratio and Bayes Factors</h2>
</div>
<div id="always-valid-p-values-ck-optimizely-white-paper" class="section level2">
<h2>always valid p-values (ck optimizely white paper)</h2>
</div>
</div>
<div id="different-estimands-for-ci" class="section level1">
<h1>Different Estimands For CI</h1>
<ul>
<li>Average treatment effect</li>
<li>Treatment on treated</li>
</ul>
</div>
<div id="more-on-variance" class="section level1">
<h1>more on variance</h1>
<div id="emperical-variance-estimation" class="section level2">
<h2>Emperical Variance Estimation</h2>
<p>bootstrapping</p>
<p>A/A testing</p>
</div>
<div id="mixed-effects-models" class="section level2">
<h2>Mixed Effects Models</h2>
<ul>
<li>Maime’s consulting project? Other use cases?</li>
</ul>
</div>
<div id="effiencicy-or-variance-reduction" class="section level2">
<h2>Effiencicy or Variance Reduction</h2>
<ul>
<li>look up variance reduction in A/B tests, a common method might be to incorporate precision variables like demographic data or user data such as device or browser.</li>
</ul>
<p>e.g. with a two sample t-test, <span class="math inline">\(Y_t - Y_{t-1} = \beta_0\)</span>, versus a model <span class="math inline">\(Y_t = \beta_0 + \beta_1 Y_{t-1}\)</span></p>
<ul>
<li>get table for different tests of means from soc sci 10 notes.</li>
</ul>
</div>
<div id="glms-1" class="section level2">
<h2>GLMs</h2>
<!--
models for when the errors aren't assumed to be normally distributed
--->
<p>Stats: descriptive, R-squared, chi-squared;</p>
<p>Probability: distributions, CLT, sampling distributions, p-value,</p>
<p>Stats: k-s, Q-Q plot, hypothesis testing, experimentation;</p>
<p>Probability: Bayes, bootstrap</p>
<p>Probability: maximum likelihood estimation; time series analysis (ARIMA models), granger causality</p>
<p>Dynamic experimentation (multi-armed bandits)</p>
<p>Unit testing; EDA visualization (seaborn, Plotly, Bokeh)</p>
</div>
</div>
<div id="causal-inference-methods" class="section level1">
<h1>Causal Inference Methods</h1>
<p>Good medium blog post <a href="https://towardsdatascience.com/causal-inference-using-difference-in-differences-causal-impact-and-synthetic-control-f8639c408268">here</a></p>
<p>For each mention the assumptions and the model.</p>
<div id="difference-in-differences" class="section level2">
<h2>Difference in Differences</h2>
</div>
<div id="causal-impact-1" class="section level2">
<h2>Causal Impact</h2>
</div>
<div id="synthetic-controls-1" class="section level2">
<h2>Synthetic Controls</h2>
</div>
<div id="propensity-score-matching-1" class="section level2">
<h2>Propensity Score Matching</h2>
</div>
<div id="fixed-effects-regression-1" class="section level2">
<h2>Fixed Effects Regression</h2>
</div>
<div id="instrumental-variables" class="section level2">
<h2>Instrumental variables</h2>
</div>
<div id="regression-discontinuity-1" class="section level2">
<h2>Regression Discontinuity</h2>
</div>
</div>

    </div>

    



    
      






  







<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  
  <img class="portrait mr-3" src="/MacStrelioff/author/admin/avatar_hu27f7dc34c9db8fd2b063f3fba6be9864_288907_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
  

  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/MacStrelioff/authors/admin">Mac Strelioff</a></h5>
    <h6 class="card-subtitle">Cognitive Scientist (Ph.D.) Statistician (M.S.)</h6>
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="/MacStrelioff/#contact" >
          <i class="fas fa-envelope"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://www.linkedin.com/in/macstrelioff/" target="_blank" rel="noopener">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
      
      
      
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://osf.io/4a75z" target="_blank" rel="noopener">
          <i class="fa fa-flask"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://twitter.com/macstrelioff" target="_blank" rel="noopener">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/macstrelioff" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>



      
      
    

    
    <div class="article-widget">
      
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/MacStrelioff/insightstudying/pinterest-deep-dive/" rel="prev">Pinterest</a>
  </div>
  
</div>

    </div>
    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "@macstrelioff" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  
  <p class="powered-by">
    <a href="/MacStrelioff/files/privacy/">Privacy Policy</a>
  </p>
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/MacStrelioff/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//@macstrelioff.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/MacStrelioff/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/MacStrelioff/js/academic.min.d813ae958640746e240f434cafc95afb.js"></script>

  </body>
</html>

