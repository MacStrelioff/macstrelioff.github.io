[{"authors":["admin"],"categories":null,"content":"blah\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/MacStrelioff/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/MacStrelioff/authors/admin/","section":"author","summary":"blah","tags":null,"title":"Mac Strelioff","type":"author"},{"authors":null,"categories":null,"content":"Use the links on the left to navigate to articles on particular topics.\n","date":1557581836,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1557581836,"objectID":"debf13c740a20e259bcf8fe4a7ff2d2e","permalink":"/MacStrelioff/data-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/MacStrelioff/data-science/","section":"data-science","summary":"Use the links on the left to navigate to articles on particular topics.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" Model Comparison: F-test for nested models  Model Comparison: F-test, all parameters in a model  Correlation: R-square and correlation  Correlation: Partial correlation  Correlation: Part correlations  ","date":1536476400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536476400,"objectID":"51191988497a9d0164009b2b4425bf82","permalink":"/MacStrelioff/video-lectures/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/MacStrelioff/video-lectures/","section":"video-lectures","summary":" Model Comparison: F-test for nested models  Model Comparison: F-test, all parameters in a model  Correlation: R-square and correlation  Correlation: Partial correlation  Correlation: Part correlations  ","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" true  Made as test\n","date":1536476400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536476400,"objectID":"e9a4998af0f4b3e05ada18a63d838e92","permalink":"/MacStrelioff/tutorial2/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/MacStrelioff/tutorial2/","section":"tutorial2","summary":"true  Made as test","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536476400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536476400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/MacStrelioff/tutorial/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/MacStrelioff/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"title","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906574400,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/MacStrelioff/talk/example/","publishdate":"2017-01-01T00:00:00-08:00","relpermalink":"/MacStrelioff/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"  Background and Setup Twitter Data Pulling Data from Twitter’s API Working with Twitter Status objects Embed a Status Convert a Status to a dict Access Status Attributes (How I accessed the data used below)  Poisson Process Assumptions Specification and Properties Checking the Homogeneity Assumption Checking the exponential distribution of intervals  Model of Tweet Fequency Using Conjugacy Inference On Tweet Rate \\(\\lambda\\) Over Time  Predicting Number Of Tweets In Interval \\(s\\) Summary   Background and Setup In this notebook I focus on explaining Poisson processes and conjugacy applied to my Twitter activity.\nTo get started, I followed directions from three main sources that walked through the twitter and python-twitter libraries, and described how to apply for Twitter API access and use the keys;\npython-twitter: Blogpost here tweepy: Blogpost here and docs here Additional information on obtaining API keys and authenticating Twitter connections in a blogpost here  I include my code to import the required libraries and set up API access keys, though the data used here were pulled and saved before writing the notebook. The main focus is on understanding Poisson processes and adaptive modeling of such processes using conjucacy.\nlibrary(reticulate) ## Warning: package \u0026#39;reticulate\u0026#39; was built under R version 3.5.2 # Setup # for working with timestamps import pandas as pd from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() # for basic math import numpy as np # for plotting import matplotlib.pyplot as plt import seaborn as sns # for working with distributions from scipy.stats import expon,gamma,poisson,nbinom # for general web data pulling import requests # for pulling tweets from Twitter API import twitter # keys for twitter API # (removed for this public document) api = twitter.Api(consumer_key=\u0026#39;\u0026#39;, consumer_secret=\u0026#39;\u0026#39;, access_token_key=\u0026#39;\u0026#39;, access_token_secret=\u0026#39;\u0026#39;) # for saving and loading Python objects like dicts import pickle def save_obj(obj, name): with open(name + \u0026#39;.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f: pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL) def load_obj(name): with open(name + \u0026#39;.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as f: return pickle.load(f)  Twitter Data Pulling Data from Twitter’s API First I pulled data using the code below. For this code to work, your API keys will need to be specified in the setup above. To conceil my keys, I ran the commented code below earlier and saved the timeline object. The uncommented code loads my timeline and looks at the first element. The timeline is represented as a list of Status objects like the one output by the code below.\n# # Twitter handel to pull data from # handle = \u0026#39;@macstrelioff\u0026#39; # # # get timeline # timeline=api.GetUserTimeline(screen_name = handle, # count=200, # 200 is maximum # include_rts=True, # trim_user=True, # exclude_replies=False) # # # save timeline object # save_obj(timeline,\u0026#39;timeline_macstrelioff_20190406\u0026#39;) # load timeline object timeline=load_obj(\u0026#39;timeline_macstrelioff_20190406\u0026#39;) timeline[0] # most recent tweet status object \u0026gt;\u0026gt;\u0026gt; Status(ID=1113860296458756097, ScreenName=None, Created=Thu Apr 04 17:46:03 +0000 2019, Text=\u0026quot;@vboykis df.dropna(how=\u0026#39;brute force\u0026#39;) https://t.co/QOULUc5a0u\u0026quot;)  Working with Twitter Status objects I cover three ways to work with Status objects. 1. display the Status as a tweet! 2. Convert the Status to a dictionary and access values from keys 3. Access values directly as attributes of the Status\n Embed a Status First, many the Status attributes (created_at, favorite_count, text, …) can be cleanly displayed as a tweet embedded in a notebook. Below I create function that takes a username and tweet ID then, using Twitter’s embedding API, displayes the tweet as it would be seen on Twitter. (Note: this will only work properly if the Python kernel is trusted)\n# for displaying tweets based on username and tweet_id class disp_tweet(object): def __init__(self, user_name, tweet_id): # see: https://dev.twitter.com/web/embedded-tweets api = \u0026#39;https://publish.twitter.com/oembed?url=https://twitter.com/\u0026#39;+ \\ user_name + \u0026#39;/status/\u0026#39; + tweet_id response = requests.get(api) self.text = response.json()[\u0026quot;html\u0026quot;] def _repr_html_(self): return self.text disp_tweet(user_name=\u0026#39;macstrelioff\u0026#39;,tweet_id=\u0026#39;981338927419109376\u0026#39;) \u0026gt;\u0026gt;\u0026gt; \u0026lt;__main__.disp_tweet object at 0x1a23c269b0\u0026gt; If run from a Jupyter notebook, this should embed a tweet as below;\nMy desk is covered in random papers. It is the support of a stationery distribution. — mac strelioff (@macstrelioff) April 4, 2018    Convert a Status to a dict Status objects have a bound method, .AsDict(), that will convert them to a Python dictionary. This way the structure of the information is easily seen. In the code below, I convert the first status to a dictionary and output it contents.\nprint(timeline[0].AsDict()) \u0026gt;\u0026gt;\u0026gt; {\u0026#39;created_at\u0026#39;: \u0026#39;Thu Apr 04 17:46:03 +0000 2019\u0026#39;, \u0026#39;favorite_count\u0026#39;: 20, \u0026#39;hashtags\u0026#39;: [], \u0026#39;id\u0026#39;: 1113860296458756097, \u0026#39;id_str\u0026#39;: \u0026#39;1113860296458756097\u0026#39;, \u0026#39;in_reply_to_screen_name\u0026#39;: \u0026#39;vboykis\u0026#39;, \u0026#39;in_reply_to_status_id\u0026#39;: 1113822568211996672, \u0026#39;in_reply_to_user_id\u0026#39;: 19304217, \u0026#39;lang\u0026#39;: \u0026#39;da\u0026#39;, \u0026#39;media\u0026#39;: [{\u0026#39;display_url\u0026#39;: \u0026#39;pic.twitter.com/QOULUc5a0u\u0026#39;, \u0026#39;expanded_url\u0026#39;: \u0026#39;https://twitter.com/macstrelioff/status/1113860296458756097/photo/1\u0026#39;, \u0026#39;id\u0026#39;: 1113860285566046210, \u0026#39;media_url\u0026#39;: \u0026#39;http://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg\u0026#39;, \u0026#39;media_url_https\u0026#39;: \u0026#39;https://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg\u0026#39;, \u0026#39;sizes\u0026#39;: {\u0026#39;thumb\u0026#39;: {\u0026#39;w\u0026#39;: 150, \u0026#39;h\u0026#39;: 150, \u0026#39;resize\u0026#39;: \u0026#39;crop\u0026#39;}, \u0026#39;large\u0026#39;: {\u0026#39;w\u0026#39;: 250, \u0026#39;h\u0026#39;: 198, \u0026#39;resize\u0026#39;: \u0026#39;fit\u0026#39;}, \u0026#39;medium\u0026#39;: {\u0026#39;w\u0026#39;: 250, \u0026#39;h\u0026#39;: 198, \u0026#39;resize\u0026#39;: \u0026#39;fit\u0026#39;}, \u0026#39;small\u0026#39;: {\u0026#39;w\u0026#39;: 250, \u0026#39;h\u0026#39;: 198, \u0026#39;resize\u0026#39;: \u0026#39;fit\u0026#39;}}, \u0026#39;type\u0026#39;: \u0026#39;animated_gif\u0026#39;, \u0026#39;url\u0026#39;: \u0026#39;https://t.co/QOULUc5a0u\u0026#39;, \u0026#39;video_info\u0026#39;: {\u0026#39;aspect_ratio\u0026#39;: [125, 99], \u0026#39;variants\u0026#39;: [{\u0026#39;bitrate\u0026#39;: 0, \u0026#39;content_type\u0026#39;: \u0026#39;video/mp4\u0026#39;, \u0026#39;url\u0026#39;: \u0026#39;https://video.twimg.com/tweet_video/D3U6BzqUUAIwYEC.mp4\u0026#39;}]}}], \u0026#39;retweet_count\u0026#39;: 2, \u0026#39;source\u0026#39;: \u0026#39;\u0026lt;a href=\u0026quot;http://twitter.com/download/android\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for Android\u0026lt;/a\u0026gt;\u0026#39;, \u0026#39;text\u0026#39;: \u0026quot;@vboykis df.dropna(how=\u0026#39;brute force\u0026#39;) https://t.co/QOULUc5a0u\u0026quot;, \u0026#39;urls\u0026#39;: [], \u0026#39;user\u0026#39;: {\u0026#39;id\u0026#39;: 70255183, \u0026#39;id_str\u0026#39;: \u0026#39;70255183\u0026#39;}, \u0026#39;user_mentions\u0026#39;: [{\u0026#39;id\u0026#39;: 19304217, \u0026#39;id_str\u0026#39;: \u0026#39;19304217\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;Vicki Boykis\u0026#39;, \u0026#39;screen_name\u0026#39;: \u0026#39;vboykis\u0026#39;}]} {\u0026#39;created_at\u0026#39;: \u0026#39;Thu Apr 04 17:46:03 +0000 2019\u0026#39;, \u0026#39;favorite_count\u0026#39;: 20, \u0026#39;hashtags\u0026#39;: [], \u0026#39;id\u0026#39;: 1113860296458756097, \u0026#39;id_str\u0026#39;: \u0026#39;1113860296458756097\u0026#39;, \u0026#39;in_reply_to_screen_name\u0026#39;: \u0026#39;vboykis\u0026#39;, \u0026#39;in_reply_to_status_id\u0026#39;: 1113822568211996672, \u0026#39;in_reply_to_user_id\u0026#39;: 19304217, \u0026#39;lang\u0026#39;: \u0026#39;da\u0026#39;, \u0026#39;media\u0026#39;: [{\u0026#39;display_url\u0026#39;: \u0026#39;pic.twitter.com/QOULUc5a0u\u0026#39;, \u0026#39;expanded_url\u0026#39;: \u0026#39;https://twitter.com/macstrelioff/status/1113860296458756097/photo/1\u0026#39;, \u0026#39;id\u0026#39;: 1113860285566046210, \u0026#39;media_url\u0026#39;: \u0026#39;http://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg\u0026#39;, \u0026#39;media_url_https\u0026#39;: \u0026#39;https://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg\u0026#39;, \u0026#39;sizes\u0026#39;: {\u0026#39;thumb\u0026#39;: {\u0026#39;w\u0026#39;: 150, \u0026#39;h\u0026#39;: 150, \u0026#39;resize\u0026#39;: \u0026#39;crop\u0026#39;}, \u0026#39;large\u0026#39;: {\u0026#39;w\u0026#39;: 250, \u0026#39;h\u0026#39;: 198, \u0026#39;resize\u0026#39;: \u0026#39;fit\u0026#39;}, \u0026#39;medium\u0026#39;: {\u0026#39;w\u0026#39;: 250, \u0026#39;h\u0026#39;: 198, \u0026#39;resize\u0026#39;: \u0026#39;fit\u0026#39;}, \u0026#39;small\u0026#39;: {\u0026#39;w\u0026#39;: 250, \u0026#39;h\u0026#39;: 198, \u0026#39;resize\u0026#39;: \u0026#39;fit\u0026#39;}}, \u0026#39;type\u0026#39;: \u0026#39;animated_gif\u0026#39;, \u0026#39;url\u0026#39;: \u0026#39;https://t.co/QOULUc5a0u\u0026#39;, \u0026#39;video_info\u0026#39;: {\u0026#39;aspect_ratio\u0026#39;: [125, 99], \u0026#39;variants\u0026#39;: [{\u0026#39;bitrate\u0026#39;: 0, \u0026#39;content_type\u0026#39;: \u0026#39;video/mp4\u0026#39;, \u0026#39;url\u0026#39;: \u0026#39;https://video.twimg.com/tweet_video/D3U6BzqUUAIwYEC.mp4\u0026#39;}]}}], \u0026#39;retweet_count\u0026#39;: 2, \u0026#39;source\u0026#39;: \u0026#39;\u0026lt;a href=\u0026quot;http://twitter.com/download/android\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;Twitter for Android\u0026lt;/a\u0026gt;\u0026#39;, \u0026#39;text\u0026#39;: \u0026quot;@vboykis df.dropna(how=\u0026#39;brute force\u0026#39;) https://t.co/QOULUc5a0u\u0026quot;, \u0026#39;urls\u0026#39;: [], \u0026#39;user\u0026#39;: {\u0026#39;id\u0026#39;: 70255183, \u0026#39;id_str\u0026#39;: \u0026#39;70255183\u0026#39;}, \u0026#39;user_mentions\u0026#39;: [{\u0026#39;id\u0026#39;: 19304217, \u0026#39;id_str\u0026#39;: \u0026#39;19304217\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;Vicki Boykis\u0026#39;, \u0026#39;screen_name\u0026#39;: \u0026#39;vboykis\u0026#39;}]}  Access Status Attributes (How I accessed the data used below) Since I’m interested in modeling expected number of tweets in a week, the most relevant attribute is the timestamps in the created_at attribute. These attributes can be accessed directly from the Status object. Below I make a list of the times at which each tweet was created and check the first element of that list;\n# get list of time stamps times = [pd.Timestamp(tweet.created_at) for tweet in timeline] times.reverse() # sort s.t. times[0] is lowest, times[-1] is highest times[0] \u0026gt;\u0026gt;\u0026gt; Timestamp(\u0026#39;2018-11-01 05:56:58+0000\u0026#39;, tz=\u0026#39;tzutc()\u0026#39;) This creates a list of timestamps. By default, times from the twitter API are localized to the UTC timezone. Below I convert these to my local time in California;\ntimes = [time.tz_convert(\u0026quot;America/Los_Angeles\u0026quot;) for time in times] times[0] \u0026gt;\u0026gt;\u0026gt; Timestamp(\u0026#39;2018-10-31 22:56:58-0700\u0026#39;, tz=\u0026#39;America/Los_Angeles\u0026#39;) Now we have a list of timestamps in local time! To get a sense of the duration over which this data spans, below I compute the time difference between the frist and last timestamp;\ntimes[-1]-times[0] \u0026gt;\u0026gt;\u0026gt; Timedelta(\u0026#39;154 days 11:49:05\u0026#39;) Woah, almost 155 days of my twitter activity!\n  Poisson Process A Poisson process is a common framework for modeling events that occurr in time or space. In this context, tweets are being created over time and we are interested in modeling the rate at which tweets are created in order to predict how many tweets will be created in a week.\nAssumptions No more than one event can occur at a single point in time.  This can be violated when a user publishes a thread of multiple tweets at once. This can be fixed by recoding threads as a single status.  Independence: The interval lengths for each event are not influenced by any other event.  This can be violated if, instead of using Twitter’s thread option, a user ends a tweet with “…” to indicate that they will soon create another tweet. In this case, there are some tweets that imply a shorter interval before the next tweet.  Homogeneity: The distribution of intervals is the same throughout the entire process.  I probe this assumption in depth below, and it almost certainly violated. There are methods for modeling inhomogeneous Poisson processes, but I ignore those here.    Specification and Properties In this context tweet events are occurring across time. I index tweets with \\(i\\in\\{1,...,N\\}\\), where \\(N\\) is the total number of tweets observed. Each tweet is created at a time, \\(t_i\\), and the next tweet is observed after an interval \\(s_{i}\\). That is, if tweet \\(i\\) is created at time \\(t_{i-1}\\) then tweet \\(i+1\\) is created at time \\(t_{i}=t_{i-1}+s_{i}\\). The interval between each tweet is \\(s_i = t_{i}-t_{i-1} = (t_{i-1}+s_i)-t_{i-1}\\). The assumptions of a Poisson process permit the following distributions for three interesting features of this scenario.\nThe distribution of time between events, \\(s_i\\), is exponential; \\(s_i\\sim Expo(\\lambda) \\Rightarrow p(s_i|\\lambda) = \\lambda e^{-\\lambda s_i}\\)  Here \\(\\lambda\\) is a parameter that describes the tweet rate.  Given an interval of length \\(s\\), the distribution of the count of events in that interval, \\(c|s\\), is Poisson; \\(c|s\\sim Poisson(\\lambda s) \\Rightarrow p(c|s,\\lambda) = \\frac{(\\lambda s)^{c}e^{-\\lambda s}}{c!}\\)  The count of events, \\(c\\), in a fixed interval depends both on the rate of the events, \\(\\lambda\\), and the duration of the interval, \\(s\\).  The distribution of the total interval required for \\(c\\) events, \\(s|c\\), is gamma; \\(s|c \\sim Gamma(c,\\lambda) \\Rightarrow p(s|c,\\lambda) = \\frac{\\lambda^c}{\\Gamma(c)}(s)^{c-1}e^{-\\lambda s}\\)  The interval, \\(s\\), required for a fixed number of events depends on both the rate of the events, \\(\\lambda\\), and the number of events, \\(c\\).   More information on these three kinds of distributions, and ways to implement them in Python, can be found in the scipy documentation on statistical functions. General information on each of these distributions can be found on the Wikipedia page for the exponential, Poisson, or gamma distribution. A key difference between the standard uses of these distributions and their roles in a Poisson process is that the rate parameter \\(\\lambda\\) is also scaled by the duration of an interval \\(s\\) when constructing a distribution for the count of events in interval \\(s\\) (2, above) or the duration of the interval required for \\(c\\) events (3, above).\n Checking the Homogeneity Assumption The homogeneity assumption strictly requires that tweet rates are constant across time. This would generate data that are uniform across meaningful intervals such as time in a week or time in a day. To check homogeneity, below I convert the timestamps into the hour within a week, minute within a day, and minute within an hour, and plot tweet counts across these representations of time.\n# convert to hours in a week hour_of_week = [t.weekday()*24+t.hour+t.minute/60 for t in times] minute_of_day = [t.hour*60+t.minute+t.second/60 for t in times] minute_of_hour = [t.minute+t.second/60 for t in times] plt.figure(figsize=(10,4)); plt.hist(hour_of_week,bins=80); plt.title(\u0026quot;My Tweet Counts By Hour Of Week\u0026quot;); plt.ylabel(\u0026quot;Count\u0026quot;); plt.xlabel(\u0026quot;Hour In Week\u0026quot;); The histogram above indicates that there might be some hours of the week that have a higher rate than others. For example, I don’t seem to tweet much early on Sunday (hours 0-5), but I do seem to tweet a lot during the day on Sunday (around hours 6-20). Below I use rug plots, which represent a tweet event with a vertical line near the x-axis, and an imposed kernel density estimate, which is a continuous version of a histogram. I remake this plot in terms of hours within a week, minutes within a day, and minutes within an hour. If the tweet rate (\\(\\lambda\\)) were homogeneous, then the kernel density estimate would be approximately flat.\ndef rug_plot_and_density(dat,bw,xlab,xlim): # rug plot + density plt.figure(figsize=(10,4)) sns.distplot(dat, hist = False, kde = True, rug = True, color = \u0026#39;darkblue\u0026#39;, kde_kws={\u0026#39;linewidth\u0026#39;: 3,\u0026quot;bw\u0026quot;:bw}, rug_kws={\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;}) # formatting plt.title(\u0026#39;Tweet Density By \u0026#39;+xlab) plt.xlabel(xlab) plt.ylabel(\u0026#39;Kernel Density\u0026#39;) plt.xlim(xlim); plt.show() # data and plot formatting arguments dats = (hour_of_week,minute_of_day,minute_of_hour) xlabs=(\u0026#39;Hour of Week\u0026#39;,\u0026#39;Minute of Day\u0026#39;,\u0026#39;Minute of Hour\u0026#39;) xlims=([0,24*7],[0,24*60],[0,60]); bws = (4,40,4) # make plots for dat,xlab,xlim,bw in zip(dats,xlabs,xlims,bws): rug_plot_and_density(dat=dat,bw=bw,xlab=xlab,xlim=xlim) Are very sensitive to the choice of the bandwidth parameter that determines the window over which to aggregate events (similar to bin size when using a histogram). I’m using these plots to demonstrate possible violations of homogeneity that I would follow up on in a real analysis, but will not follow up on here.\nFrom the top plot, there seems to be two patterns. First, a series of peaks and troughs that roughly correspond to daytime and night-time hours. I probably tweet with a higher frequency when I am awake, rather than asleep – meaning that \\(\\lambda\\) may depend on time within a day. Second, a generally lower kernel density estimate during the middle than the edges. I may tweet more during the weekends (edge hours) than week days (middle hours).\nFrom the middle plot that displays tweet frequencies by minutes within a day, there again seem to be two trends. First, I rarely tweet before minute 400 (around 6:40AM). Second, I have peaks around minute 600 (10:00AM), 1000 (4:40PM), and 1350 (10:00PM). This might be related to the times that I take a break from working. I generally take a break around 5:00PM, and usually take another break before bed around 9:00-10:00PM.\nThe bottom plot displays tweet frequencies by minutes within an hour. This seems more flat overall longer periods of time, but I may strangely tend to tweet more during the first half of hours.\nOverall, there are many reasons that the homogeneity assumption may be violated. For cases like this, the tweet rate \\(\\lambda\\) can be modeled as a function of time. However, to keep this example simple, I’ll ignore possible violations and proceede as if \\(\\lambda\\) were a constant with respect to time.\n Checking the exponential distribution of intervals Let’s the distribution of the time between tweets, \\(s_i\\). If the assumptions of the Poisson process were satisfied, then the intervals between tweets would follow an exponential distribution. Below I compute the number of seconds between tweets and display each value as a black dash on the x-axis. I overlay a histogram, a kernel density, and a exponential density based on the observed mean interval.\n# compute intervals between tweets ss = [times[i]-times[i-1] for i in range(1,len(times))]; # convert from Timedelta to total time in seconds ss = [si.total_seconds() for si in ss] # histogram and density plot plt.figure(figsize=(10,4)) sns.distplot(ss, hist = True, kde = True, rug = True, color = \u0026#39;darkblue\u0026#39;, bins=100, hist_kws={\u0026#39;color\u0026#39;:[0,.7,.5,.5],\u0026#39;label\u0026#39;:\u0026#39;Histogram\u0026#39;}, kde_kws={\u0026#39;linewidth\u0026#39;: 3,\u0026quot;bw\u0026quot;:60*60,\u0026#39;label\u0026#39;:\u0026#39;Kernel Density\u0026#39;}, rug_kws={\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;}) plt.xlim([0,680000]); plt.title(\u0026#39;Distribution of intervals between tweets\u0026#39;) plt.xlabel(\u0026#39;Seconds\u0026#39;) plt.ylabel(\u0026#39;Frequency\u0026#39;) # overlay an exponential density tmp_rate=np.mean(ss) tmpx = np.linspace(0,680000,680000*5) tmpy = expon.pdf(tmpx,scale=tmp_rate) plt.plot(tmpx,tmpy,color=[.7,0,0,1],linewidth=3,label=\u0026#39;Exponential Density\u0026#39;); # add legend plt.legend(); Based on the relative heights of the kernel density and the exponential density, there seem to be more short intervals, fewer moderate length intervals, and more long intervals relative to the exponential distribution. The mean and standard deviation of an exponential distribution should be the same value. Below I check the standard deviaion and mean of the observed intervals.\n# check standard deviation np.std(ss),np.mean(ss),np.std(ss)/np.mean(ss) \u0026gt;\u0026gt;\u0026gt; (90150.9122414953, 67076.1055276382, 1.3440093388300445) The observed standard deviation is about 1.34 times larger (variance is about 1.80 times larger) than it would be if the data were exponentially distributed with the observed mean. While this could be accounted for with an overdispersion parameter, I will ignore this issue here for the sake of having a simple and fast online model.\n  Model of Tweet Fequency Using Conjugacy First, I’ll assume (despite the overdispersion) that the intervals between tweets follow an exponential distribution;\n\\[ \\begin{aligned} s_i|\\lambda \u0026amp;\\sim Expo(\\lambda) \\\\ \\Rightarrow p(s_i|\\lambda) \u0026amp;= \\lambda e^{-s_i \\lambda} \\end{aligned} \\]\nTo account for uncertainty in \\(\\lambda\\), I’ll use a Gamma distribution with shape \\(\\alpha\\) and rate \\(\\beta\\);\n\\[ \\begin{aligned} \\lambda \u0026amp;\\sim Gamma(\\alpha,\\beta) \\\\ \\Rightarrow p(\\lambda|\\alpha,\\beta) \u0026amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1}e^{-\\lambda \\beta} \\end{aligned} \\]\nThe choice of a Gamma distribution allows for fast updates using conjugacy between the prior beliefs about \\(\\lambda\\) based on data observed up to time \\(t\\) and the exponential likelihood for the interval observed at time \\(t\\);\n\\[ \\begin{aligned} \\lambda | s_t \u0026amp;\\sim Gamma(\\alpha_t,\\beta_t) \\\\ p(s_{t+1}|\\lambda) \u0026amp;= \\lambda e^{-s_{t+1} \\lambda} \\\\ p(\\lambda|\\alpha_t,\\beta_t,s_{t+1}) \u0026amp;\\propto p(s_{t+1}|\\lambda) p(\\lambda|\\alpha_t,\\beta_t) \\\\ \u0026amp;= \\lambda e^{-s_{t+1} \\lambda} \\frac{\\beta_t^{\\alpha_t}}{\\Gamma(\\alpha_t)} \\lambda^{\\alpha_t-1}e^{-\\lambda \\beta_t} \\\\ \u0026amp;= \\lambda^{\\alpha_t} e^{-\\lambda(s_{t+1}+\\beta_t)} \\\\ \\Rightarrow \\lambda | s_{t+1} \u0026amp;\\sim Gamma(\\alpha_t+1,\\beta_t+s_{t+1}) \\end{aligned} \\]\nThis implies the following update rules for computing the parameters of the posterior over \\(\\lambda\\);\n\\[ \\begin{aligned} \\alpha_{t+1} \u0026amp;\\leftarrow \\alpha_t +1 \\\\ \\beta_{t+1} \u0026amp;\\leftarrow \\beta_t + s_{t+1} \\\\ \\end{aligned} \\]\nTo answer the question of how many tweets might be observed in a period of time, I’ll assume that the count of tweets \\(c\\) is Poisson distributed with rate \\(\\lambda\\). Then the number of tweets expected in an interval of length \\(s\\) would be;\n\\[ \\begin{aligned} \\theta \u0026amp;= s\\lambda \\\\ c|\\theta \u0026amp;\\sim Poisson(\\theta) \\\\ \\Rightarrow p(c|\\theta) \u0026amp;= \\frac{\\theta^c e^{-\\theta}}{c!} \\end{aligned} \\]\nThis means that, rather than \\(\\lambda\\), we are actually interested in the distribution of \\(\\theta=s\\lambda\\). I derive this below using a change of vairables;\n\\[ \\begin{aligned} \\theta = s\\lambda \u0026amp;\\Rightarrow \\lambda = \\frac{\\theta}{s} = \\theta s^{-1} \\\\ p_\\theta(\\theta | \\lambda,s) \u0026amp;= p_\\lambda\\left(\\theta s^{-1} | s,\\alpha,\\beta\\right) \\left| \\frac{d \\lambda}{d \\theta}\\right|\\\\ \u0026amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left(\\frac{\\theta}{s}\\right)^{\\alpha-1}e^{-\\frac{\\theta}{s}\\beta} \\left|s^{-1}\\right| \\\\ \u0026amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{(\\alpha-1)}s^{-({\\alpha-1})-1}e^{-\\frac{\\theta}{s}\\beta} \\\\ \\Rightarrow p(\\theta|s,\\alpha,\\beta)\u0026amp;= \\frac{\\left(\\frac{\\beta}{s}\\right)^\\alpha}{\\Gamma(\\alpha)} \\theta^{(\\alpha-1)}e^{-\\theta\\frac{\\beta}{s}}\\\\ \\end{aligned} \\]\nNow we can find the distribution of \\(c\\) that accounts for uncertainty in \\(\\theta\\) through the prior on \\(\\lambda\\);\n\\[ \\begin{aligned} p(c|s,\\alpha,\\beta) \u0026amp;= \\int_\\theta p(c|\\theta)p(\\theta|s,\\alpha,\\beta)d\\theta \\\\ \u0026amp;=\\int_\\theta \\frac{\\theta^c e^{-\\theta}}{c!} \\frac{\\left(\\frac{\\beta}{s}\\right)^\\alpha}{\\Gamma(\\alpha)} \\theta^{(\\alpha-1)}e^{-\\theta\\frac{\\beta}{s}} d\\theta \\\\ \u0026amp;= \\frac{\\left(\\frac{\\beta}{s}\\right)^\\alpha}{c!\\Gamma(\\alpha)} \\int_\\theta \\theta^c e^{-\\theta} \\theta^{(\\alpha-1)}e^{-\\theta\\frac{\\beta}{s}} d\\theta \\\\ \u0026amp;=\\frac{\\left(\\frac{\\beta}{s}\\right)^\\alpha}{c!\\Gamma(\\alpha)} \\int_\\theta \\theta^{c+\\alpha-1} e^{-\\theta\\left(\\frac{\\beta+s}{s}\\right)} d\\theta \\\\ \u0026amp;=\\frac{\\left(\\frac{\\beta}{s}\\right)^\\alpha}{c!\\Gamma(\\alpha)} \\frac{\\Gamma(c+\\alpha)}{\\left(\\frac{\\beta+s}{s}\\right)^{c+\\alpha}} \\\\ \u0026amp;=\\frac{\\Gamma(c+\\alpha)}{c!\\Gamma(\\alpha)} \\beta^\\alpha s^{-\\alpha} s^{c+\\alpha} (\\beta+s)^{-(c+\\alpha)} \\\\ \u0026amp;=\\frac{\\Gamma(c+\\alpha)}{\\Gamma(c+1)\\Gamma(\\alpha)} \\left(\\frac{s}{\\beta+s}\\right)^c \\left(\\frac{\\beta}{\\beta+s}\\right)^\\alpha \\\\ \\end{aligned} \\]\nSince a Poisson Process assumes that no more than one event can occur in an interval, intervals can be treated as discrete Bernoulli trials in which an event either occurs or does not occur. In this discrete settng, the distribution of the count of intervals with an event \\(k\\) for a given number of intervals without the event \\(r\\) and a probability of the event in each interval \\(p\\) will follow a negative binomial distribution;\n\\[ \\begin{aligned} k \u0026amp;\\sim NegBino(r,p) \\\\ \\Rightarrow p(k|r,p) \u0026amp;= \\frac{(k+r-1)!}{r! (k-1)!}p^{k}(1-p)^{r} \\\\ \u0026amp;= \\frac{\\Gamma(k+r)}{\\Gamma(r+1) \\Gamma(k)}p^{k}(1-p)^{r} \\\\ \\end{aligned} \\]\nCombining the two results above, we see that the count \\(c\\) in an interval \\(s\\) will follow a negative binomial distribution such that \\(\\alpha\\) fixes the number of intervals in which no event occurs, and \\(\\frac{s}{\\beta+s}\\) captures the probability of an event in any unit length interval;\n\\[ \\begin{aligned} c \u0026amp;\\sim NegBino\\left(\\alpha,\\frac{s}{\\beta+s}\\right) \\\\ \\Rightarrow p(c|s,\\alpha,\\beta) \u0026amp;= \\frac{\\Gamma(c+\\alpha)}{\\Gamma(c+1)\\Gamma(\\alpha)} \\left(\\frac{s}{\\beta+s}\\right)^c \\left(\\frac{\\beta}{\\beta+s}\\right)^\\alpha \\\\ \\end{aligned} \\]\nIn summary, the key components of this model are the exponential likelihood and gamma priors which allow for the fast and simple updating rules to compute the posterior over \\(\\lambda\\), and the negative binomial predictive distribution which accounts for the uncertainty in \\(\\lambda\\). So the applicable information from above is;\n\\[ \\begin{aligned} \\lambda \u0026amp;\\sim Gamma(\\alpha_t,\\beta_t) \\\\ s_{t+1} \u0026amp;\\sim Expo(\\lambda) \\\\ \\alpha_{t+1} \u0026amp;\\leftarrow \\alpha_t + 1 \\\\ \\beta_{t+1} \u0026amp;\\leftarrow \\beta_t + s_{t+1} \\\\ c|s,\\alpha,\\beta \u0026amp;\\sim NegBino\\left(\\alpha,\\frac{s}{\\beta+s}\\right) \\\\ \\end{aligned} \\]\nHere I investigate posterior predictive intervals for tweets over a period of time \\(s\\) while using different components of this model. In the code below, I compute all values of \\(\\alpha\\) and \\(\\beta\\) based on the update rules derived above from conjugacy.\n# initialize alpha,beta as 0 alpha,beta = 0,0 alphas,betas = list(),list() # for each observed interval in ss, for si in ss: si = si/(60*60*24) # si s/1 * 1/60 m/s * 1/60 h/m * 1/24 d/h convert to days alpha+=1 # increment alpha by 1 beta+=si # increment beta by the interval length # save parameters for analysis alphas.append(alpha) betas.append(beta) The code above converts intervals from seconds to days and comptutes \\(\\alpha\\) and \\(\\beta\\) for all intervals in my dataset. This gives the parameters for posterior beliefs over \\(\\lambda\\) as each tweet is observed. Below I check the final parameters and some statistics from the last posterior.\na,b,mode,median,mean=(alpha,beta,(alpha-1)/beta,gamma.ppf(.5,a=alpha,scale=1/beta),alpha/beta) print( \u0026#39;alpha : \u0026#39;+str(a)+ \u0026#39;\\nbeta : \u0026#39;+str(b)+ \u0026#39;\\nmode : \u0026#39;+str(mode)+ \u0026#39;\\nmedian: \u0026#39;+str(median)+ \u0026#39;\\nmean : \u0026#39;+str(mean)) \u0026gt;\u0026gt;\u0026gt; alpha : 199 \u0026gt;\u0026gt;\u0026gt; beta : 154.49241898148153 \u0026gt;\u0026gt;\u0026gt; mode : 1.2816162845099446 \u0026gt;\u0026gt;\u0026gt; median: 1.2859321345366828 \u0026gt;\u0026gt;\u0026gt; mean : 1.2880890940276715 The value of \\(\\alpha\\) correctly indicates 199 observed intervals, and the \\(\\beta\\) of 154.59 also correctly reflects the time difference in days that was computed in the first section. Lastly the ordinal relationship of the mode, median, and mean is consistent with that of a gamma distribution.\nInference On Tweet Rate \\(\\lambda\\) Over Time In the code below, I compute the maximum a posteriori (MAP) estimate, or posterior mode, across time. I plot this across time with the 97.5\\(^{th}\\) and 2.5\\(^{th}\\) percentiles as a shaded region representing the 95% credible interval.\n# get MAP lambda lambda_map = [(alpha-1)/beta for alpha,beta in zip(alphas,betas)] # get CI upper and lower bounds lambda_upper = [gamma.ppf(.975,a=alpha,scale=1/beta) for alpha,beta in zip(alphas,betas)] lambda_lower = [gamma.ppf(.025,a=alpha,scale=1/beta) for alpha,beta in zip(alphas,betas)] # plot over time plt.figure(figsize=(10,4)) sns.distplot(times[1:], hist = False, kde = False, rug = True, color = \u0026#39;darkblue\u0026#39;, rug_kws={\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;}); plt.plot(times[1:],lambda_map,color=[0,0,1],label=\u0026#39;MAP\u0026#39;); plt.fill_between(times[1:], lambda_lower, lambda_upper,color=[0,0,1,.1],label=\u0026#39;95% CI\u0026#39;) plt.ylabel(\u0026#39;Posterior on Lambda (tweets/day)\u0026#39;) plt.xlabel(\u0026#39;Time\u0026#39;) plt.title(\u0026#39;Summary of the posterior over $\\lambda$ across time\u0026#39;) plt.legend(); The posterior seems to tighten dramatically around the mode during the first month, and the mode seems to stabalize after about that much time. However, this method seems slow to adjust to the slower rate of tweets in 2019. One way to address this might be to add weights to the parameter updates such that \\(\\alpha\\) and \\(\\beta\\) are more sensitive to recent data than past data. These weights (or stepsize or learning rates) can be based on the surprisal, or likelihood, of a new tweet under the posterior. If the new interval was well anticipated, then little updating is needed, but if the new interval was very surprising or unlikely, then a sharpe change in \\(\\alpha\\) and \\(\\beta\\) might be warrented.\n  Predicting Number Of Tweets In Interval \\(s\\) Now I’ll visually compare predictions for the count of tweets in a week based on estimates of \\(\\lambda\\) after the first 20 tweets and using all 200 tweets, using two models; 1. A Poisson using \\(\\theta=s\\lambda_{MAP}\\) 2. The negative binomial derived in the model section above\nFirst, an issue with the parameterization of the negative binomial has to be addressed. In my derivation, I ended with a negative binomial parameterized as;\n\\[ \\begin{aligned} k\u0026amp;:\\text{Number of successes} \\\\ r\u0026amp;:\\text{Number of failures} \\\\ p\u0026amp;:\\text{Probability of failure} \\\\ p(k|r,p) \u0026amp;= \\frac{(k+r-1)!}{r! (k-1)!}p^{k}(1-p)^{r} \\\\ \\end{aligned} \\]\nThe Scipy.stats.nbinom function defines a negative binomial with;\n\\[ \\begin{aligned} k\u0026amp;:\\text{Number of failures} \\\\ n\u0026amp;:\\text{Number of successes} \\\\ p\u0026amp;:\\text{Probability of success} \\\\ p(k|n,p) \u0026amp;= \\frac{(k+n-1)!}{(n-1)!k!} p^{n}(1-p)^{k} \\end{aligned} \\]\nRecall in my derivation that the parameter corresponding to probability of failure was \\(p=\\frac{s}{\\beta+s}\\). The differences in parameterization can be accounted for by supplying the nbinom function with \\(p=1-\\frac{s}{\\beta+s}=\\frac{\\beta}{\\beta+s}\\).\nThe code below plots predictive densities based on the two approaches above from estimates of \\(\\lambda\\) based only on the first 20 observations.\n# for the first 20 tweets # parameters cs = range(20) s = 7 index = 19 theta = s*lambda_map[index] plt.figure(figsize=(10,4)) # plot the Poisson density plt.scatter(cs,poisson.pmf(cs,theta)) plt.plot( cs,poisson.pmf(cs,theta),label=\u0026#39;Poisson\u0026#39;) # plot the negative binomial density plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index]))) plt.plot( cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=\u0026#39;Negative Binomial\u0026#39;); # labels plt.ylabel(\u0026#39;Mass\u0026#39;) plt.xlabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Predictions for Tweet Counts in a Week, Using 20 Observations\u0026#39;) plt.legend(); With a small amount of data, the uncertainty in the estimate of tweet rates carries through the negative binomial model, which gives more mass to a wider range of counts relative to the Poisson model that discards uncertainty when by \\(\\lambda_{MAP}\\).\n# Using all data # parameters index = 198 theta = s*lambda_map[index] plt.figure(figsize=(10,4)) # plot the Poisson density plt.scatter(cs,poisson.pmf(cs,theta)) plt.plot( cs,poisson.pmf(cs,theta),label=\u0026#39;Poisson\u0026#39;) # plot the negative binomial density plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index]))) plt.plot( cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=\u0026#39;Negative Binomial\u0026#39;); # labels plt.ylabel(\u0026#39;Mass\u0026#39;) plt.xlabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Predictions for Tweet Counts in a Week, Using All Data\u0026#39;) plt.legend(); With a larger amount of data and predicting the tweet count over a short interval (7 days), the predictive distribution from the negative binomial and Poisson models are nearly indistinguishable. Below I compare the predictions of these model for an interval of 1 year (365 days), again using all data.\n# Using all data # parameters cs=range(350,600) s = 365 index = 198 theta = s*lambda_map[index] plt.figure(figsize=(10,4)) # plot the Poisson density plt.scatter(cs,poisson.pmf(cs,theta)) plt.plot( cs,poisson.pmf(cs,theta),label=\u0026#39;Poisson\u0026#39;) # plot the negative binomial density plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index]))) plt.plot( cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=\u0026#39;Negative Binomial\u0026#39;); # labels plt.ylabel(\u0026#39;Mass\u0026#39;) plt.xlabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Predictions for Tweet Counts in a Year, Using All Data\u0026#39;) plt.legend(); When considering a longer period of time, The uncertainty in \\(\\lambda\\) again carries through the negative binomial model, but is discarded in the Poisson model that uses \\(\\lambda_{MAP}\\).\n Summary This was a beefy notebook!\nFirst I showed how to pull tweets as Status objects from Twitter’s API. Given Status objects, I then showed how to embed them in a notebook, view them as a dictionary, and otherwise access their attributes to construct a list of tweet timestamps. I then described Poisson Processes as they may apply to modeling user tweet rates and tweet counts over periods of time. I used kernel densities to check assumptions of a Poisson Process and found possible violations of homogeneity, and of the result that intervals between tweets should follow an exponential distribution.\nPunting these violations, I developed a model of tweet frequency using conjugacy between Gamma priors and Exponential likelihoods. To predict tweet counts over a period of time, I derived a negative binomial predictive distribution that accounted for uncertainty in a user’s tweet rate. I compared this distribution to a Poisson distribution that ignored uncertainty by taking only the maximum a posteriori estimate of a user’s tweet rate.\nOverall the model that discards posterior uncertainty attributes less mass to fringe counts, especially when there is little data or when the interval over which counts are being predicted is long. Incorporating posterior uncertainty broadens the predictive distribution to reflect uncertainty in the underlying tweet rate. That uncertainty exists regardless of the modeling approach – excluding it from predictive distributions only leads to narrow, overconfident predictions. This general principle of propagating uncertainty through a statistical process is one strong advantage of the Bayeian modeling approach that I developed and applied here.\n ","date":1557581836,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557581836,"objectID":"0b4907df5df827ce35a063ba893b6eb6","permalink":"/MacStrelioff/data-science/twitter-poisson-processes-and-conjugacy/","publishdate":"2019-05-11T13:37:16Z","relpermalink":"/MacStrelioff/data-science/twitter-poisson-processes-and-conjugacy/","section":"data-science","summary":"Background and Setup Twitter Data Pulling Data from Twitter’s API Working with Twitter Status objects Embed a Status Convert a Status to a dict Access Status Attributes (How I accessed the data used below)  Poisson Process Assumptions Specification and Properties Checking the Homogeneity Assumption Checking the exponential distribution of intervals  Model of Tweet Fequency Using Conjugacy Inference On Tweet Rate \\(\\lambda\\) Over Time  Predicting Number Of Tweets In Interval \\(s\\) Summary   Background and Setup In this notebook I focus on explaining Poisson processes and conjugacy applied to my Twitter activity.","tags":null,"title":"Twitter, Poisson Processes, and Conjugacy","type":"docs"},{"authors":["Mac Strelioff"],"categories":null,"content":" Supplementary notes can be added here, including code and math.   ","date":1554620400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554620400,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/MacStrelioff/publication/preprint/","publishdate":"2019-04-07T00:00:00-07:00","relpermalink":"/MacStrelioff/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Mac Strelioff"],"categories":[],"content":" from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and Jupyter Install Anaconda which includes Python 3 and Jupyter notebook.\nOtherwise, for advanced users, install Jupyter notebook with pip3 install jupyter.\nCreate a new blog post as usual Run the following commands in your Terminal, substituting \u0026lt;MY_WEBSITE_FOLDER\u0026gt; and my-post with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:\ncd \u0026lt;MY_WEBSITE_FOLDER\u0026gt; hugo new --kind post post/my-post cd \u0026lt;MY_WEBSITE_FOLDER\u0026gt;/content/post/my-post/  Create or upload a Jupyter notebook Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (New \u0026gt; Python Notebook) or upload a notebook.\njupyter notebook  Convert notebook to Markdown jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=. # Copy the contents of Untitled.md and append it to index.md: cat Untitled.md | tee -a index.md # Remove the temporary file: rm Untitled.md  Edit your post metadata Open index.md in your text editor and edit the title etc. in the front matter according to your preference.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/MacStrelioff/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/MacStrelioff/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/MacStrelioff/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/MacStrelioff/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Check back later\u0026hellip;\n --- ","date":1536476400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536476400,"objectID":"e7d4a56c1a53559b9d1870c69c485368","permalink":"/MacStrelioff/video-lectures/fun/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/MacStrelioff/video-lectures/fun/","section":"video-lectures","summary":"Check back later\u0026hellip;\n --- ","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" H1 In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nH3 123\nTip 2 \u0026hellip;\n","date":1536476400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536476400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/MacStrelioff/tutorial/example/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/MacStrelioff/tutorial/example/","section":"tutorial","summary":"H1 In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nH3 123\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":" true  H1 In this tutorial, I’ll share my top 10 tips for getting started with Academic:\nTip 1 plot(rnorm(20),runif(20)) …\nH3 123\n  Tip 2 …\n  ","date":1536476400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536476400,"objectID":"cfe2453ca8d8ae42ddecf626ed999e22","permalink":"/MacStrelioff/tutorial2/example/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/MacStrelioff/tutorial2/example/","section":"tutorial2","summary":" true  H1 In this tutorial, I’ll share my top 10 tips for getting started with Academic:\nTip 1 plot(rnorm(20),runif(20)) …\nH3 123\n  Tip 2 …\n  ","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/MacStrelioff/project/external-project/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/MacStrelioff/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/MacStrelioff/project/internal-project/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/MacStrelioff/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Mac Strelioff"],"categories":null,"content":" Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n Setup Academic Get Started View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Color Themes Academic comes with day (light) and night (dark) mode built-in. Click the sun/moon icon in the top right of the Demo to see it in action!\nChoose a stunning color and font theme for your site. Themes are fully customizable and include:\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461135600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555484400,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/MacStrelioff/post/getting-started/","publishdate":"2016-04-20T00:00:00-07:00","relpermalink":"/MacStrelioff/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Mac Strelioff"],"categories":null,"content":" Supplementary notes can be added here, including code and math.   ","date":1441090800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441090800,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/MacStrelioff/publication/journal-article/","publishdate":"2015-09-01T00:00:00-07:00","relpermalink":"/MacStrelioff/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"ca77a28b841b271a15d5420351f3e354","permalink":"/MacStrelioff/post/2015-07-23/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/MacStrelioff/post/2015-07-23/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Mac Strelioff","Robert Ford"],"categories":null,"content":" Supplementary notes can be added here, including code and math.   ","date":1372662000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372662000,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/MacStrelioff/publication/conference-paper/","publishdate":"2013-07-01T00:00:00-07:00","relpermalink":"/MacStrelioff/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":" TODO: redo in Python, make an agent that uses each strategy as a method. Build the agent throughout the script.   Bandit Algorithms Much of this is from this blog.\nHere I evaluate different algorithms for bandit problems similar to those anticipated in industry or testing settings. Criteria of consideration are;\nProblem Formalization Bandit tasks can be cast as a Markov Decision Process.\n\\[ \\begin{aligned} t \u0026amp;\\in \\{1,...,T\\} \\\\ a_t \u0026amp;\\in \\mathcal{A} \\\\ s_t \u0026amp;\\in \\mathcal{S} \\\\ r_t \u0026amp;= R(s_{t+1}|a_t,s_t) = v(s_{t+1}|a_t,s_t) \\\\ T(s_{t+1}|a_t,s_t) \u0026amp;= p(s_{t+1}|a_t,s_t) \\\\ \\pi(s_t)\u0026amp;=p(a_t|s_t) \\end{aligned} \\]\n\\(\\pi\\) is referred to as a policy or decision rule. Mathematically, it is a probability distribution over actions – a funciton that maps from states to actions.\nThe decision maker’s goal here is to learn a policy (\\(\\pi\\)) that is optimal for some criteria. A variety of possible criteria are discussed and evaluated below. This objective is considered when chooseing the value function \\(v(s_{t+1}|a_t,s_t)\\).\nObjective is to maximize \\[ E\\left(\\sum_t r_t\\right)=\\sum_tE(r_t) \\]\nIn a typical testing scenario, the policy replaces the assignment mechanism. Hence, I treat assignment mechanisms as policies here.\n Use Cases and Evaluation Criteria Bandit problems arise across many theoretical and applied fields. Everything from industry A/B testing, medical clinical trials, experimental lab studies, and toy problems for reinforcement learning algorithms can be cast as a bandit task. These different domains generally have different goals. A/B tests are conducted to find evidence for an advantage of one version of a product over another while emphasizing classical statistical objectives like minimizing type I error rates or false discovery rates. The goal of clinical experiments is to quickly discover the best treatment so that patient lives can be improved or saved. In simulation settings, bandit problems have been used to benchmark a variety of algorithms in terms of regret. Here I conduct similar benchmarks, while also evaluating standard ‘best proctices’ in terms of type I error rates and false discovery rates.\nType I Error Among situations where there is no difference, how often is a difference detected?\nType I error occurs when a null hypothesis is reongly rejected – so when a difference in outcomes of arms is detected when none actually exists.\n False Discovery Rate Among detected differences, how many are real?\nA false discovery occurs when an arm is selected but is not the actual optimal arm.\nI’ll consider this on a trial-by-trial level as well as the result of the overall experiment.\n Regret Regret is the difference between the reward that would have been obtained had the optimal action been chosen, and the reward that was actually obtained from the chosen action.\n\\[ \\begin{aligned} E(Regret) \u0026amp;= \\sum_t E(r^*_{t}-r_t) \\end{aligned} \\]\n  Uniform Policy This corresponds to a simple random sample type of assignment mechanism – the gold standard for causal inference from experimental data.\n\\[ \\begin{aligned} \\pi(s_t) \u0026amp;= \\frac{1}{|\\mathcal{A}|} \\\\ E(Regret) \u0026amp;= TE(r^*_t) - \\sum_t E(r_t|\\pi) \\\\ \u0026amp;= T(E(r^*_t) - E(r_t)) \\\\ \\end{aligned} \\]\nThe expected regret on each trial is the difference between the maximal reward and mean reward across actions.\n Greedy Policies \\(\\epsilon\\)-Greedy Policy  Softmax Policy Might be good for parameter estimation while also reducing regret.\nna=5 # Number of arms as=1:na # action IDs beta = 5 R = runif(na) # arm probabilities #R[1]=1; R[2]=.5; R[3]=0 T = 1500 # Number of trials q=rep(0,na) pis = c() qs = c() ats = c() rts = c() regret = c() regrett= 0; for(ti in 1:T){ # select action pi = exp(beta*q)/sum(exp(beta*q)) at = rmultinom(n=1,size=as,prob=pi) at = which(at==1) # observe outcome rt = rbinom(1,size=1,prob=R[at]) # save stats pis = rbind(pis,pi) ats = c(ats,at) rts = c(rts,rt) # update action values q[at] = q[at] + .2 * (rt - q[at]) qs = rbind(qs,q) # regret regrett= regrett+max(R)-R[at] regret = c(regret,regrett) } plot(regret,type=\u0026quot;l\u0026quot;) cols = rainbow(na,s=1,v=.7) for(ai in as){ if( ai\u0026gt;1){par(new=TRUE)} plot(qs[,ai],type=\u0026quot;l\u0026quot;,ylim=c(0,1),col=cols[ai], main=\u0026quot;Estimation\u0026quot;, ylab=\u0026quot;Q-value\u0026quot;, xlab=\u0026quot;Trial\u0026quot;) abline(h=R[ai],col=cols[ai],lty=2) } for(ai in as){ if( ai\u0026gt;1){par(new=TRUE)} plot(pis[,ai],type=\u0026quot;l\u0026quot;,ylim=c(0,1),col=cols[ai], main=\u0026quot;Action Probabilities\u0026quot;, ylab=\u0026quot;Policy\u0026quot;, xlab=\u0026quot;Trial\u0026quot;) #abline(h=R[ai],col=cols[ai],lty=2) } ent = 0 ps=c() for(ai in as){ p = sum(ats==ai)/length(ats) ent = ent + p*(-log2(p)) ps=c(ps,p) } c(ent,ps) ## [1] 1.41206826 0.01533333 0.68200000 0.03400000 0.14333333 0.12533333 c(entropy.empirical(table(ats),unit=\u0026quot;log2\u0026quot;),freqs.empirical(table(ats))) ## 1 2 3 4 5 ## 1.41206826 0.01533333 0.68200000 0.03400000 0.14333333 0.12533333   Upper Confidence Bound (UCB) Policy  Thompson Sampling Policy #na=5 # Number of arms #as=1:na # action IDs #beta = 5 # keeps same as those above # R = runif(na) # arm probabilities #T = 5000 # Number of trials #priors: rows: actions, col1:alpha, col2:beta prior = matrix(1,nrow=na,ncol=2) aposts=c() bposts=c() thompsonSamples = c() ats = c() rts = c() regret = c() regrett= 0; for(ti in 1:T){ # select action thompsonSample=c() for(ai in as){ thompsonSample = c(thompsonSample,rbeta(1,prior[ai,1],prior[ai,2])) } at = which.max(thompsonSample) # observe outcome rt = rbinom(1,size=1,prob=R[at]) # save stats thompsonSamples = rbind(thompsonSamples,thompsonSample) ats = c(ats,at) rts = c(rts,rt) # update beta distributions prior[at,1]=prior[at,1]+rt prior[at,2]=prior[at,2]+(1-rt) # save posterior parameters aposts=rbind(aposts,prior[,1]) bposts=rbind(bposts,prior[,2]) # regret regrett= regrett+max(R)-R[at] regret = c(regret,regrett) } plot(regret,type=\u0026quot;l\u0026quot;, main=\u0026quot;Regret = max E(reward) - E(reward|choice)\u0026quot;, ylab=\u0026quot;Cumulative Regret\u0026quot;, xlab=\u0026quot;Trial\u0026quot;) # something might be wrong with rbeta(x,vec1,vec2) cols = rainbow(na,s=1,v=.7) for(ai in as){ if( ai\u0026gt;1){par(new=TRUE)} plot(aposts[,ai]/(aposts[,ai]+bposts[,ai]),type=\u0026quot;l\u0026quot;,ylim=c(0,1),col=cols[ai], main=\u0026quot;Reward Probability Estimation\u0026quot;, ylab=\u0026quot;Posterior Means\u0026quot;, xlab=\u0026quot;Trial\u0026quot;) abline(h=R[ai],col=cols[ai],lty=2) } legend(1100,.8,c(\u0026quot;Estimate\u0026quot;,\u0026quot;True\u0026quot;),lty=c(1,2)) for(ai in as){ if( ai\u0026gt;1){par(new=TRUE)} plot(thompsonSamples[,ai],type=\u0026quot;l\u0026quot;,ylim=c(0,1),col=cols[ai], main=\u0026quot;Posterior Samples\u0026quot;, #ylab=expression(\u0026#39;Sampled Value -- Policy=argmax\u0026#39;[\u0026#39;a\u0026#39;]*\u0026#39;(sample\u0026#39;[\u0026#39;a\u0026#39;]*\u0026#39;)\u0026#39;), ylab=expression(\u0026#39;Sampled Value\u0026#39;), xlab=\u0026quot;Trial\u0026quot;) #abline(h=R[ai],col=cols[ai],lty=2) } ent = 0 ps=c() for(ai in as){ p = sum(ats==ai)/length(ats) ent = ent + p*(-log2(p)) ps=c(ps,p) } c(ent,ps) ## [1] 0.109426242 0.001333333 0.988666667 0.003333333 0.001333333 0.005333333 c(entropy.empirical(table(ats),unit=\u0026quot;log2\u0026quot;),freqs.empirical(table(ats))) ## 1 2 3 4 5 ## 0.109426242 0.001333333 0.988666667 0.003333333 0.001333333 0.005333333   Policy Comparisons  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e63993d0b6bdc1a53b39e8c13704179f","permalink":"/MacStrelioff/unlisted/banditalgos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/MacStrelioff/unlisted/banditalgos/","section":"Unlisted","summary":"TODO: redo in Python, make an agent that uses each strategy as a method. Build the agent throughout the script.   Bandit Algorithms Much of this is from this blog.\nHere I evaluate different algorithms for bandit problems similar to those anticipated in industry or testing settings. Criteria of consideration are;\nProblem Formalization Bandit tasks can be cast as a Markov Decision Process.\n\\[ \\begin{aligned} t \u0026amp;\\in \\{1,...,T\\} \\\\ a_t \u0026amp;\\in \\mathcal{A} \\\\ s_t \u0026amp;\\in \\mathcal{S} \\\\ r_t \u0026amp;= R(s_{t+1}|a_t,s_t) = v(s_{t+1}|a_t,s_t) \\\\ T(s_{t+1}|a_t,s_t) \u0026amp;= p(s_{t+1}|a_t,s_t) \\\\ \\pi(s_t)\u0026amp;=p(a_t|s_t) \\end{aligned} \\]","tags":null,"title":"Bandit Algos for Estimation, Hypothesis Testing, and Decision Making","type":"Unlisted"},{"authors":null,"categories":null,"content":"  Overview: Setup Professional Skills Mindset (Day 1: 6/3/2019) Opportunities: Networking and Company Visits Applying LinkedIn Resume Pitch   Projects Ideation (5/21-6/5) Prediction Market Sentiment Analysis Stats Website Bandit Analysis Car price recommender Kaggle competition?  Selection Presentation 5 min  How to solve a problem    Overview: Made this doc to document my journey through the Insight Data Science program.\n Setup I installed Python and worked through steps for creating a virtual environment with Anaconda.\n Professional Skills Mindset (Day 1: 6/3/2019) Imposter syndrome\nFail quickly and iterate\nOpen mindedness – new problem space, …\nAdaptability, identify weakenesses and adapt\n Opportunities: Networking and Company Visits At Insight, data science hiring teams frequently visit. During the first week, we had visits from Square, Lab 41, and App Annie.\nAttempt:\n Ask questions to assess fit: size, hierarchy, business model / monetization  Avoid:\n “Questions” purely intended to demonstrate your own knowledge. Instead, introduce yourself after the talk.    Applying Identify strengths, and demonstrantions of value to a company\nLinkedIn workshop\n Resume workshop lecture\nResume is a marketing tool to get an interview. Resumes should be understood in 15-45 seconds.\nAudience:\nHiring managers   Wants: quickly hire, culture fit, compliments skillsets on team Dislike: lack of detail, ‘fluff’, verbose  Recruiter   Wants: Pass high quality Dislikes: Poor writing or grammer, “creative” resumes that take time to orient to, verbose  Convey:\n Technical fit for role Potential for learning and growth Unique professional value (differentiation)  Components:\nOrder from most to least impactful. Focus on content relevant for next role.\n Header, contact information. Location that is local to the company being applied to. LinkedIn should also include anything not on resume. Skills, core competencies for recruiter to check off required skills. Sort into meaningful clusters. Experience, for hiring manager. Include evidence of the skills. Talk about publications here, and emphasize their impact or value. Start with past-tense verb. Situation, Task, Action, Result. Education, also mainly for recruiters. Could include a section on specific courses, but would be more impactful as projects in the experience section. NO SUMMARY, experience is more valuable. Can put one on LinkedIn. Avoid making too specific (pidgenhole) or general (cliche).  Tips:\n Include numbers to quantify impacts, numbers can be salient relative to text. Include URLs, for those who read on paper. Consistent puncuation, … .   Pitch    Projects Email;\nThe start of Insight is next week and we’re looking forward to meeting all of you in person! You’re likely aware by now that you’ll be spending a large portion of your time at Insight working on a data science project. These projects are designed to provide evidence of the value you would bring to a data science team and your ability to learn and iterate at a rapid pace.\nIt’s not necessary for you to come to Insight with a project idea in mind (but it can’t hurt!), and we’ll spend the first few days generating and refining project ideas. However, we know that inspiration often strikes when you’re not seeking it, and previous Fellows have mentioned they wish they had come with some ideas in mind.\nHere are some tips:\nDon’t dismiss any ideas while brainstorming. You might find after talking through the idea with someone else that it’s more viable than you thought. We’re emphasizing quantity over quality at this point - add it to the list!\nStart with the question/product and think personal. What issues in your life can be solved using data? Think about your hobbies, your daily routine, your job, the tech you regularly interact with. How could it be improved or personalized?\nThink about the user and use case. Who would benefit from this product? Why would they use it?\nStart thinking about data sources. There are many publicly available datasets, and Fellows in the past have gotten creative with techniques like web scraping and calls to website APIs to get the data they want. That being said, don’t find a dataset first and then try to turn it into a product.\nDon’t become overly attached to just one project idea. We’ll work together during the first week to refine your project, and we know very well which types of projects work well and which don’t!\nTo reiterate - First think about the problem and your target audience rather than spending all of your time searching for datasets. And don’t fret if you haven’t been struck by inspiration before Day 1! We encourage you to discuss project ideas in the ~projects-DS channel on our 19BSV Mattermost Team!\nIdeation (5/21-6/5) Prediction Market  Sentiment Analysis tools\nhttps://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\nhttp://sentdex.com/sentiment-analysis/\n Stats Website Shiny app that generates practice problems and contains lessons\n Bandit Analysis good example\nhttps://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d\ngood dashboarding\n Car price recommender (this has already been done on fb…)\nSync data from Facebook, Amazon, and Kelly Blue Book to find distributions over car prices, to empower buyers and sellers in social markets.\nI wanted to sell a car and didn’t know what it was worth.\nLearn the KBB API, setup a shiny app that allows one to enter KBB info, and query KBB and FB and Amazon for KBB price, and amazon and fb postings.\n Kaggle competition? Air bnb new user bookings\n Model a user’s behavior from landing on site to their first booking. Look for ways to improve bookings? – book in fewer searches, book in less time, … Do all NDF locations have a missing ‘date first booked’ variable? Does a missing ‘date_first_booked’ mean that the person did not book?  Google store revenue - few items - makeup items? - predict probability of purchasing an item\nFacebook checkin prediction - fake data…\n  Selection Data scientists make data products.\nAssess value\nWhat is the product? Why is this useful to a specific company? Why is it useful to users?  Assess feasibility\nWhat data is needed for this product? How much data is there, where does it come from? What technical methods are used? How could AI/ML improve the product? Are there any limits on data access or ethical constraints? How could it be monetized?  Assess potential\nHow might a company expand the product?   Presentation 5 min  context need vision (‘stretch goal’) outcome    How to solve a problem Think of the variables and their structure, before seeing data; confounds, precision, neusance, …     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f63259ce0213d0353398ab77db038ab7","permalink":"/MacStrelioff/unlisted/insight/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/MacStrelioff/unlisted/insight/","section":"Unlisted","summary":"Overview: Setup Professional Skills Mindset (Day 1: 6/3/2019) Opportunities: Networking and Company Visits Applying LinkedIn Resume Pitch   Projects Ideation (5/21-6/5) Prediction Market Sentiment Analysis Stats Website Bandit Analysis Car price recommender Kaggle competition?  Selection Presentation 5 min  How to solve a problem    Overview: Made this doc to document my journey through the Insight Data Science program.\n Setup I installed Python and worked through steps for creating a virtual environment with Anaconda.","tags":null,"title":"Insight","type":"docs"},{"authors":null,"categories":null,"content":" What are Metrics? Metrics are how the data scientist communicates with project managers.\nTwo categories; success and tracking.\nGoals are defined in terms of metrics, and user experience is understood through metrics.\nSuccess Metrics  Tracking Metrics   Metrics for Goals  Metrics for Insights  Communicating Metrics Idea from:\n Add metrics from google analytics, and youtube creator.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ee32f2b3c8c15d92808f0617b8d64f2","permalink":"/MacStrelioff/unlisted/metricsthroughyoutubecreator/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/MacStrelioff/unlisted/metricsthroughyoutubecreator/","section":"Unlisted","summary":" What are Metrics? Metrics are how the data scientist communicates with project managers.\nTwo categories; success and tracking.\nGoals are defined in terms of metrics, and user experience is understood through metrics.\nSuccess Metrics  Tracking Metrics   Metrics for Goals  Metrics for Insights  Communicating Metrics Idea from:\n Add metrics from google analytics, and youtube creator.\n ","tags":null,"title":"Introduction to Metrics Through YouTube Creator","type":"Unlisted"},{"authors":null,"categories":null,"content":"The sections below are documents I made based on things that were important, but not used often enough to commit to memory.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"173528b19a7b0e4a105bfc928d11b594","permalink":"/MacStrelioff/data-science/learn-data-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/MacStrelioff/data-science/learn-data-science/","section":"data-science","summary":"The sections below are documents I made based on things that were important, but not used often enough to commit to memory.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" Probability Theory and Information Theory Types of Probabilities and Their Computations motivation? Many important variables (future income, GDP, who I’ll marry, whether I’ll get sick, what disease I have given some symptoms, …)\n Randomness, Event Spaces, and Probability motivating example.. Imagine that a vaccine against a dangerous disease recently becomes available. However, there is a small risk that the vaccine will produce a condition just as bad as the disease itself. You do not know whether you will get the disease, or whether, given the vaccine, you will contract the side effect. Should you get the vaccine or abstain and live with a higher risk of disease?\nKnowledge of random variables, and related concepts, can help in this and many other situations. Specifically; %Knowing the probabilities of each event in the event space under conditions where you take or abstain from the vaccine could determine your decision.\nA random variable random variable is a variable with an unknown value, but known possible values. An event is an observed value of a random variable. An event space contains all possible values of the random variable. Probabilities are values assigned to the events in an event space (i.e. the possible values of a random variable) and represent how likely each event is relative to other events in the event space.\nHere \\(p(e)\\) will be used to represent the probability of an event. Some special probabilities include the probability of any event in the event space, which is 1. And the probability of any event other than \\(p(e)\\), known as the compliment of \\(p(e)\\), denoted with \\(p(\\neg e)\\), is found by; \\[\\begin{equation} p(\\neg e) = 1 - p(e) \\end{equation}\\] For example, the value observed when a six sided die is rolled could be considered a random variable with possible values of 1 through 6. Rolling the die could be an experiment, and, if a 5 lands face up, then 5 would be the outcome.\nProbabilities are often thought of with reference to experiments and outcomes. In this context, an experiment is any opportunity to observe a relevant outcome. Outcomes are the consequences of an experiment, and one is typically interested in outcomes of a particular kind. For example, if coin flips are thought of as an experiment, then the outcomes of this experiment could be heads or tails. In this context, probabilities assign numbers to the outcomes (e.g. heads, or tails) of an experiment (e.g. coin toss).\nsee https://en.wikipedia.org/wiki/Probability_interpretations\ndefine event define event space as all the events that could happen probability = events of interest / all possible events\n Joint Events and the Multiplication Rule A joint event refers to two or more events occurring together. They are colloquially talked about as one event ‘and’ another event occurring together. More formally, joint events are called intersections (represented with the \\(\\cap\\) symbol) between events. For events \\(e_1\\) and \\(e_2\\), the probability of their joint event will be represented with \\(p(e_1 \\cap e_2)\\). The probability of an intersection of events is the same regardless of which event is considered first;\n\\[\\begin{equation} \\label{joint reflexivity} p(e_1 \\cap e_2) = p(e_2 \\cap e_1) \\end{equation}\\] For two events, \\(e_1\\) and \\(e_2\\), their intersection is found by; \\[\\begin{equation} \\label{joint probability definition} p(e_1 \\\u0026amp; e_2) = p(e_1\\cap e_2) = p(e_1|e_2)p(e_2) \\end{equation}\\] where \\(p(e_1|e_2)\\) is a conditional probability, discussed in the next section.\n Conditional Events and Bayes’ Rule Conditional events refer to one event, \\(e_1\\), after another event, \\(e_2\\), is known. \\(p(e_1|e_2)\\) represents the probability of \\(e_1\\) given, or after knowing, \\(e_2\\). These can be defined by rearranging the multiplication rule (equation ) as follows;\n\\[\\begin{equation} p(e_1|e_2)p(e_2)=p(e_1\\cap e_2) \\rightarrow p(e_1|e_2)= \\frac{p(e_1\\cap e_2)}{p(e_2)} \\end{equation}\\] Noting that, by the reflexively of joint events (equation ) and the definition of the multiplication rule (equation ), \\(p(e_1 \\cap e_2) = p(e_2 \\cap e_1)= p(e_2 | e_1) p(e_1)\\), and so the above equation becomes; \\[\\begin{equation} \\label{Bayes Theorem} p(e_1|e_2)=\\frac{p(e_2 | e_1) p(e_1)}{p(e_2)} \\end{equation}\\] Equation is known as Bayes Theorem.\n Unions of Events and the Addition Rule A union (represented with the \\(\\cup\\) symbol) of events refers to at least one of multiple events occurring. For events, \\(e_1\\) and \\(e_2\\), their union would include the probability that \\(e_1\\) occurs, the probability that or \\(e_2\\) occurs, and the probability that both \\(e_1\\) and \\(e_2\\) occur. Colloquially this is talked about the probability of \\(e_1\\) ‘or’ \\(e_2\\). Formally this is expressed and computed as;\n\\[\\begin{equation} \\label{addition rule} p(e_1\\text{ or } e_2) = p(e_1 \\cup e_2) = p(e_1) + p(e_2) - p(e_1 \\cap e_2) \\end{equation}\\] Where \\(p(e_1 \\cap e_2)\\) is a joint probability, defined in Equation .\n Independent Events Independence is a common assumption in many statistical techniques. Statisticians assume independence primarily because it simplifies the computation of certain probabilities. Events are said to be independent if knowing one event does not change the probability of the other event. Formally, if events \\(e_1\\) and \\(e_2\\) are independent, this would mean that; \\[\\begin{equation} \\label{independent definition} p(e_1|e_2) = p(e_1) \\text{ and } p(e_2|e_1) = p(e_2) \\end{equation}\\] If \\(e_1\\) and \\(e_2\\) are independent, then their joint probability, defined in in Equation , simplifies as so; \\[\\begin{equation} \\label{product rule with independence} p(e_1 \\cap e_2) = p(e_1|e_2)p(e_2) = p(e_1)p(e_2) \\end{equation}\\] Where the last equality is only true if \\(e_1\\) and \\(e_2\\) are independent (i.e. \\(p(e_1|e_2)=p(e_1)\\)).\nThis then simplifies the addition rule in Equation so that; \\[\\begin{equation} p(e_1 \\cup e_2) = p(e_1) + p(e_2) - p(e_1|e_2)p(e_2) = p(e_1) + p(e_2) - p(e_1)p(e_2) \\end{equation}\\] Where again, the last part of this equality is only true if \\(e_1\\) and \\(e_2\\) are independent.\n Mutually Exclusive Events Events \\(e_1\\) and \\(e_2\\) are said to be mutually exclusive if the occurrence of either event precludes the occurrence of the other event. Formally mutual exclusivity means that;\n\\[\\begin{equation} \\label{mutually exclusive definition} p(e_1 | e_2) = 0 \\text{ and } p(e_2 | e_1) = 0 \\end{equation}\\] If two events are mutually exclusive, then the probability of their joint event is; \\[\\begin{equation} p(e_1 \\cap e_2) = p(e_1|e_2)p(e_2) = 0*p(e_2) = 0 \\end{equation}\\] And the probability of their union simplifies to; \\[\\begin{equation} p(e_1 \\cup e_2) = p(e_1) + p(e_2) - p(e_1 \\cap e_2) = p(e_1) + p(e_2) - 0 = p(e_1) + p(e_2) \\end{equation}\\]   Probability Functions and Parameter Estimation via Likelihoods The general concepts of parameterized probability functions and likelihoods are introduced here. For specific distributions, see Chapter .\nevents can be thought of as values in spaces.. probability functions assign probabilities to data, assuming that parameters are known likelihoods assign probabilities to parameters, assuming the data are known or observed\nDomain of a Function as an Event Space In Section , probabilities were introduced with respect to events. If these events are a value, \\(x\\), from a large set of possible values, \\(X\\), then a probability function, denoted \\(f(x|\\theta)\\), can be used to assign probabilities to the values \\(x \\in X\\).\n PDFs or PMFs, and CDFs Probability functions are referred to as probability density functions (PDFs) if \\(x\\) takes continuous values, and probability mass functions (PMFs) if \\(x\\) takes discrete values. Regardless of the type of variable \\(x\\) is, a probability function that assignes probabilities to values \\(x\\) that depend on a known parameter or parameter vector, \\(\\theta\\), is denoted \\(f(x|\\theta)\\). The cumulative distribution function (CDF), denoted \\(F(x|\\theta)\\), represents the total probability assigned to values of \\(X\\) less than a particular value \\(x\\). The CDF is related to the PDF or PMF through integration from the left side. All of this can be summarized as;\n\\[\\begin{equation} p(X \\leq x |\\theta) = F(x|\\theta) = \\int_{-\\infty}^x f(x|\\theta) = \\int_{-\\infty}^x p(X=x|\\theta) \\end{equation}\\] Note that integrals are replaced with summation when \\(x\\) takes discrete values.\nA technical difficulty when considering values in a continuous domain is that the probability of any single value is 0. To sidestep this issue, one can consider values within a tiny set of values around \\(x\\), that is, \\(\\{x:x\\in x\\pm \\Delta\\}\\), for an arbitrarily small \\(\\Delta\\).\nInstead of being based on observations, these assign probabilities to a range of values, \\(x\\), in the domain of the function.\n Likelihood and Log Likelihood The likelihood, denoted \\(\\mathcal{L}(\\theta|x)\\), assigns probabilities to values of a parameter or parameter vector, \\(\\theta\\), when the events or data, \\(x\\) are treated as known. When a number, \\(n\\), of observations are made, and independence between the observations is assumed, the likelihood can be expressed as;\n\\[\\begin{equation} \\mathcal{L}(\\theta | x_1,x_2,...,x_n) = p(\\theta | x_1,x_2,...,x_n) = p(\\theta|x_1) p(\\theta|x_2) ... p(\\theta|x_n) = \\prod_{i=1}^n p(\\theta|x_i) \\end{equation}\\] Likelihoods can be difficult to work with analytically if they are complicated functions, and numerically if they are very small numbers. Because of this, log likelihoods are often used in place of likelihoods. A log likelihood is defined as;\n\\[\\begin{equation} \\ell(\\theta | x_1,x_2,...,x_n) = log(\\mathcal{L}(\\theta | x_1,x_2,...,x_n)) = \\sum_{i=1}^n p(\\theta | x_i) \\end{equation}\\]  Score Function and Fisher Information Usually statisticians are interested in the most likely parameters, that is, the parameter values that maximize the likelihood. One way to find these parameters is to find the derivative of the log likelihood and solve for the parameter values that make this equal to zero. The score function, \\(U(\\theta)\\), is the derivative of the log likelihood. Specifically;\n\\[\\begin{equation} U(\\theta) = \\frac{d}{d\\theta}\\ell(\\theta| x_1,x_2,...,x_n) \\end{equation}\\] Statisticians are also concerned with the uncertainty in parameter estimates. One statistic used to assess this is Fisher Information, \\(I(\\theta)\\), defined as the negative expectation of the second derivative of the log likelihood;\n\\[\\begin{equation} I(\\theta) = -E\\bigg(\\frac{d^2}{d^2\\theta}\\ell (\\theta | x_1,x_2,...,x_n)\\bigg) = \\int_{-\\infty}^{\\infty} \\frac{d^2}{d^2\\theta}\\ell (\\theta | x_1,x_2,...,x_n) f(x|\\theta) d\\theta \\end{equation}\\] The inverse Fisher Information, \\(\\frac{1}{I(\\theta)}\\), is the variance of the sampling distribution for parameter estimates obtained by maximizing the log likelihood. As a second derivative, Fisher Information also represents the peakedness of the likelihood around the maximum likelihoods estimate. A large \\(I(\\theta)\\) indicates a steeper peak around the maximum likelihood estimate, which is interpreted as a more informative sample.\n  Information and Entropy see https://en.wikipedia.org/wiki/Quantities_of_information see https://en.wikipedia.org/wiki/Information_theory\nSurprise (Self Information) For an event, \\(e\\), surprise (also called information or self-information) is defined to be the function satisfying these properties;\nThe function satisfying these properties is:\n\\[\\begin{equation}\\label{InformationDfn} I(e)=-log(p(e))=log\\left(\\frac{1}{p(e)} \\right) \\end{equation}\\]  Entropy Entropy is the expected information;\n\\[\\begin{equation}\\label{EntropyDfn} H(E)=E(I(E)) = E(-log(p(E))) = \\int_E p(e)I(e) = -\\int_E p(e)log(p(e)) \\end{equation}\\] entropy is the expected information\n Joint Entropy  Conditional Entropy  Relative Entropy (Divergence) KL divergence\n Mutual Information Mutual information quantifies the information obtained about one random variable through the observation of another. can be expressed as average divergance? self information is a special case of mutual information, the mutual information of a variable and itself. Given the definition of Information in Equation , Mutual Information is defined as:\n\\[\\begin{equation}\\label{MutualInformationDfn} I(E_1;E_2)=\\int_{E_2} \\int_{E_1} p(e_a,e_2)log\\left(\\frac{p(e_1,e_2)}{p(e_1)p(e_2)} \\right) \\end{equation}\\]   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c6430250197a6347734ff9e5a3bf9c05","permalink":"/MacStrelioff/unlisted/probabilityandinformation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/MacStrelioff/unlisted/probabilityandinformation/","section":"Unlisted","summary":"Probability Theory and Information Theory Types of Probabilities and Their Computations motivation? Many important variables (future income, GDP, who I’ll marry, whether I’ll get sick, what disease I have given some symptoms, …)\n Randomness, Event Spaces, and Probability motivating example.. Imagine that a vaccine against a dangerous disease recently becomes available. However, there is a small risk that the vaccine will produce a condition just as bad as the disease itself.","tags":null,"title":"Probability and Information","type":"Unlisted"},{"authors":null,"categories":null,"content":"  Recommendation Sort Algorithms Search Algorithms Ranking Algorithms   Recommendation Lots of recommendation algorithms\nData science methods play a major role in discovering recommendations for users.\nEcon, compliments and substitutes (competitors).\nTypes of recommendation; compliments (these go together), competitors (you might also like…), temporal (might buy this again in the future). Horizontal and vertical focus\nTemporal involves forecasting, which is a topic for another post.\n Sort Algorithms Lots of sort algorithms\n Search Algorithms Lots of search algorithms\n Ranking Algorithms Lots of ranking algorithms\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"676de85ee3cafb38225bcf5fac2ec817","permalink":"/MacStrelioff/unlisted/searchsortrank/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/MacStrelioff/unlisted/searchsortrank/","section":"Unlisted","summary":"Recommendation Sort Algorithms Search Algorithms Ranking Algorithms   Recommendation Lots of recommendation algorithms\nData science methods play a major role in discovering recommendations for users.\nEcon, compliments and substitutes (competitors).\nTypes of recommendation; compliments (these go together), competitors (you might also like…), temporal (might buy this again in the future). Horizontal and vertical focus\nTemporal involves forecasting, which is a topic for another post.\n Sort Algorithms Lots of sort algorithms","tags":null,"title":"Recommending: Searching, Sorting, Ranking","type":"docs"}]