<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.2.5">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Mac Strelioff">

  
  
  
    
  
  <meta name="description" content="Sources / Alternatives  Netflix Experimentation and Sequential Testing Optamizely Google Analytics   TODO: redo in Python, make an agent that uses each strategy as a method. Build the agent throughout the script.   Intro Scientists and business students are trained in decision making from an outdated perspective – classical decision making based on p-values.
(make a case against p-values – inflated error rates, incoherence, difficulty integrating with expected value)">

  
  <link rel="alternate" hreflang="en-us" href="/MacStrelioff/unlisted/banditalgos/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="rgba(0,130,160,1)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/MacStrelioff/css/academic.min.9c3a903cc870878595d69f08d98aa322.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-140153670-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/MacStrelioff/site.webmanifest">
  <link rel="icon" type="image/png" href="/MacStrelioff/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/MacStrelioff/img/icon-192.png">

  <link rel="canonical" href="/MacStrelioff/unlisted/banditalgos/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@macstrelioff">
  <meta property="twitter:creator" content="@macstrelioff">
  
  <meta property="og:site_name" content="Mac Strelioff">
  <meta property="og:url" content="/MacStrelioff/unlisted/banditalgos/">
  <meta property="og:title" content="Bandit Algos for Estimation, Hypothesis Testing, and Decision Making | Mac Strelioff">
  <meta property="og:description" content="Sources / Alternatives  Netflix Experimentation and Sequential Testing Optamizely Google Analytics   TODO: redo in Python, make an agent that uses each strategy as a method. Build the agent throughout the script.   Intro Scientists and business students are trained in decision making from an outdated perspective – classical decision making based on p-values.
(make a case against p-values – inflated error rates, incoherence, difficulty integrating with expected value)"><meta property="og:image" content="/MacStrelioff/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  
  
  

  

  

  <title>Bandit Algos for Estimation, Hypothesis Testing, and Decision Making | Mac Strelioff</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/MacStrelioff/">Mac Strelioff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/data-science/">
            
            <span>Data Science</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/video-lectures/">
            
            <span>Videos</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/cv/">
            
            <span>Resume</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/MacStrelioff/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Bandit Algos for Estimation, Hypothesis Testing, and Decision Making</h1>

  

  
    



<meta content="" itemprop="datePublished">
<meta content="" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jan 1, 0001</time>
  </span>
  

  

  

  
  
  <span class="middot-divider"></span>
  <a href="/MacStrelioff/unlisted/banditalgos/#disqus_thread"></a>
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=&amp;url="
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u="
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=&amp;body=">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    
















  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<div id="sources-alternatives" class="section level1">
<h1>Sources / Alternatives</h1>
<ul>
<li><a href="https://medium.com/netflix-techblog/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be">Netflix Experimentation and Sequential Testing</a></li>
<li>Optamizely</li>
<li>Google Analytics</li>
</ul>
</div>
<div id="todo" class="section level1">
<h1>TODO:</h1>
<ol style="list-style-type: decimal">
<li>redo in Python, make an agent that uses each strategy as a method. Build the agent throughout the script.</li>
</ol>
<!---
See: 
For Insight Bandit blogpost:
https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d

For Air BNB experimentation dashboard:
https://medium.com/airbnb-engineering/experiment-reporting-framework-4e3fcd29e6c0
--->
</div>
<div id="intro" class="section level1">
<h1>Intro</h1>
<p>Scientists and business students are trained in decision making from an outdated perspective – classical decision making based on p-values.</p>
<p>(make a case against p-values – inflated error rates, incoherence, difficulty integrating with expected value)</p>
</div>
<div id="bandit-algorithms" class="section level1">
<h1>Bandit Algorithms</h1>
<p>Much of this is from <a href="https://sudeepraja.github.io/Bandits/">this blog</a>.</p>
<p>Here I evaluate different algorithms for bandit problems similar to those anticipated in industry or testing settings. Criteria of consideration are;</p>
<div id="problem-formalization" class="section level2">
<h2>Problem Formalization</h2>
<p>Bandit tasks can be cast as a Markov Decision Process.</p>
<p><span class="math display">\[
\begin{aligned}
t &amp;\in \{1,...,T\} \\
a_t &amp;\in \mathcal{A} \\
s_t &amp;\in \mathcal{S} \\
r_t &amp;= R(s_{t+1}|a_t,s_t) = v(s_{t+1}|a_t,s_t) \\
T(s_{t+1}|a_t,s_t) &amp;= p(s_{t+1}|a_t,s_t) \\
\pi(s_t)&amp;=p(a_t|s_t)
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\pi\)</span> is referred to as a policy or decision rule. Mathematically, it is a probability distribution over actions – a funciton that maps from states to actions.</p>
<p>The decision maker’s goal here is to learn a policy (<span class="math inline">\(\pi\)</span>) that is optimal for some criteria. A variety of possible criteria are discussed and evaluated below. This objective is considered when chooseing the value function <span class="math inline">\(v(s_{t+1}|a_t,s_t)\)</span>.</p>
<p>Objective is to maximize <span class="math display">\[
E\left(\sum_t r_t\right)=\sum_tE(r_t)
\]</span></p>
<p>In a typical testing scenario, the policy replaces the assignment mechanism. Hence, I treat assignment mechanisms as policies here.</p>
</div>
<div id="use-cases-and-evaluation-criteria" class="section level2">
<h2>Use Cases and Evaluation Criteria</h2>
<p>Bandit problems arise across many theoretical and applied fields. Everything from industry A/B testing, medical clinical trials, experimental lab studies, and toy problems for reinforcement learning algorithms can be cast as a bandit task. These different domains generally have different goals. A/B tests are conducted to find evidence for an advantage of one version of a product over another while emphasizing classical statistical objectives like minimizing type I error rates or false discovery rates. The goal of clinical experiments is to quickly discover the best treatment so that patient lives can be improved or saved. In simulation settings, bandit problems have been used to benchmark a variety of algorithms in terms of regret. Here I conduct similar benchmarks, while also evaluating standard ‘best proctices’ in terms of type I error rates and false discovery rates.</p>

<div id="type-i-error" class="section level3">
<h3>Type I Error</h3>
<p>Among situations where there is no difference, how often is a difference detected?</p>
<p>Type I error occurs when a null hypothesis is reongly rejected – so when a difference in outcomes of arms is detected when none actually exists.</p>
</div>
<div id="false-discovery-rate" class="section level3">
<h3>False Discovery Rate</h3>
<p>Among detected differences, how many are real?</p>
<p>A false discovery occurs when an arm is selected but is not the actual optimal arm.</p>
<p>I’ll consider this on a trial-by-trial level as well as the result of the overall experiment.</p>
</div>
<div id="regret" class="section level3">
<h3>Regret</h3>
<p>Regret is the difference between the reward that would have been obtained had the optimal action been chosen, and the reward that was actually obtained from the chosen action.</p>
<p><span class="math display">\[
\begin{aligned}
E(Regret) &amp;= \sum_t E(r^*_{t}-r_t)
\end{aligned}
\]</span></p>
</div>
</div>
<div id="uniform-policy" class="section level2">
<h2>Uniform Policy</h2>
<p>This corresponds to a simple random sample type of assignment mechanism – the gold standard for causal inference from experimental data.</p>
<p><span class="math display">\[
\begin{aligned}
\pi(s_t) &amp;= \frac{1}{|\mathcal{A}|} \\
E(Regret) &amp;= TE(r^*_t) - \sum_t E(r_t|\pi) \\
&amp;= T(E(r^*_t) - E(r_t)) \\
\end{aligned}
\]</span></p>
<p>The expected regret on each trial is the difference between the maximal reward and mean reward across actions.</p>
</div>
<div id="greedy-policies" class="section level2">
<h2>Greedy Policies</h2>
<div id="epsilon-greedy-policy" class="section level3">
<h3><span class="math inline">\(\epsilon\)</span>-Greedy Policy</h3>
</div>
<div id="softmax-policy" class="section level3">
<h3>Softmax Policy</h3>
<p>Might be good for parameter estimation while also reducing regret.</p>
<pre class="r"><code>na=5      # Number of arms
as=1:na   # action IDs
beta = 5
R = runif(na) # arm probabilities
#R[1]=1; R[2]=.5; R[3]=0
T = 1500   # Number of trials
q=rep(0,na) 

pis = c()
qs = c()
ats = c()
rts = c()
regret = c()
regrett= 0;

for(ti in 1:T){
# select action
pi = exp(beta*q)/sum(exp(beta*q))
at = rmultinom(n=1,size=as,prob=pi)
at = which(at==1)
# observe outcome  
rt = rbinom(1,size=1,prob=R[at])
# save stats
pis = rbind(pis,pi)
ats = c(ats,at)
rts = c(rts,rt)
# update action values
q[at] = q[at] + .2 * (rt - q[at])
qs = rbind(qs,q)
# regret
regrett= regrett+max(R)-R[at]
regret = c(regret,regrett)
}</code></pre>
<pre class="r"><code>plot(regret,type=&quot;l&quot;)</code></pre>
<p><img src="/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>cols = rainbow(na,s=1,v=.7)
for(ai in as){
if( ai&gt;1){par(new=TRUE)}
plot(qs[,ai],type=&quot;l&quot;,ylim=c(0,1),col=cols[ai],
     main=&quot;Estimation&quot;,
     ylab=&quot;Q-value&quot;,
     xlab=&quot;Trial&quot;)
abline(h=R[ai],col=cols[ai],lty=2)
}</code></pre>
<p><img src="/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<pre class="r"><code>for(ai in as){
if( ai&gt;1){par(new=TRUE)}
plot(pis[,ai],type=&quot;l&quot;,ylim=c(0,1),col=cols[ai],
     main=&quot;Action Probabilities&quot;,
     ylab=&quot;Policy&quot;,
     xlab=&quot;Trial&quot;)
#abline(h=R[ai],col=cols[ai],lty=2)
}</code></pre>
<p><img src="/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-3.png" width="672" /></p>
<pre class="r"><code>ent = 0
ps=c()
for(ai in as){
p = sum(ats==ai)/length(ats)
ent = ent + p*(-log2(p))
ps=c(ps,p)
}
c(ent,ps)</code></pre>
<pre><code>## [1] 1.41206826 0.01533333 0.68200000 0.03400000 0.14333333 0.12533333</code></pre>
<pre class="r"><code>c(entropy.empirical(table(ats),unit=&quot;log2&quot;),freqs.empirical(table(ats)))</code></pre>
<pre><code>##                     1          2          3          4          5 
## 1.41206826 0.01533333 0.68200000 0.03400000 0.14333333 0.12533333</code></pre>
</div>
</div>
<div id="upper-confidence-bound-ucb-policy" class="section level2">
<h2>Upper Confidence Bound (UCB) Policy</h2>
</div>
<div id="thompson-sampling-policy" class="section level2">
<h2>Thompson Sampling Policy</h2>
<pre class="r"><code>#na=5      # Number of arms
#as=1:na   # action IDs
#beta = 5
# keeps same as those above
# R = runif(na) # arm probabilities
#T = 5000   # Number of trials
#priors: rows: actions, col1:alpha, col2:beta
prior = matrix(1,nrow=na,ncol=2)

aposts=c()
bposts=c()
thompsonSamples = c()
ats = c()
rts = c()
regret = c()
regrett= 0;

for(ti in 1:T){
# select action
thompsonSample=c()
for(ai in as){
thompsonSample = c(thompsonSample,rbeta(1,prior[ai,1],prior[ai,2]))
}
at = which.max(thompsonSample)
# observe outcome
rt = rbinom(1,size=1,prob=R[at])
# save stats
thompsonSamples = rbind(thompsonSamples,thompsonSample)
ats = c(ats,at)
rts = c(rts,rt)
# update beta distributions
prior[at,1]=prior[at,1]+rt
prior[at,2]=prior[at,2]+(1-rt)
# save posterior parameters
aposts=rbind(aposts,prior[,1])
bposts=rbind(bposts,prior[,2])
# regret
regrett= regrett+max(R)-R[at]
regret = c(regret,regrett)
}</code></pre>
<pre class="r"><code>plot(regret,type=&quot;l&quot;,
     main=&quot;Regret = max E(reward) -  E(reward|choice)&quot;,
     ylab=&quot;Cumulative Regret&quot;,
     xlab=&quot;Trial&quot;)</code></pre>
<p><img src="/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code># something might be wrong with rbeta(x,vec1,vec2)

cols = rainbow(na,s=1,v=.7)
for(ai in as){
if( ai&gt;1){par(new=TRUE)}
plot(aposts[,ai]/(aposts[,ai]+bposts[,ai]),type=&quot;l&quot;,ylim=c(0,1),col=cols[ai],
     main=&quot;Reward Probability Estimation&quot;,
     ylab=&quot;Posterior Means&quot;,
     xlab=&quot;Trial&quot;)
abline(h=R[ai],col=cols[ai],lty=2)
}
legend(1100,.8,c(&quot;Estimate&quot;,&quot;True&quot;),lty=c(1,2))</code></pre>
<p><img src="/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code>for(ai in as){
if( ai&gt;1){par(new=TRUE)}
plot(thompsonSamples[,ai],type=&quot;l&quot;,ylim=c(0,1),col=cols[ai],
     main=&quot;Posterior Samples&quot;,
     #ylab=expression(&#39;Sampled Value -- Policy=argmax&#39;[&#39;a&#39;]*&#39;(sample&#39;[&#39;a&#39;]*&#39;)&#39;),
     ylab=expression(&#39;Sampled Value&#39;),
     xlab=&quot;Trial&quot;)
#abline(h=R[ai],col=cols[ai],lty=2)
}</code></pre>
<p><img src="/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
<pre class="r"><code>ent = 0
ps=c()
for(ai in as){
p = sum(ats==ai)/length(ats)
ent = ent + p*(-log2(p))
ps=c(ps,p)
}
c(ent,ps)</code></pre>
<pre><code>## [1] 0.109426242 0.001333333 0.988666667 0.003333333 0.001333333 0.005333333</code></pre>
<pre class="r"><code>c(entropy.empirical(table(ats),unit=&quot;log2&quot;),freqs.empirical(table(ats)))</code></pre>
<pre><code>##                       1           2           3           4           5 
## 0.109426242 0.001333333 0.988666667 0.003333333 0.001333333 0.005333333</code></pre>
</div>
</div>
<div id="policy-comparisons" class="section level1">
<h1>Policy Comparisons</h1>
</div>

    </div>

    



    
      






  







<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  
  <img class="portrait mr-3" src="/MacStrelioff/author/admin/avatar_hu27f7dc34c9db8fd2b063f3fba6be9864_288907_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
  

  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/MacStrelioff/authors/admin">Mac Strelioff</a></h5>
    <h6 class="card-subtitle">Cognitive Scientist (Ph.D.) Statistician (M.S.)</h6>
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="/MacStrelioff/#contact" >
          <i class="fas fa-envelope"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://www.linkedin.com/in/macstrelioff/" target="_blank" rel="noopener">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
      
      
      
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://osf.io/4a75z" target="_blank" rel="noopener">
          <i class="fa fa-flask"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://twitter.com/macstrelioff" target="_blank" rel="noopener">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/macstrelioff" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>



      
      
    

    
    <div class="article-widget">
      
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/MacStrelioff/unlisted/youtube_strategy/" rel="next">YouTube Strategy</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/MacStrelioff/unlisted/runningremotejobs/" rel="prev">Bandit Algos for Estimation, Hypothesis Testing, and Decision Making</a>
  </div>
  
</div>

    </div>
    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "@macstrelioff" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  
  <p class="powered-by">
    <a href="/MacStrelioff/files/privacy/">Privacy Policy</a>
  </p>
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/MacStrelioff/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//@macstrelioff.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/MacStrelioff/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/MacStrelioff/js/academic.min.d813ae958640746e240f434cafc95afb.js"></script>

  </body>
</html>

