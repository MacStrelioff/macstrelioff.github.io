<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mac Strelioff</title>
    <link>/MacStrelioff/</link>
    <description>Recent content on Mac Strelioff</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Jun 2030 13:00:00 -0700</lastBuildDate>
    
	    <atom:link href="/MacStrelioff/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Example Talk</title>
      <link>/MacStrelioff/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SQL Solutions</title>
      <link>/MacStrelioff/insightstudying/sql-solutions/</link>
      <pubDate>Wed, 25 Sep 2019 10:36:11 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/sql-solutions/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#general-queries-and-tricks&#34;&gt;General Queries and Tricks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stanford-lagunita-course&#34;&gt;Stanford Lagunita Course&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sql-movie-rating-query-exercises&#34;&gt;SQL Movie-Rating Query Exercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sql-movie-rating-query-exercises-extras&#34;&gt;SQL Movie-Rating Query Exercises Extras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sql-social-network-query-exercises&#34;&gt;SQL Social-Network Query Exercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sql-social-network-query-exercises-extras&#34;&gt;SQL Social-Network Query Exercises Extras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;general-queries-and-tricks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;General Queries and Tricks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Below is a general template for generic queries;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;SELECT (column names to select) [AS (name for selection)]
FROM (table to select from) 
[INNER,LEFT,RIGHT,OUTTER] JOIN table2 
ON [join keys and conditionals]
WHERE (logical condition on rows)
  column = (value or subquery),
  OR column IN (list or subquery)
GROUP BY (column[,column,...])
HAVING [AGGREGATION(column) condition]
LIMIT n [OFFSET m]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stanford-lagunita-course&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Stanford Lagunita Course&lt;/h1&gt;
&lt;div id=&#34;sql-movie-rating-query-exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SQL Movie-Rating Query Exercises&lt;/h2&gt;
&lt;p&gt;1.Find the titles of all movies directed by Steven Spielberg.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;SELECT title
from movie
where director like &amp;quot;Steven%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Find all years that have a movie that received a rating of 4 or 5, and sort them in increasing order.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;SELECT year
FROM movie as m
JOIN rating as r
ON r.mid=m.mid
WHERE r.stars&amp;gt;=4
GROUP BY title
ORDER BY year&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the titles of all movies that have no ratings.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;SELECT distinct title
FROM movie as m
WHERE m.mid not in (select distinct mid from rating)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some reviewers didn’t provide a date with their rating. Find the names of all reviewers who have ratings with a NULL value for the date.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;SELECT distinct name
from rating join reviewer on rating.rid=reviewer.rid
where ratingdate is null&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a query to return the ratings data in a more readable format: reviewer name, movie title, stars, and ratingDate. Also, sort the data, first by reviewer name, then by movie title, and lastly by number of stars.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;SELECT DISTINCT re.name,m.title,r.stars,r.ratingdate
FROM rating r 
JOIN movie m on r.mid=m.mid
JOIN reviewer re on r.rid=re.rid&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For all cases where the same reviewer rated the same movie twice and gave it a higher rating the second time, return the reviewer’s name and the title of the movie.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select reviewer.name,m.title
from rating as r
join movie as m on r.mid=m.mid
join reviewer on r.rid = reviewer.rid
join rating as r2 on r2.ratingdate &amp;gt; r.ratingdate and r2.stars &amp;gt; r.stars and r2.mid=r.mid and r2.rid=r.rid&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each movie that has at least one rating, find the highest number of stars that movie received. Return the movie title and number of stars. Sort by movie title.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select distinct m.title,max(r.stars)
from rating as r
join movie m on r.mid=m.mid
group by m.title&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each movie, return the title and the ‘rating spread’, that is, the difference between highest and lowest ratings given to that movie. Sort by rating spread from highest to lowest, then by movie title.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select m.title,max(r.stars)-min(r.stars)
from rating as r
join movie as m on r.mid=m.mid
group by m.title
order by 2 desc&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the difference between the average rating of movies released before 1980 and the average rating of movies released after 1980. (Make sure to calculate the average rating for each movie, then the average of those averages for movies before 1980 and movies after. Don’t just calculate the overall average rating before and after 1980.)&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select
(
select avg(mavg) from
(
select avg(r.stars) as mavg
from rating as r 
join movie as m 
on r.mid=m.mid 
where m.year &amp;lt; 1980 
group by m.title
) as tab1) -
( select avg(mavg) 
from (
select avg(r.stars) as mavg
from rating as r 
join movie as m 
on r.mid=m.mid 
where m.year &amp;gt; 1980 
group by m.title
) as tab2
)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;sql-movie-rating-query-exercises-extras&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SQL Movie-Rating Query Exercises Extras&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Find the names of all reviewers who rated Gone with the Wind.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;SELECT DISTINCT re.name
FROM rating as r 
JOIN movie as m 
ON r.mid=m.mid AND m.title = &amp;#39;Gone with the Wind&amp;#39;
JOIN reviewer as re 
ON r.rid=re.rid&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For any rating where the reviewer is the same as the director of the movie, return the reviewer name, movie title, and number of stars.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select re.name,m.title,r.stars
from rating as r 
join reviewer as re on r.rid = re.rid
join movie as m on re.name = m.director and r.mid=m.mid&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return all reviewer names and movie names together in a single list, alphabetized. (Sorting by the first name of the reviewer and first word in the title is fine; no need for special processing on last names or removing “The”.)&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select name 
from(
select title as name from movie
union
select name from reviewer
)
order by name&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the titles of all movies not reviewed by Chris Jackson.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select title 
from movie
where mid not in (select ra.mid
from rating as ra 
join reviewer as re on ra.rid=re.rid
join movie as m on ra.mid=m.mid
where re.name LIKE &amp;#39;Chris Jackson&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For all pairs of reviewers such that both reviewers gave a rating to the same movie, return the names of both reviewers. Eliminate duplicates, don’t pair reviewers with themselves, and include each pair only once. For each pair, return the names in the pair in alphabetical order.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select distinct re1.name,re2.name
from rating as ra1 
join rating as ra2 on ra1.mid = ra2.mid
join reviewer as re1 on ra1.rid=re1.rid
join reviewer as re2 on ra2.rid=re2.rid
where re1.name &amp;lt; re2.name 
order by 1,2&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each rating that is the lowest (fewest stars) currently in the database, return the reviewer name, movie title, and number of stars.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select re.name,m.title,ra.stars
from rating as ra
join reviewer as re on ra.rid = re.rid
join movie as m on ra.mid = m.mid
where ra.stars &amp;lt;= (select stars from rating)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;List movie titles and average ratings, from highest-rated to lowest-rated. If two or more movies have the same average rating, list them in alphabetical order.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select m.title, avg(ra.stars)
from rating as ra
join movie as m on ra.mid=m.mid
group by m.title
order by 2 desc, 1 asc&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the names of all reviewers who have contributed three or more ratings. (As an extra challenge, try writing the query without HAVING or without COUNT.)&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select re.name as name
from rating as ra 
join reviewer as re on ra.rid = re.rid
group by re.name
having count(re.name) &amp;gt;= 3&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some directors directed more than one movie. For all such directors, return the titles of all movies directed by them, along with the director name. Sort by director name, then movie title. (As an extra challenge, try writing the query both with and without COUNT.)&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select title,director
from movie
where director in (
select director
from movie
group by director
having count(*) &amp;gt; 1
)
order by 2,1&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the movie(s) with the highest average rating. Return the movie title(s) and average rating. (Hint: This query is more difficult to write in SQLite than other systems; you might think of it as finding the highest average rating and then choosing the movie(s) with that average rating.)&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select title, avg 
from 
(
select ra.mid, m.title as title, avg(stars) as avg
from rating as ra join movie m on ra.mid=m.mid
group by ra.mid
)
where avg = (select max(avg) from (
select avg(stars) as avg
from rating as ra join movie m on ra.mid=m.mid
group by ra.mid
))&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the movie(s) with the lowest average rating. Return the movie title(s) and average rating. (Hint: This query may be more difficult to write in SQLite than other systems; you might think of it as finding the lowest average rating and then choosing the movie(s) with that average rating.)&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select title, avg 
from 
(
select ra.mid, m.title as title, avg(stars) as avg
from rating as ra join movie m on ra.mid=m.mid
group by ra.mid
)
where avg = (select min(avg) from (
select avg(stars) as avg
from rating as ra join movie m on ra.mid=m.mid
group by ra.mid
))&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each director, return the director’s name together with the title(s) of the movie(s) they directed that received the highest rating among all of their movies, and the value of that rating. Ignore movies whose director is NULL.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select distinct m.director,m.title,max(ra.stars)
from rating as ra
join reviewer as re on ra.rid=re.rid
join movie as m on ra.mid = m.mid
where m.director is not null
group by m.director
having max(ra.stars)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;sql-social-network-query-exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SQL Social-Network Query Exercises&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Find the names of all students who are friends with someone named Gabriel.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h2.name
from friend
join highschooler h1 on friend.ID2=h1.ID
join highschooler h2 on friend.ID1=h2.ID
where h1.name = &amp;#39;Gabriel&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For every student who likes someone 2 or more grades younger than themselves, return that student’s name and grade, and the name and grade of the student they like.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h1.name,h1.grade,h2.name,h2.grade
from likes as l
join highschooler as h1 on l.ID1 = h1.id
join highschooler as h2 on l.ID2 = h2.id
where (h1.grade - h2.grade) &amp;gt;= 2&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For every pair of students who both like each other, return the name and grade of both students. Include each pair only once, with the two names in alphabetical order.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h1.name,h1.grade,h2.name,h2.grade
from likes as l1
join likes as l2 on l1.id1=l2.id2 and l1.id2 = l2.id1 /* like eachother */
join highschooler h1 on l1.id1=h1.id
join highschooler h2 on l1.id2=h2.id
where h1.name &amp;lt; h2.name /* alphabetical order across columns */
order by 1,2&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find all students who do not appear in the Likes table (as a student who likes or is liked) and return their names and grades. Sort by grade, then by name within each grade.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h.name,h.grade
from highschooler h
where h.id not in (select id1 from likes union select id2 from likes)
order by 2,1&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For every situation where student A likes student B, but we have no information about whom B likes (that is, B does not appear as an ID1 in the Likes table), return A and B’s names and grades.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select hA.name,hA.grade,h.name,h.grade
from highschooler as h
join likes on h.id=likes.id2
join highschooler as hA on likes.id1=hA.id
where h.id not in (select id1 from likes)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find names and grades of students who only have friends in the same grade. Return the result sorted by grade, then by name within each grade.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h.name,h.grade
from highschooler as h
where h.id not in 
( /* list of ids of friends in different grades */
select f1.id1
from friend f1
join highschooler h1 on f1.id1=h1.id
join highschooler h2 on f1.id2=h2.id
where h1.grade &amp;lt;&amp;gt; h2.grade
)
order by 2,1&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each student A who likes a student B where the two are not friends, find if they have a friend C in common (who can introduce them!). For all such trios, return the name and grade of A, B, and C.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select ha.name,ha.grade,hb.name,hb.grade,hc.name,hc.grade
from likes
join friend f1 on likes.id1=f1.id2 
join friend f2 on likes.id2=f2.id2 and f2.id1=f1.id1
join highschooler ha on likes.id1=ha.id
join highschooler hb on likes.id2=hb.id
join highschooler hc on f1.id1=hc.id
/* list of like ids who are friends */
where likes.id1 not in 
(select likes.id1
from likes
join friend f1 on likes.id1=f1.id1
where likes.id2 = f1.id2
)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the difference between the number of students in the school and the number of different first names.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select count(*)-count(distinct name)
from highschooler&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the name and grade of all students who are liked by more than one other student.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h.name,h.grade
from likes
join highschooler as h on likes.id2=h.id
group by h.id
having count(likes.id2) &amp;gt; 1&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;sql-social-network-query-exercises-extras&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SQL Social-Network Query Exercises Extras&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For every situation where student A likes student B, but student B likes a different student C, return the names and grades of A, B, and C.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h1.name,h1.grade,h2.name,h2.grade,h3.name,h3.grade
from likes l1
join likes l2 on l1.id2=l2.id1
join highschooler h1 on l1.id1=h1.id
join highschooler h2 on l1.id2=h2.id
join highschooler h3 on l2.id2=h3.id
where l1.id1 &amp;lt;&amp;gt; l2.id2 /* filters out cycles */&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;(3 min) Find those students for whom all of their friends are in different grades from themselves. Return the students’ names and grades.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h.name,h.grade
from highschooler as h
where h.id not in 
( /* list of ids with friends in same grade */
select distinct h1.id
from friend as f
join highschooler h1 on f.id1=h1.id
join highschooler h2 on f.id2=h2.id
where h1.grade=h2.grade
)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the average number of friends per student? (Your result should be just one number.)&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select avg(nfriends) 
from
(select count(f1.id2) as nfriends
from friend f1
join highschooler h1 on h1.id=f1.id1
group by h1.id
)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the number of students who are either friends with Cassandra or are friends of friends of Cassandra. Do not count Cassandra, even though technically she is a friend of a friend.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select count(*)
from friend f1
join friend f2 on f1.id2=f2.id1
join highschooler h on h.id=f1.id1 
where h.name = &amp;#39;Cassandra&amp;#39; &lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the name and grade of the student(s) with the greatest number of friends.&lt;/p&gt;
&lt;pre class=&#34;{,eval=false,echo=true}&#34;&gt;&lt;code&gt;select h.name,h.grade
from friend f join highschooler h on h.id=f.id1
group by h.id
having count(*) = 
(
select max(nfriends) from
(
select count(*) as nfriends
from friend f
group by f.id1
)
)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Basic Computer Science</title>
      <link>/MacStrelioff/insightstudying/computer-science/</link>
      <pubDate>Tue, 24 Sep 2019 17:27:18 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/computer-science/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#foundations&#34;&gt;Foundations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#complexity&#34;&gt;Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#computational-complexity&#34;&gt;Computational Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#constant-time-o1&#34;&gt;Constant Time: O(1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logrithmic-time-ologn&#34;&gt;Logrithmic Time: O(log(n))&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polynomial-time-on-on2&#34;&gt;Polynomial Time: O(n), O(n^2), …&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exponential-time-oxn&#34;&gt;Exponential Time: &lt;span class=&#34;math inline&#34;&gt;\(O(x^n)\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-structures&#34;&gt;Data Structures&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lists-and-dictionaries&#34;&gt;Lists and Dictionaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#queues-and-stacks&#34;&gt;Queues and Stacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graphs-and-trees&#34;&gt;Graphs and Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sorting&#34;&gt;Sorting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#searching&#34;&gt;Searching&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear-search-binary-search&#34;&gt;Linear Search: Binary Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graph-search&#34;&gt;Graph Search&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#breadth-first-bfs-and-queues&#34;&gt;Breadth First (BFS) and queues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#depth-first-dfs&#34;&gt;Depth First (DFS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dfs-vs-bfs&#34;&gt;DFS vs BFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examples&#34;&gt;Examples:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pointers&#34;&gt;Pointers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#algorithms&#34;&gt;Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#todo-other-topics&#34;&gt;TODO / Other Topics:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recursion&#34;&gt;Recursion;&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#dijkstras-algorithm&#34;&gt;Dijkstra’s algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dynamic-programming-knapsack-problem-fibonacci-sequence&#34;&gt;dynamic programming (knapsack problem, Fibonacci sequence)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functional-programming&#34;&gt;Functional programming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tools&#34;&gt;Tools&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#git-and-github&#34;&gt;Git and GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sql&#34;&gt;SQL&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#general-structure-of-sql-querys&#34;&gt;General Structure of SQL querys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aggregation-functions&#34;&gt;Aggregation functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#window-functions&#34;&gt;Window functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tricks&#34;&gt;Tricks:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;!--
basic structures (tuples, list, arrays, dict), list comprehensions, 
Data structures (hash, stack, queue, tree, linked list);
sorting (bubble, selection, insertion, merge, quick)
Sorting and graph algos (depth-first and breadth-first seach)
sorting (heap, tim)
--&gt;
&lt;div id=&#34;foundations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Foundations&lt;/h1&gt;
&lt;p&gt;Data type manipulation (char, string, numeric, binary, ascii) regular expressions;&lt;/p&gt;
&lt;div id=&#34;complexity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Complexity&lt;/h2&gt;
&lt;p&gt;The worst-case complexity of an algorithm is represented with big-O notation. Big-O notation is adapted from mathematics where &lt;span class=&#34;math inline&#34;&gt;\(O(f(n))\)&lt;/span&gt; is used to represent the terms that remain relevant when taking a limit of the computations required by the algorithm, &lt;span class=&#34;math inline&#34;&gt;\(f(n)\)&lt;/span&gt;, as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; approaches &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. The same notation is used for time complexity (number of operations) and for space complexity (memory requirements).&lt;/p&gt;
&lt;div id=&#34;computational-complexity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Computational Complexity&lt;/h3&gt;
&lt;p&gt;Here I’ll check the number of computations required by algorithms of different complexities.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ns_to_test = [0,1,10,20]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;constant-time-o1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Constant Time: O(1)&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(O(1)\)&lt;/span&gt; represents comstant time complexity – a component of an algorithm that is only performed once, regardless of the input size.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def constant_example(n):
    num_ops = 1      # 1 operation
    num_ops +=1      # 1 operation
    return num_ops   # total: 2 operations


for n in ns_to_test:
    out=constant_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 2
&amp;gt;&amp;gt;&amp;gt; f(1): 2
&amp;gt;&amp;gt;&amp;gt; f(10): 2
&amp;gt;&amp;gt;&amp;gt; f(20): 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logrithmic-time-ologn&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Logrithmic Time: O(log(n))&lt;/h4&gt;
&lt;p&gt;Very slowly increases in computational demand. It can result from splitting the input on each recusrive call.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def log_example(n,num_ops=0):
    # 1 operation per call
    # if n&amp;gt;1, half n and recursively call again
    if n&amp;gt;=1:
        n/=2; num_ops += 1
        num_ops += log_example(n)
    return num_ops
        
for n in ns_to_test:
    out=log_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 0
&amp;gt;&amp;gt;&amp;gt; f(1): 1
&amp;gt;&amp;gt;&amp;gt; f(10): 4
&amp;gt;&amp;gt;&amp;gt; f(20): 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-time-on-on2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Polynomial Time: O(n), O(n^2), …&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=P8Xa2BitN3I&amp;amp;list=PLI1t_8YX-ApvMthLj56t1Rf-Buio5Y8KL&amp;amp;index=11&#34;&gt;good hackerrank video with explanation of polynomial complexity for a recursive solution to fibbinochi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Each for loop scales complexity by a factor of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, so one loop would be linear (&lt;span class=&#34;math inline&#34;&gt;\(O(n)\)&lt;/span&gt;), two loops would be quadratic &lt;span class=&#34;math inline&#34;&gt;\(O(n^2)\)&lt;/span&gt;, and three loops would be cubic (&lt;span class=&#34;math inline&#34;&gt;\(O(n^3)\)&lt;/span&gt;), and so on. The example below is a quadratic time example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def linear_example(n): 
    num_ops = 0         # initialize counter
    for i in range(n):  # n times, perform...
        num_ops +=1     # 1 operation (n*1)
    return num_ops      # 1+n*1 = 1+n operations, O(n)

for n in ns_to_test:
    out=linear_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 0
&amp;gt;&amp;gt;&amp;gt; f(1): 1
&amp;gt;&amp;gt;&amp;gt; f(10): 10
&amp;gt;&amp;gt;&amp;gt; f(20): 20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def quadratic_example(n):
    num_ops = 0             # initialize counter
    for i in range(n):      # n times
        for j in range(n):  # n times (n*n)
            num_ops+=1      # 1 operation (1*n*n)
    return num_ops          # total 1 + 1*n + 1*n*n, O(n^2)

for n in ns_to_test:
    out=quadratic_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 0
&amp;gt;&amp;gt;&amp;gt; f(1): 1
&amp;gt;&amp;gt;&amp;gt; f(10): 100
&amp;gt;&amp;gt;&amp;gt; f(20): 400&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-time-oxn&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Exponential Time: &lt;span class=&#34;math inline&#34;&gt;\(O(x^n)\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;With exponential time, the number of operations increases by a constant &lt;em&gt;factor&lt;/em&gt; whith the length of the input. An example of this is in recursion, where a function iteratively calls itself &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times for each element of the input.&lt;/p&gt;
&lt;p&gt;In the example below, I call the function twice within each call. The complexity is then a geometric series, and the closed for solution for the number of operations can be found with;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_{i=1}^N ar^{i-1} = \frac{a(1-r^{N})}{1-r} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(r=2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(a=2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is looped over, and we start the index at 0 (&lt;span class=&#34;math inline&#34;&gt;\(i-1\)&lt;/span&gt;) instead of 1, so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_{i=0}^n 2*2^{i} = \frac{(1-2^{n+1})}{1-2} = \frac{(1-2^{n+1})}{-1}=-(1-2^{n+1}) = 2^{n+1}-1 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recursive calls generally have time complexity of &lt;span class=&#34;math inline&#34;&gt;\(O(branches^{depth})\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def exponential_example(n,num_ops=0):
    num_ops += 1 # 1 operation per call
    # recursively called twice for each n&amp;gt;0 (2**n times)    
    if n &amp;gt; 0:
        n-=1
        num_ops += exponential_example(n) # 2**(n)
        num_ops += exponential_example(n) # 2**(n)
    return num_ops # 
    
for n in ns_to_test:
    out=exponential_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 1
&amp;gt;&amp;gt;&amp;gt; f(1): 3
&amp;gt;&amp;gt;&amp;gt; f(10): 2047
&amp;gt;&amp;gt;&amp;gt; f(20): 2097151&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-structures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Structures&lt;/h2&gt;
&lt;div id=&#34;lists-and-dictionaries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lists and Dictionaries&lt;/h3&gt;
&lt;p&gt;Good descriptions of &lt;a href=&#34;https://runestone.academy/runestone/books/published/pythonds/AlgorithmAnalysis/Lists.html&#34;&gt;lists&lt;/a&gt;, &lt;a href=&#34;https://runestone.academy/runestone/books/published/pythonds/AlgorithmAnalysis/Dictionaries.html&#34;&gt;dicts&lt;/a&gt;, and &lt;a href=&#34;https://stackoverflow.com/questions/8929284/what-makes-sets-faster-than-lists&#34;&gt;why some operations are much faster in sets or dict keys&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In depth description of &lt;a href=&#34;https://en.wikipedia.org/wiki/Hash_table?fbclid=IwAR1N5-jAoM9e6iY58CDP9MAwycenvOXXJwmkpa0eBVDKed3RBs9uVBm9Kkc&#34;&gt;hash tables&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;queues-and-stacks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Queues and Stacks&lt;/h3&gt;
&lt;p&gt;Queues and stacks are like lists that are used to prioritize operations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Queues are First-in-last-out structures – meaning that the highest priority is given to the least recently queued thing. Imagine people in a queue for a theme park, those who entered the queue first are prioritized for admission to the park.&lt;/li&gt;
&lt;li&gt;Stacks are Last-in-First-out structures – meaning that the highest priority is given to the most recently stacked element. Imagine a stack of books, the ones at the top of the stack are read first.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;graphs-and-trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Graphs and Trees&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jenniferbland.com/the-difference-between-a-tree-and-a-graph-data-structure/&#34;&gt;short definition with pictures&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sorting&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://www.geeksforgeeks.org/sorting-algorithms/&#34;&gt;sort algorithms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Python’s sotred() function uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Timsort&#34;&gt;TimSort&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Complexity for common sorts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(O(Nlog(N))\)&lt;/span&gt; for quicksort, and merge sort.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In efficient sorting algorithms, the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; part comes from the algorithm being called on up to each element, and the &lt;span class=&#34;math inline&#34;&gt;\(logN\)&lt;/span&gt; part comes from binary sorting in ways that render parts of the array unnecessary later.&lt;/p&gt;
&lt;p&gt;e.g. ‘find median in a large dataset’ – sort half of it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;searching&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Searching&lt;/h1&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Category:Search_algorithms&#34;&gt;search algorithms&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;linear-search-binary-search&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Search: Binary Search&lt;/h2&gt;
&lt;p&gt;Complexity &lt;span class=&#34;math inline&#34;&gt;\(O(logN)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=P3YID7liBug&#34;&gt;good hackerrank video&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;requires sorting before applying the search&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;graph-search&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Graph Search&lt;/h2&gt;
&lt;p&gt;Graphs are a collection of nodes and edges (connections between nodes). In the code below, I use dictionaries to represent graphs – keys represent nodes, and the list of elements represents all of the nodes that the parent node is connected to.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# examples
Examples = []

# Example 1: something where DFS would do equally well?
tmp_graph = {1: [2,4], 2: [3], 3: [5], 4: [5,7], 5: [6,7], 6: [], 7: []}
Examples.append(tmp_graph)

# Example 2: something where DFS will run faster
tmp_graph = {1:[2,3,4,5],2:[6,10,8,9],3:[],4:[],5:[],6:[7],7:[],8:[],9:[],10:[]}
Examples.append(tmp_graph)

# Example 3: something where BFS will run faster (swapped 3 and 10)
tmp_graph = {1:[2,7,4,5],2:[6,10,8,9],3:[],4:[],5:[],6:[3],7:[],8:[],9:[],10:[]}
Examples.append(tmp_graph)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;breadth-first-bfs-and-queues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Breadth First (BFS) and queues&lt;/h3&gt;
&lt;p&gt;Breadth Frist Search (BFS) queues (FILO structures) nodes based on their distance from an origin node. This can be implemented with a list (queue) by searching the nodes at the head of the list and appending encountered nodes to the tail of the list.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# breadth first search: just a queue
def BFS_queue(graph,start,end):
    queue = [start] 
    visited = set()
    # build and search through queue
    while queue:
        visiting = queue[0]
        # if this was visited already, continue
        if visiting in visited: continue
        # mark node as visited
        visited.add(visiting)
        # if this is the end, return 
        if visiting == end: return True,visited
        # otherwise, append children to search queue
        for child in graph[visiting]: queue.append(child)
        queue.remove(visiting) # remove the visited node
    # if end was never found, return False
    return False,visited

# breadth first search: For loop over each layer
def BFS(graph,start,end):
    queue = [start] 
    visited = set()
    # build and search through queue
    while queue:
        for visiting in queue:
            # if this was visited already, continue
            if visiting in visited: continue
            # mark node as visited
            visited.add(visiting)
            # if this is the end, return 
            if visiting == end: return True,visited
            # otherwise, append children to search queue
            for child in graph[visiting]:
                queue.append(child)
    # if end was never found, return False
    return False,visited&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;depth-first-dfs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Depth First (DFS)&lt;/h3&gt;
&lt;p&gt;Depth First search can be implemented with recursively (below) or with a stack (LIFO structures). &lt;a href=&#34;https://www.youtube.com/watch?v=zaBhtODEL0w&#34;&gt;good hackerrank video&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example using DFS to find if a path to a node exists.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Depth first search with recursion
def DFS(graph,start,end):
    visited = set()
    return _DFS(graph,start,end,visited),visited
    
def _DFS(graph,start,end,visited):
    # if this was visited already, return False
    if start in visited: return False
    # mark node as visited
    visited.add(start)
    # if this is the target node, return True
    if start == end: return True
    # DFS through the children of this node
    for child in graph[start]:
        if _DFS(graph,child,end,visited):
            return True
    return False&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dfs-vs-bfs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DFS vs BFS&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
for example in Examples:
    print(&amp;#39;DFS  path: &amp;#39;, DFS(example,1,7))
    print(&amp;#39;BFSq path: &amp;#39;, BFS(example,1,7))
    print(&amp;#39;BFS  path: &amp;#39;, BFS(example,1,7),&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; DFS  path:  (True, {1, 2, 3, 5, 6, 7})
&amp;gt;&amp;gt;&amp;gt; BFSq path:  (True, {1, 2, 3, 4, 5, 7})
&amp;gt;&amp;gt;&amp;gt; BFS  path:  (True, {1, 2, 3, 4, 5, 7}) 
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; DFS  path:  (True, {1, 2, 6, 7})
&amp;gt;&amp;gt;&amp;gt; BFSq path:  (True, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10})
&amp;gt;&amp;gt;&amp;gt; BFS  path:  (True, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}) 
&amp;gt;&amp;gt;&amp;gt; 
&amp;gt;&amp;gt;&amp;gt; DFS  path:  (True, {1, 2, 3, 6, 7, 8, 9, 10})
&amp;gt;&amp;gt;&amp;gt; BFSq path:  (True, {1, 2, 7})
&amp;gt;&amp;gt;&amp;gt; BFS  path:  (True, {1, 2, 7})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Examples:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;L332: Reconstruct a path from a starting node that hits all nodes once&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
tickets = [[&amp;quot;JFK&amp;quot;,&amp;quot;KUL&amp;quot;],[&amp;quot;JFK&amp;quot;,&amp;quot;NRT&amp;quot;],[&amp;quot;NRT&amp;quot;,&amp;quot;JFK&amp;quot;]]

# convert to graph implemented in dict:
import collections
tree = collections.defaultdict(list)
for parent,child in tickets:
    tree[parent].append(child)

# start at JFK
path = [&amp;quot;JFK&amp;quot;]

# define DFS algo
def dfs(parent = &amp;#39;JFK&amp;#39;):
    # success condition -- all tickets have been accounted for
    if len(path) == len(tickets) + 1: 
        print(&amp;quot;Final Path: {}&amp;quot;.format(path))
        return path
    # get destinations (children) from current node
    children = sorted(tree[parent])
    # iterate over children of this node
    for child in children:
        # include child in path
        tree[parent].remove(child)
        path.append(child)
        # call DFS again starting at the child node
        print(&amp;quot;Checking: {} -&amp;gt; {}&amp;quot;.format(parent,child))
        sub_route = dfs(child)
        if sub_route: 
            return sub_route
        else: print(&amp;quot;No successful sub-route&amp;quot;)
        # implicitely return None if sub_route empty
        # if no successful sub-route, remove child from path, 
        path.pop()
        #  and add child back to parent to be searched later
        tree[parent].append(child)

# run the dfs algo
dfs()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Checking: JFK -&amp;gt; KUL
&amp;gt;&amp;gt;&amp;gt; No successful sub-route
&amp;gt;&amp;gt;&amp;gt; Checking: JFK -&amp;gt; NRT
&amp;gt;&amp;gt;&amp;gt; Checking: NRT -&amp;gt; JFK
&amp;gt;&amp;gt;&amp;gt; Checking: JFK -&amp;gt; KUL
&amp;gt;&amp;gt;&amp;gt; Final Path: [&amp;#39;JFK&amp;#39;, &amp;#39;NRT&amp;#39;, &amp;#39;JFK&amp;#39;, &amp;#39;KUL&amp;#39;]
&amp;gt;&amp;gt;&amp;gt; [&amp;#39;JFK&amp;#39;, &amp;#39;NRT&amp;#39;, &amp;#39;JFK&amp;#39;, &amp;#39;KUL&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;!--
# Ranking Algorithms

Lots of [ranking algorithms](https://en.wikipedia.org/wiki/Ranking_(information_retrieval))

# Recommendation

Lots of [recommendation algorithms](https://en.wikipedia.org/wiki/Recommender_system)
--&gt;
&lt;!---
Data science methods play a major role in discovering recommendations for users. 

Econ, compliments and substitutes (competitors). 

Types of recommendation; compliments (these go together), competitors (you might also like...), temporal (might buy this again in the future). Horizontal and vertical focus 

Temporal involves forecasting, which is a topic for another post. 
---&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pointers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pointers&lt;/h2&gt;
&lt;p&gt;e.g. Find longest substring that is a pallendrome&lt;/p&gt;
&lt;p&gt;Optimal way: Start at each letter, move out each&lt;/p&gt;
&lt;p&gt;e.g. reverse a linked list&lt;/p&gt;
&lt;p&gt;e.g. max price from a single buy and sell?&lt;/p&gt;
&lt;p&gt;e.g. swapping values in an array?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;algorithms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Algorithms&lt;/h1&gt;
&lt;p&gt;lambda functions;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
nums=[1,1,2,2,1]
val=1

count = 0
for i in range(len(nums)):
  if nums[i] != val:
    nums[count] = nums[i]
    count += 1

print(count)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(nums)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; [2, 2, 2, 2, 1]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;todo-other-topics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TODO / Other Topics:&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;recursion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recursion;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Recursive calls&lt;/li&gt;
&lt;li&gt;Methods applied within each recursion&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;dijkstras-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dijkstra’s algorithm&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;dynamic-programming-knapsack-problem-fibonacci-sequence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;dynamic programming (knapsack problem, Fibonacci sequence)&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;functional-programming&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Functional programming&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tools&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tools&lt;/h1&gt;
&lt;div id=&#34;git-and-github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Git and GitHub&lt;/h2&gt;
&lt;p&gt;git for version control (resolving merge conflicts)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sql&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SQL&lt;/h2&gt;
&lt;div id=&#34;general-structure-of-sql-querys&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;General Structure of SQL querys&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sqlbolt.com/lesson/select_queries_order_of_execution&#34;&gt;Order of execution for clauses below&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;```{, eval=FALSE, echo=TRUE} SELECT (column names to select) [AS (name for selection)]&lt;/p&gt;
&lt;p&gt;FROM (table to select from) [INNER,LEFT,RIGHT,OUTTER] JOIN table2 ON [join keys and conditionals]&lt;/p&gt;
&lt;p&gt;WHERE (logical condition on rows) column = (value or subquery), OR column IN (list or subquery)&lt;/p&gt;
&lt;p&gt;GROUP BY (column[,column,…])&lt;/p&gt;
&lt;p&gt;HAVING [AGGREGATION(column) condition]&lt;/p&gt;
&lt;p&gt;LIMIT n [OFFSET m] ```&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;SELECT&lt;/code&gt; clause can contain columns, aggregations, window functions, and subqueries.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FROM&lt;/code&gt; clause can contain a single table, or joins between tables&lt;/li&gt;
&lt;li&gt;&lt;code&gt;JOIN&lt;/code&gt;s are useful for appending tables as columns, versus&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ON&lt;/code&gt; filters the table that is returned from the JOIN – it acts similarly to the WHERE clause.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WHERE&lt;/code&gt; clause filters the table retreived by FROM can also contain sub queries, and can perform the same filtering as &lt;code&gt;ON&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GROUP BY&lt;/code&gt; groups by a single column or a set of values if given multiple columns&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HAVING&lt;/code&gt; can be used to filter groups based on an aggregation function&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UNION&lt;/code&gt; appends tables as rows&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregation-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Aggregation functions&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;GROUP BY&lt;/code&gt; clause can contain &lt;code&gt;GROUPING SETS&lt;/code&gt;, such as a &lt;code&gt;ROLLUP()&lt;/code&gt; and &lt;code&gt;CUBE()&lt;/code&gt; functions. These hierarchically group the rows passed to the aggregation functions. &lt;code&gt;ROLLUP()&lt;/code&gt; hierarchically groups by arguments, starting left to right. &lt;code&gt;CUBE()&lt;/code&gt; passes all permutations of its arguments to the aggregation functions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pgexercises.com/questions/aggregates/fachoursbymonth3.html&#34;&gt;Grouping sets are described a little here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;window-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Window functions&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://pgexercises.com/questions/aggregates/countmembers.html&#34;&gt;Orver() explained here&lt;/a&gt;. Some more detailed information &lt;a href=&#34;http://www.sqlservertutorial.net/sql-server-window-functions/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Window functions include &lt;code&gt;OVER()&lt;/code&gt; and &lt;code&gt;ROW_NUMBER()&lt;/code&gt;. They take two optional arguments, &lt;code&gt;PARTITION BY&lt;/code&gt; and &lt;code&gt;ORDER BY&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;RANK() OVER([partition_by_clause] order_by_clause)&lt;/code&gt;: ranks the column specified in the order by clause, ties are assigned the same rank and result in skipped values&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DENSE_RANK() OVER([partition_by_clause] order_by_clause)&lt;/code&gt;: ranks the column specified in the order by clause, ties are assigned the same rank, but no rank number is skipped&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LAG(column,lag) OVER([partition_by_clause] order_by_clause)&lt;/code&gt;: Retreives values from a &lt;code&gt;lag&lt;/code&gt; rows before, see &lt;a href=&#34;http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/&#34;&gt;example with a partition here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;tricks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tricks:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;When wanting to loop and compare, one can do a join or add a subquery and compare all columns at once. &lt;a href=&#34;https://leetcode.com/problems/department-highest-salary/discuss/53607/Three-accpeted-solutions&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SQL training resources;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://academy.vertabelo.com/#courses_list_section&#34;&gt;vertabelo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pgexercises.com/&#34;&gt;pgexercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://classroom.udacity.com/courses/ud198&#34;&gt;Udacity course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sqlbolt.com/lesson/select_queries_introduction&#34;&gt;sqlbolt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/&#34;&gt;Mode Analytics&lt;/a&gt; HackerRank (basic select, aggregation)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.hackerrank.com/domains/sql&#34;&gt;Hacker Rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.w3schools.com/sql/exercise.asp?filename=exercise_select1&#34;&gt;W3School&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Python and other coding languages:&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.hackerrank.com/dashboard&#34;&gt;HackerRank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://leetcode.com/&#34;&gt;LeetCode&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overview videos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Top 10 Algorithms for the Coding Interview &lt;a href=&#34;https://www.youtube.com/watch?v=r1MXwyiGi_U&#34;&gt;Part I&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=zHczhZn-z30&#34;&gt;Part II&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Instacart Notes</title>
      <link>/MacStrelioff/insightstudying/instacart-deep-dive/</link>
      <pubDate>Tue, 24 Sep 2019 17:21:25 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/instacart-deep-dive/</guid>
      <description>


&lt;div id=&#34;todo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TODO:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Check out info from &lt;a href=&#34;https://medium.com/acing-ai/instacart-data-science-interview-questions-e8d89bea1a34&#34;&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read articles on their &lt;a href=&#34;https://tech.instacart.com/tagged/data-science&#34;&gt;DS blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/jobs/view/748128414/?refId=aa6b5d2c-8b03-4c2d-9643-1171a9fca283&amp;amp;trk=d_flagship3_company&#34;&gt;LinkedIn application for data scientist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://boards.greenhouse.io/instacart/jobs/1863911&#34;&gt;Instacart posting for data scientist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/jobs/view/1331659318/?refId=aa6b5d2c-8b03-4c2d-9643-1171a9fca283&amp;amp;trk=d_flagship3_company&#34;&gt;LinkedIn app for analyst&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;recruiter-call&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recruiter call&lt;/h1&gt;
&lt;p&gt;“Just be prepared to share with me what you are looking for, what is exciting to you and feel free to have questions ready about the team, role, business etc”&lt;/p&gt;
&lt;!-- excitement --&gt;
&lt;ul&gt;
&lt;li&gt;Groceries have been hard for internet companies to get right – it’s exciting to see Instacart managing this well, and I’d be excited to be a part of this technological revolution of grocery delivery&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- questions --&gt;
&lt;ul&gt;
&lt;li&gt;More about role?&lt;/li&gt;
&lt;li&gt;general interview at this point, then team placement later?&lt;/li&gt;
&lt;li&gt;to see what product team is best fit, then match that with their priorities&lt;/li&gt;
&lt;li&gt;data cleaning, kpis, models&lt;/li&gt;
&lt;li&gt;experimentation heavy&lt;/li&gt;
&lt;li&gt;DS vs ml; DS models increase understanding and guide decisions&lt;/li&gt;
&lt;li&gt;hiring in customer - growth, retention and engagement, marketing, core product&lt;/li&gt;
&lt;li&gt;and in performance - delivery logistics and operations for shoppers, fulfillment, shoppers and delivery&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;surrounding shopper model incentivized by flexibility in hours, and pay structure.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;competitive - they could be in postmates, doordash, …&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;incitives, earnings, …&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;focused on incentivizing quality rather and balancing with speed.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;experiment with different incentive structures..&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;and in ads platform - cupons, product promotions (buy 1 get 1 free), …&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;all from brand partners&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;sales drive brand partners into business&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;DS around 10 individuals&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;currently have 5 products in ads now, some tools help partners decide how to spend dollars&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Specific needs or challenges?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Instacart growing at a pace faster than others in the bay area&lt;/li&gt;
&lt;li&gt;DS only 25 individuals, still one head of DS&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DS highly valuable and high ownership&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Sr contributor, prioritization important.&lt;/li&gt;
&lt;li&gt;Each product team is cross functional, 1DS, 1 designer, 1-8 SWE, and a product manager. Desks are organized around these.&lt;/li&gt;
&lt;li&gt;Owning metrics with the product teams.&lt;/li&gt;
&lt;li&gt;E.g. earnings model might be partnering with growth team and delivery logistics side&lt;/li&gt;
&lt;li&gt;Report into a DS leader for mentorship, collaboration, … .&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Still have autonomy and collaboration in day to day&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;variety of team members in interview to chat about different projects&lt;/li&gt;
&lt;li&gt;&lt;p&gt;feel free to ask about their day to day&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;problem solving phone interview (45min) - business case problem from a Sr DS, high level instacart related problem. E.g. how would we gain more customers, retain more customers. Something you’d think of as a DS. ask insightful questions, setup framework correctly, back up analysis and solution over phone. Looking how you organize thoughts and pivot thinking, might have a wrench thrown.&lt;/li&gt;
&lt;li&gt;statistics SQL phone interview (45min). Stats 101, SQL is a necessity so quick 15 min exercise should be straightforward. – coding over shared doc?&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;no, first 30min over phone, next 30min online through a link&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Onsite interview with a lot of topics. Experimental design, live DS project with clean data. Want to see how I solve it and present answers. Multiple ways of answering prompt.&lt;/li&gt;
&lt;li&gt;project in three sessions;&lt;/li&gt;
&lt;li&gt;1 framework and metrics, 2 doing the work, 3 presentation to team of DS, strengths and weaknesses and justification of choices&lt;/li&gt;
&lt;li&gt;&lt;p&gt;experimental design, problem solving/product sense&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;hiring leader on customers will be out of office starting October 7th&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;will be asked about compensition at onsite.&lt;/li&gt;
&lt;li&gt;DS, Sr DS, Staff DS – depends on interviews, and determines pay range. Will get cash based and equity in RSUs and then benefits.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sr range between 170-190k on cash. Look at technical contributions, level of inititive, thought leadership, 400-600k over 4 years of equity.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-solving-phone-interview-45min&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Problem solving phone interview (45min)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;business case problem from a Sr DS, high level instacart related problem.&lt;/li&gt;
&lt;li&gt;E.g. how would we gain more customers, retain more customers.&lt;/li&gt;
&lt;li&gt;Something you’d think of as a DS.&lt;/li&gt;
&lt;li&gt;ask insightful questions,&lt;/li&gt;
&lt;li&gt;setup framework correctly,&lt;/li&gt;
&lt;li&gt;back up analysis and solution over phone&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Looking how you organize thoughts and pivot thinking, might have a wrench thrown.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;‘How do we get more customers?’&lt;/li&gt;
&lt;li&gt;&lt;p&gt;referral bonuses, ads, ads + discount codes&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;increase customers&lt;/li&gt;
&lt;li&gt;&lt;p&gt;retain customers&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Possible topics --&gt;
&lt;ul&gt;
&lt;li&gt;Improve predictability for shopper experience&lt;/li&gt;
&lt;li&gt;Inventory prediction or modeling&lt;/li&gt;
&lt;li&gt;Model for item substitutes (crowd source this so that orderers can specify acceptable substitutes?)&lt;/li&gt;
&lt;li&gt;Labor models in local areas, accounting for unions and labor laws&lt;/li&gt;
&lt;li&gt;Trust with person delivering produce&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Insight Mock Qs --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Instacart (a grocery delivery app) implemented a pickup option at a subset of grocery stores that partner with the company. The subset of grocery stores that got the pickup option was not randomly assigned. How would you figure out whether the revenue from pickup is incremental to delivery? In other words, is pickup cannibalizing delivery?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At Instacart, if you order 1-7 items, the delivery window is 1 hour. If you order 8-24 items, the window is 2 hours. If you order 25+ items, the window is 4 hours. Why is this approach problematic?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Imagine Instacart is rolling out a program which allows grocery stores to reduce wait times for pickups. Design an experiment for this program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Brett&lt;/li&gt;
&lt;li&gt;fulfillment side: shopper success&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Customer side has ads, growth, and core&lt;/li&gt;
&lt;li&gt;bus&lt;/li&gt;
&lt;li&gt;&lt;p&gt;falls under customers catelogue team&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;from-video-on-hiring-practices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From video on hiring practices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;objective, assumptions, scope&lt;/li&gt;
&lt;li&gt;reliable, readable, flexible&lt;/li&gt;
&lt;li&gt;logically sound and complete&lt;/li&gt;
&lt;li&gt;clear description of work and sharp Q&amp;amp;A&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;framework&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Framework&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Instacart delivers groceries online and on app&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;online - cookie assignment issue..&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;assume people order all groceries from one shop&lt;/li&gt;
&lt;li&gt;Instacart dispatches shoppers to fulfill order&lt;/li&gt;
&lt;li&gt;shoppers (picker+delivery) or picker and deliverer&lt;/li&gt;
&lt;li&gt;customers fill out their cart, then see available delivery times&lt;/li&gt;
&lt;li&gt;times in blocks of an hour&lt;/li&gt;
&lt;li&gt;marketplace involves stores who give catelogues and sometimes pay&lt;/li&gt;
&lt;li&gt;&lt;p&gt;and goods companies place ads in app to have product come up&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;aim to deliver asap, don’t always have enough shoppers to meet demand.&lt;/li&gt;
&lt;li&gt;Turn off timeslot if unavailable&lt;/li&gt;
&lt;li&gt;lets say we have busy pricing to combat this, so just charge more instead of turning slot off&lt;/li&gt;
&lt;li&gt;can increase up to 4x, usually just by a few dollars&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Goals: how would you know if surege pricing is a good idea? What next?&lt;/li&gt;
&lt;li&gt;Metrics:&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;moving demand?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;see regular prices as baseline&lt;/li&gt;
&lt;li&gt;see surge prices&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;higher revenue?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Interventions&lt;/li&gt;
&lt;li&gt;Incentives for each stakeholder (what if we optimize for this)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Models: assumptions, estimators&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Metrics:&lt;/h3&gt;
&lt;p&gt;many from &lt;a href=&#34;https://www.slideshare.net/JagannathPutrevu/supply-optimization-instacartslideshare&#34;&gt;this presentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;note when looking at product in video&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;efficiency = delivery time / active time * active time / total time = active efficiency * utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shoppers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Driving time: batching, proximity to store, traffic&lt;/li&gt;
&lt;li&gt;Picking time: speed of shopper, shopping list ordering, checkout times&lt;/li&gt;
&lt;li&gt;Bags pickup time: staging area layout, numeber of bags being picked up&lt;/li&gt;
&lt;li&gt;Delivery time: Traffic, order time/space density, routing algorithm efficiency&lt;/li&gt;
&lt;li&gt;Idle time: Supply/Demand equilibrium, variance in cancellations, variance in shifts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demand forecasts&lt;/li&gt;
&lt;li&gt;Space/time density&lt;/li&gt;
&lt;li&gt;Store Locations&lt;/li&gt;
&lt;li&gt;Shopper pool or staffing&lt;/li&gt;
&lt;li&gt;Traffic&lt;/li&gt;
&lt;li&gt;Weather&lt;/li&gt;
&lt;li&gt;Shopper ability (speed, precision)&lt;/li&gt;
&lt;li&gt;Fulfillment times&lt;/li&gt;
&lt;li&gt;Cancellation probability&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Constraints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;handoff vs full-service&lt;/li&gt;
&lt;li&gt;idleness vs efficiency vs lost delivery&lt;/li&gt;
&lt;li&gt;store timings&lt;/li&gt;
&lt;li&gt;shift length requirement&lt;/li&gt;
&lt;li&gt;business rules&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Costs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;idle hours&lt;/li&gt;
&lt;li&gt;lost deliveries&lt;/li&gt;
&lt;li&gt;lost efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Outputs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of shoppers required&lt;/li&gt;
&lt;li&gt;percent of volume done through handoff&lt;/li&gt;
&lt;li&gt;estimated efficiency and utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;complications&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Complications&lt;/h3&gt;
&lt;p&gt;For ad experiments;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Users go on vacation, (W=1,T=0,Y=0)&lt;/li&gt;
&lt;li&gt;Competition bids more&lt;/li&gt;
&lt;li&gt;Ad targeting biases sample&lt;/li&gt;
&lt;li&gt;Algorithms for routing shoppers can’t be tested within the same geography since routes for T shoppers would influence the routes for C shoppers&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;my-value-ads&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My Value ads&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.dominodatalab.com/data-science-instacart/&#34;&gt;personalization&lt;/a&gt; mentioned at 31:00 in this video.&lt;/li&gt;
&lt;li&gt;My research on utility-based decision making, and on RL and learning about environments to optimize decision making could both contribute here?&lt;/li&gt;
&lt;li&gt;My work on inferring what strategy a person is following could relate to inferring their utility functions?&lt;/li&gt;
&lt;li&gt;My work on dynamic logistic bayesian regression could offer a fast way to approximate utility functions for different users, since random utility models are similar in functional form to logistic regressions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ads:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Motivation has two components: activation and direction.&lt;/li&gt;
&lt;li&gt;Activation: I’m not really hungry, then I see sizzling bacon and that makes me hungry.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Directing: I want bacon, then I see a nice omlette and decide to get that instead. Changes the action that manifests out of the latent state.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;From YouTube, you can advertize to viewers who watch competing channels, e.g. I’d advertize on Ben Lambert since he also has educational stats videos.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At Instacart, this means advertizing to customers who are searching for a substitute product..&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Using substitutes and complements&lt;/li&gt;
&lt;li&gt;Advertize to users who are searching for a substiture item (eggs from a different farm)&lt;/li&gt;
&lt;li&gt;Advertize to users who are searching for a complement item (eggs for someone searching bacon)&lt;/li&gt;
&lt;li&gt;Bigger issue – an ad to someone who is going to buy the product anyway is wasted money..&lt;/li&gt;
&lt;li&gt;Ads should target people who would buy if they see the ad, and wouldn’t if they don’t.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Showing ads to people who will buy anyway confounds the effectiveness of the ad experiments.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;discussion-of-past-work&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Discussion of past work&lt;/h3&gt;
&lt;p&gt;“Be prepared to discuss your past work involving analyzing large and complicated datasets, defending your approaches and communicating what you learned during your project.”&lt;/p&gt;
&lt;p&gt;“elaborate on related projects from your resume”&lt;/p&gt;
&lt;p&gt;PredictIt Project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Situation:&lt;/li&gt;
&lt;li&gt;Project:&lt;/li&gt;
&lt;li&gt;Maybe sketch the fig of price for Trump win versus Republican win&lt;/li&gt;
&lt;li&gt;I also aggregated data based on days until expiration, and applied a linear in log odds model at each day.&lt;/li&gt;
&lt;li&gt;Found the common S shaped calibration curve that comes up in lab experiments, particularly at 1-2 days before market expiration.&lt;/li&gt;
&lt;li&gt;Result:&lt;/li&gt;
&lt;li&gt;Value to Yelp: Relates to the economic patterns maybe?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Strategy Inference Project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Situation:&lt;/li&gt;
&lt;li&gt;Project:&lt;/li&gt;
&lt;li&gt;Children and adults might use different strategies when making decisions.&lt;/li&gt;
&lt;li&gt;I worked with a friend to formalize the possible set of strategies, and then I implemented a model to infer the probability of a kid following each strategy on each trial, and applied Bayes Theorem to get a probability that the kid was following any strategy over time.&lt;/li&gt;
&lt;li&gt;Result: A lot of experiments focus on which strategy best fits all data across trials – the approach that I took allows for changes in strategies, and the exciting result was an ability to track latent decision making strategies over time.&lt;/li&gt;
&lt;li&gt;Value to Yelp: At yelp, for example, a user might be choosing restaurants based primarily on value, then shift to preferring restaurants based on rating. An analogous modeling approach might be able to detect, in near real time, the features that are driving an individual user’s decisions. This informaiton could then be used to modify search rankings.&lt;/li&gt;
&lt;li&gt;More specifically, model click probability as a function of value and stars, estimate the coefficients online, and use the coefficients to prioritize these features in rankings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;General interest in decision making:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Situation: Models of multi-alternative decision making are often applied to food preferences to see how people integrate information about price, healthyness, flavor, … .&lt;/li&gt;
&lt;li&gt;Excitement: Yelp is basically a giant experiment in multiattribute decision making.&lt;/li&gt;
&lt;li&gt;Value to Yelp: Theoretical insights might help in guiding how restaurant informaiton is displayed to consumers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Insight Project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User funnel and user flow, maybe should have started at these simpler analyses&lt;/li&gt;
&lt;li&gt;Learned lots about decision trees, random forests versus gradient boosting and gini impurity versus mean decreas in accuracy&lt;/li&gt;
&lt;li&gt;Chose gini impurity because mean decrease in accuracy labels collinear features as unimportant&lt;/li&gt;
&lt;li&gt;Chose random forests because it does a better job of pulling out important collinear features&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-dive&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Deep Dive&lt;/h1&gt;
&lt;div id=&#34;business-model-culture-and-vision&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Business Model, Culture, and Vision&lt;/h2&gt;
&lt;p&gt;Business model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instacart brings groceries online, through stores rather than through fulfillment centers (e.g. how Amazon does it).&lt;/li&gt;
&lt;li&gt;Recently showed their model was profitable and sustainable&lt;/li&gt;
&lt;li&gt;Became economical by batching orders, taking advantage of economies of scale, …&lt;/li&gt;
&lt;li&gt;Gross margin positive, implying growth will be profitable&lt;/li&gt;
&lt;li&gt;Tips can only go to the last person that touched order (delivery) even if they didn’t put together the order (pickers).&lt;/li&gt;
&lt;li&gt;Brings incremental volume to grocery retailers&lt;/li&gt;
&lt;li&gt;Around 50/% of grocers have same price on app as in store, others have markups&lt;/li&gt;
&lt;li&gt;Seen as a antidote to Amazon by supporting local business&lt;/li&gt;
&lt;li&gt;There’s a grocerie store within 10-15min from every household&lt;/li&gt;
&lt;li&gt;Most valuable ads because they influence behavior right before the purchase&lt;/li&gt;
&lt;li&gt;Want to build AdWords for groceries&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Delivery Models;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Handoff model: in-store pickers fulfill orders and stage groceries for pickup, drivers pickup groceries and deliver them&lt;/li&gt;
&lt;li&gt;Full service: a single shopper picks and delivers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Revenue&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Subscription or delivery fee from cusomter&lt;/li&gt;
&lt;li&gt;Service fee partially goes towards balancing tips for delivery and picking.&lt;/li&gt;
&lt;li&gt;fee from retailer&lt;/li&gt;
&lt;li&gt;revenue from advertisers (talked about around 29:00)&lt;/li&gt;
&lt;li&gt;coupons&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;costs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;delivery costs, pickers and delivery (can be different people)&lt;/li&gt;
&lt;li&gt;credit card transaction fees&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I like the impact that data scientists can have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minutes saved in deliveries is /$0.25 in gross margin, which can be passed to savings for the customer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I like the enturpneural spirit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;6 months after Amazon bought Wholefoods, Instacart signed partnerships with nearly every large retailer&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;projects-tasks-and-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Projects, Tasks, and Data&lt;/h2&gt;
&lt;p&gt;Open problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improve predictability for shopper experience&lt;/li&gt;
&lt;li&gt;Inventory prediction or modeling&lt;/li&gt;
&lt;li&gt;Model for item substitutes (crowd source this so that orderers can specify acceptable substitutes?)&lt;/li&gt;
&lt;li&gt;Labor models in local areas, accounting for unions and labor laws&lt;/li&gt;
&lt;li&gt;Trust with person delivering produce&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-dive-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Deep Dive&lt;/h1&gt;
&lt;div id=&#34;stakeholders-and-use-cases&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stakeholders and Use Cases&lt;/h2&gt;
&lt;p&gt;User experiences covered in the beginning of &lt;a href=&#34;https://blog.dominodatalab.com/data-science-instacart/&#34;&gt;this video&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Customer: Want to search and pick groceries for delivery, paritcularly items they are loyal to, possibly from stores that they are loyal to&lt;/li&gt;
&lt;li&gt;Pickers: Part time employees with hourly pay and incentive structures go to stores, find and scan items from a customer’s list.&lt;/li&gt;
&lt;li&gt;Delivery: Contractors with incentive based pay deliver items from picker to customer. These are contract workers&lt;/li&gt;
&lt;li&gt;Stores: Want their store to be chosen by customer&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Advertisers: Want to increase demand possibly through ads&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Instacart doesn’t tell shoppers where to find an item if a store is out of inventory. One reason is to maintain relationships with stores, rather than products.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adding other types of retailers to the platform, but don’t want to canibalize the relationship with current retailers&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;metrics-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metrics&lt;/h2&gt;
&lt;p&gt;Ideas from &lt;a href=&#34;https://www.youtube.com/watch?v=-Y8GGgT4Qf8&#34;&gt;this video&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;Customers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User funnel; store search, item search, checkout, delivery screen&lt;/li&gt;
&lt;li&gt;Delivery page views might indicate demand&lt;/li&gt;
&lt;li&gt;Bounce before delivery screen might indicate browsing without intent&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bounces at delivery screen might indicate a decision to go to the store instead of use instacart&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Browsing metrics&lt;/li&gt;
&lt;li&gt;Checkout metrics: can check out (convert), get to checkout screen and see no delivery times then decide to go to store on their own (lost sale), or not go to check out screen (just browsing). Neglecting the lost sales could negatively impact staffing decisions, e.g. fewer delivery slots -&amp;gt; fewer shoppers needed -&amp;gt; fewer delivery slots -&amp;gt; … -&amp;gt; no more business.&lt;/li&gt;
&lt;li&gt;Log every exposure to availability&lt;/li&gt;
&lt;li&gt;Model probability of a checkout given customer-store-geography-hour. Then go back and model the probability of a checkout if the past customers had seen available checkouts. Demand models like this help with staffing decisions, and identifying undre staffed areas.&lt;/li&gt;
&lt;li&gt;Demand = p(convert | 100% supply)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CI methods for inferring demand if there had been supply? Can compare demand in similar regions that have sufficient supply (nearly no bounces from checkout page)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pickers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;time between scans on barcodes (time between picking items)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Delivery:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Goals: 1 match demand to maximize conversions 2 efficiency for good unit economics&lt;/li&gt;
&lt;li&gt;Delays in delivery could indivate holidays, bad weather, regional traffic events&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For profitable unit economics;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More efficient deliveries; more deliveries per shopper (picker, delivery) hour&lt;/li&gt;
&lt;li&gt;Number of deliveries / number of labor hours&lt;/li&gt;
&lt;li&gt;Can improve if shoppers are better utilized (less idle) or faster (more efficient, less time spent searching for items)&lt;/li&gt;
&lt;li&gt;Improve operational organization&lt;/li&gt;
&lt;li&gt;Improve incentives and training for shoppers to reduce&lt;/li&gt;
&lt;li&gt;Part time employees in store&lt;/li&gt;
&lt;li&gt;Contractors who deliver&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;monetization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monetization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;delivery fees from customers&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Service Fees and Tips from customers for shoppers&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Product partnerships from advertisers for ads&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Retail partners subsidize delivery to get more customers&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;transaction costs, driver insurance&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;time driving and shopping, can cost more than service fees and tips&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Instacart ads are highly valuable because of opportunities to measure intent and influence behavior at the point of sale.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;user-flows-and-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;User Flows and Metrics&lt;/h2&gt;
&lt;p&gt;Thinking of the broader user flow can help with identifying measurable events within that flow (metrics). Overall, consider units of individuals (accounts, unique visitors, cookies, IPs…), units of time (daily, weekly), and referents (week over week, …)&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;https://www.youtube.com/watch?v=-Y8GGgT4Qf8&#34;&gt;Jeremy Stanley small talk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Customer Experience&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select store&lt;/li&gt;
&lt;li&gt;Shop by selecting items&lt;/li&gt;
&lt;li&gt;Checkout&lt;/li&gt;
&lt;li&gt;Select delivery time&lt;/li&gt;
&lt;li&gt;Return&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shoppers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accept order&lt;/li&gt;
&lt;li&gt;Drive to store, get list of groceries&lt;/li&gt;
&lt;li&gt;Scan bar codes&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Delivery&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;‘buy it again model’ sorts items shown to users based on how likely it is they’ll buy them again&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-science-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Science work&lt;/h2&gt;
&lt;p&gt;From &lt;a href=&#34;https://www.youtube.com/watch?v=-Y8GGgT4Qf8&#34;&gt;video&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Forecasting demand and supply (bounces from checkout and idle shoppers)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Different time series models perform better or worse at different times of year, so the team uses different models depending on time of year. Could this be optimized with bayesian model averaging? Or would that lag the changes in the data?&lt;/li&gt;
&lt;li&gt;Brain following habitual (model-free) vs goal-directed (model-based) strategies and shifts between these strategies depending on outcome variance. At instacart, a similar modeling approach that automates changes between forecasting models or models for staffing shoppers might be more efficient than a current method?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Balancing supply and demand (changing delivery prices based on capacity)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Charge more for delivery when estimated capacity drops&lt;/li&gt;
&lt;li&gt;Incentivise people to place orders where it will be most efficient by offering cheaper prices&lt;/li&gt;
&lt;li&gt;Account for consumers feeling gouged? 1) UX team can think about how this pricing influenced customers satisfaction, 2) AB testing, 3) models of longer term behavior of users exposed to higher surge prices to see causal effects of price strategies on longer term behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fulfuillment timeliness (fulfillment speed, probability deliver on time, chance shopper gets all items, customer happiness)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Customers happiest near the beginning of time window, either before or after beginning of time window&lt;/li&gt;
&lt;li&gt;Could start with Google API, but this breaks at scale&lt;/li&gt;
&lt;li&gt;Instacart models outperform Google API estimates&lt;/li&gt;
&lt;li&gt;Shoppers pick and then stage orders&lt;/li&gt;
&lt;li&gt;Then drivers go to stores and pick up orders, then deliver them&lt;/li&gt;
&lt;li&gt;Use quantile regression to model variance, which is important because of all the steps being modeled (picking each order, driver picking up each order and delivering each order)&lt;/li&gt;
&lt;li&gt;(XGBoost?) Gradient boosting machines for comples time and space features and for scale&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Account for picker variance? Design app to minimize picker veriance?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;cost of idle hour (know shopper hourly pay)&lt;/li&gt;
&lt;li&gt;cost of being late, know how much it affects customer happines, can relate to customer retention&lt;/li&gt;
&lt;li&gt;guard rail metrics: probability of being late,&lt;/li&gt;
&lt;li&gt;Netflix heuristically doubles value of customers for network effects to account for 1 other friend&lt;/li&gt;
&lt;li&gt;understanding queues is crucial for efficency&lt;/li&gt;
&lt;li&gt;&lt;p&gt;having staff that can pick or deliver enables efficency of handoff model where pickers stage orders for delivery, or can deliver themselves&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Capacitized vehicle route planning with time windows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computational infeasibility e.g. O orders, D deliveries, N shoppers leads to O choose D * N options&lt;/li&gt;
&lt;li&gt;pick subset such that orders in only one batch, orders delivered on time, shoppers move as fast as possible&lt;/li&gt;
&lt;li&gt;impossible problem…&lt;/li&gt;
&lt;li&gt;use greedy heuristics; for each unassigned order, assign to shopper that can do fastest, find other orders they can do quickly, … .&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shopper efficency:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shopper speed (items per minute) vs batch size (items on list)&lt;/li&gt;
&lt;li&gt;Control: sort departments randomly, then sort items within departments alphabetically. Shoppers got faster as batch size increases because item density increases (more adjacent items) and fewer unencountered items in unknown locations&lt;/li&gt;
&lt;li&gt;Human sorting: people curate the orderings&lt;/li&gt;
&lt;li&gt;Travling salesman solution: graph of items penalized by observed times between pickups. Ignores things like cold items parishing&lt;/li&gt;
&lt;li&gt;Deep learning solution: outperforms human curation and travling salesman solution at large batch sizes.&lt;/li&gt;
&lt;li&gt;Problem: In a store, just pickes item a, need a model to suggest which item to pick next that would maximize efficency (RL problem with markov state space determined by which item was just picked!)&lt;/li&gt;
&lt;li&gt;Estimate P(next=item A | last = item B, candidates = unpicked items in list)&lt;/li&gt;
&lt;li&gt;Deep learning structure - embed products, shoppers, stores, account for candidate set, predict next item, penalize from cross entropy loss function&lt;/li&gt;
&lt;li&gt;Make predictions either for individual shopper or for best shopper at a store location&lt;/li&gt;
&lt;li&gt;see article on ‘deep learning with emojis’&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Driving basket sizes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;important way to increase revenue&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Considerations;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;long tail, ie storing lots of items&lt;/li&gt;
&lt;li&gt;parishability, storing items for limited time&lt;/li&gt;
&lt;li&gt;timeliness, physical pickups need to be close to dropoffs&lt;/li&gt;
&lt;li&gt;cost&lt;/li&gt;
&lt;li&gt;human errors;&lt;/li&gt;
&lt;li&gt;pick wrong item (protected by scanning UPC code)&lt;/li&gt;
&lt;li&gt;flag item as not there, when it was just not found&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Product Discovery&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discovery in stores based on laying out items in a way that forces users to walk past new items&lt;/li&gt;
&lt;li&gt;At instacart, deep learning from image to see how similar item is to one that users may have bought&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;on-web&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On Web&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Users (IPs) arrive on Yelp, can be assigned a cookie, or associated with an account if the user creates one.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Growth: DAU, MAU&lt;/li&gt;
&lt;li&gt;Growth: monthly new users, monthly unique users&lt;/li&gt;
&lt;li&gt;Engagement: Reviews per user per month&lt;/li&gt;
&lt;li&gt;Engagement:&lt;/li&gt;
&lt;li&gt;Engagement: Traffic to business pages&lt;/li&gt;
&lt;li&gt;Monetization: Reservations&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Monetization: Waitlist&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Performance: Wait times at restaurants&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biz.yelp.com/&#34;&gt;Some metrics for business partners&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;on-mobile&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On Mobile&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;current-issues&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Current issues&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Supply/Demand balancing?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;competitors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Competitors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amazon&lt;/li&gt;
&lt;li&gt;Other food delivery – Grubhub, Uber Eats, …&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;areas-where-ml-is-impactful&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Areas where ML is impactful:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning is critical for our logistics teams, where we balance supply &amp;amp; demand and optimize the vehicle routing problem we have to fulfill grocery deliveries. We use machine learning (and operations research) to forecast distributions of outcomes, optimize control problems, predict grocery shopping and driving times, and solve routing optimization problems. We also use machine learning to assist our shoppers in stores by sorting their shopping lists.&lt;/li&gt;
&lt;li&gt;Another area machine learning has had a significant impact is in our catalog. We use machine learning to predict the probability any given item will be found at a given store location, and to suggest replacements for items that might not be found. There are many applications for using machine learning to improve the catalog (remove duplicates, tag products, enrich metadata, etc.), and so we are just beginning there.&lt;/li&gt;
&lt;li&gt;Last, but not least, we use machine learning extensively for search &amp;amp; discovery. I’ll let Sharath follow-on there.&lt;/li&gt;
&lt;li&gt;We haven’t used machine learning to affect growth (user acquisition) much. In part that’s because we have been growing so fast, we haven’t needed to optimize our growth strategies. In part it’s because we are still rapidly iterating our growth products, and so simpler solutions have been preferable.&lt;/li&gt;
&lt;li&gt;On the Search and Discovery side, there is search matching and ranking, merchandising across the site and several contextual recommendations including ads targeting and repurchase modeling (this competition!), which have benefitted from machine learning techniques.&lt;/li&gt;
&lt;li&gt;There are others such as query understanding - query expansion, spell-correction and autocomplete - where simpler data mining style approaches (aggregation, domain knowledge etc.) have been effective. This would be one of those areas where strict latency and engineering complexity raises the bar on how much more complex models must help to justify the investment (personalized query autocompletion for eg.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;notes-on-ab-testing-from-jeremy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes on A/B testing from Jeremy&lt;/h2&gt;
&lt;p&gt;It very much depends upon the application. If we have a rigorous way of measuring the desired impact when deployed (usually an A/B test), then black-box models that maximize predictive power are often preferred. Even then, we often seek to understand those models in order to improve them, and in order to understand where they may fail in edge cases. At times, this means building a more structured model as well purely to aid understanding.&lt;/p&gt;
&lt;p&gt;In some cases, we cannot A/B test. This can arise because of marketplace dynamics (groups of shoppers or customers are not independent) or because we are seeking to estimate something for measurement rather than for action. In these cases we prefer more structured models, as they can be interrogated and validated and their failings are more easily understood.&lt;/p&gt;
&lt;p&gt;We also have applications where we expect the model to be used frequently outside of the domain it was trained in. This can happen when first introducing machine learning models. You begin a product with a simple set of heuristics that preclude you from ever observing a large part of the potential data space. In these cases, simpler models may generalize better to these unobserved spaces, and we can slowly build up exposure through explore / exploit approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-exciting-things-about-instacart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some exciting things about Instacart&lt;/h2&gt;
&lt;p&gt;From &lt;a href=&#34;https://medium.com/@instacart/our-new-vp-of-data-science-answers-why-i-joined-instacart-2034f467577d&#34;&gt;why VP of DS joined&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instacart has a very unique business model, and that is why data science is so important for our success. Instacart is in part an ecommerce marketplace, and so we have all of the same fascinating catalog, search, recommendation and community opportunities that eBay and Etsy have. But Instacart is also a real-time logistics platform. So we also have all of the same fascinating forecasting, scheduling, operations and fulfillment opportunities that Uber or Lyft have.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ideas-excitement-questions-to-ask&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ideas, excitement, questions to ask&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;glassdoor-interview-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Glassdoor interview questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How to do power analysis for an A/B test.&lt;/li&gt;
&lt;li&gt;SQL timed exam&lt;/li&gt;
&lt;li&gt;Questions about efficiency curves in the supply demand distribution of Instacart’s logistics.&lt;/li&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Applied online, got a HackerRank link with SQL question. It is supposed to be completed within 2 hours, but it usually takes 30-40 minutes to solve.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Phone screen with recruiter. It was a bit different since the recruiter didn’t ask me any of my background or job fit, but rather asked me to drive the interview with my questions. She did ask some logistical questions like location, visa status, etc.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Got a data analytics takehome assignment. Was given 2 days to accomplish it. The assignment was related to A/B tests, and exploratory data analysis. Had to prepare presentation.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Phone Screen with Hiring Manager. Asked me about my background, ideal job and couple of open-ended questions regarding Shoppers in Instacart.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Onsite Round 6 interviews, which constituted analytics take home presentation, analytical problem solving, statistics, culture fit interview during lunch, engineering partnership, product partnership, and with hiring manager. I liked the rigorousness of the process but was really disappointed when no feedback was given after I was rejected after the onsite.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Without giving away the challenge I can say that I assume they were looking for more of a linear programming algorithm then the simple arithmetic I used to solve the problem.&lt;/li&gt;
&lt;li&gt;SQL query and then an employee optimization dataset.&lt;/li&gt;
&lt;li&gt;The most interesting part of the process was the take-home which consists of 1 ambiguous at best question with a data set of 1 month of order data, which is less data than they open-sources.&lt;/li&gt;
&lt;li&gt;The HR representative said that for the 48 hour take home most people put in 4-6 hours to complete it, but because they want a visualization board, powerpoint, or presentation medium which can take a lot of work, it can feel like just another Silicon Valley rush assignment to do a full project.&lt;/li&gt;
&lt;li&gt;How would you staff the team based on delivery data?&lt;/li&gt;
&lt;li&gt;Estimate the demand and supply&lt;/li&gt;
&lt;li&gt;Code challenge, hiring manager scan and then onsite. The HRs are very professional, but the interviewers are expecting you to know their business very well and they give very vague questions without enough explanation, without any guidance. There would be three technical white boarding rounds, each rounds you will deal with two data scientists. They ask questions about the business is dealing with, but no guidance, and they explain things really poorly, even one of them didn’t say a word during the whole 45 min session.??????? Excuse me???? And they will tell you the result immediately, if you don’t pass, all the following interviews will be cancelled.&lt;/li&gt;
&lt;li&gt;All HackerRank tests. An algorithm question about a kind of thing you will NEVER do at a job, and a SQL question that was reasonable and not the usual trivial thing. Phase II if you get past that or aren’t age screened out yet, is analyzing a simple data set and make a presentation about your findings.&lt;/li&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;SQL test 2.Recruiter screen 3. Data take home challenge 4. Phone screen with a data scientist 5. On site Overall it was a smooth interview process. The recruiter was very helpful. The data scientists and PM’s I met seemed smart but also very serious (could sense some stress). The on site has a behavioral competent (freebie if you can communicate decently) as well as several technical interviews. They will ask about statistical techniques you would use to solve a specific scenario (monte carlo simulation, decision tree, etc.). The interview was pretty rigorous and they will gauge how advanced you are with statistical knowledge and ability to apply it to business problems. Overall I thought I did well in all interviews other than a portion of one of them (didn’t know what statistical method to use to solve specific case question), and ended up getting rejected.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We are observing this trend, what are possible explanations for why? 2. Behavioral (talk about a time when…) 3. SQL. Did not have live coding, only in the initial problem and take home portion&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The first interview was to send me a data set. Given a set of training cases, you need to extract the features and train a model, to predict for the test set.&lt;/li&gt;
&lt;li&gt;I can’t reveal them but mostly around problems in Logistics that Instacart solves on a day to day basis.&lt;/li&gt;
&lt;li&gt;The coding challenge was a timed test mainly on predictive modeling.&lt;/li&gt;
&lt;li&gt;During the phone interview it was clear the interviewer had spent time looking at my past work and asked highly detailed questions about my work, and how I may have gone about certain tasks differently.&lt;/li&gt;
&lt;li&gt;How might you have optimized parameters for this model differently?&lt;/li&gt;
&lt;li&gt;You just have to create a predictive model from a couple csv files they send you.&lt;/li&gt;
&lt;li&gt;How would you tune a random forest?&lt;/li&gt;
&lt;li&gt;He was kind enough to answer a lot of questions, and asked an open-ended yet very specific question about the shopping process they’re trying to optimize. Despite the ambiguity of the problem it felt like he was looking for some particular answers he may have been familiar with. In the end he proposed to send over detailed descriptions of the problem and some actual data, and asked for the solution to be coded up and sent back.&lt;/li&gt;
&lt;li&gt;The problem he asked to be solved is one of their most immediate business challenges, full-blown, not a coding test or even a restricted example at all. So it was odd they’d expect you to work on something for a week that an actual employee would spend several weeks refining. I thought it was to demonstrate coding and analytics skills, and provided a succint, well-commented solution, but didn’t try to overdo it.&lt;/li&gt;
&lt;li&gt;Then I had an analytical case where I had to build and explain a mode. Next I had an in-person interview which was another analytical challenge, but more whiteboarding. It included a data scientist and the hiring manager, who seemed great to work with. Lastly I chatted with the CEO who was friendly and down-to-earth.&lt;/li&gt;
&lt;li&gt;Explored various ways of monitoring user growth and retention.&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;insight-alum-mock-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Insight Alum Mock Questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Instacart (a grocery delivery app) implemented a pickup option at a subset of grocery stores that partner with the company. The subset of grocery stores that got the pickup option was not randomly assigned. How would you figure out whether the revenue from pickup is incremental to delivery? In other words, is pickup cannibalizing delivery?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At Instacart, if you order 1-7 items, the delivery window is 1 hour. If you order 8-24 items, the window is 2 hours. If you order 25+ items, the window is 4 hours. Why is this approach problematic?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Imagine Instacart is rolling out a program which allows grocery stores to reduce wait times for pickups. Design an experiment for this program.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;insight-from-jeremy-on-kaggle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Insight from Jeremy on Kaggle&lt;/h2&gt;
&lt;p&gt;from &lt;a href=&#34;https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/34200#latest-262904&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How is time spent?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5% getting the data (if working with a new data source, this is higher)&lt;/li&gt;
&lt;li&gt;10% analyzing the data (understand context, build intuition)&lt;/li&gt;
&lt;li&gt;10% manipulating the data (combining many sources, manipulating in non-trivial ways)&lt;/li&gt;
&lt;li&gt;25% modeling R&amp;amp;D (trying out different algorithms, backtesting)&lt;/li&gt;
&lt;li&gt;25% putting into production (scalability, testing, app integration)&lt;/li&gt;
&lt;li&gt;5% documenting (making it easy for future self and / or others to follow)&lt;/li&gt;
&lt;li&gt;5% A/B test setup (setting up key metrics for success, resolving design decisions)&lt;/li&gt;
&lt;li&gt;10% deployment (flipping a switch and then dealing with any issues)&lt;/li&gt;
&lt;li&gt;5% maintenance (if not immediately iterating, ongoing maintenance for other changes)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How are projects evaluated?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Metric impact. If we moved target metric by Y, how much would it matter to the business?&lt;/li&gt;
&lt;li&gt;User influence. If we changed the product, would it affect the users and drive the metric?&lt;/li&gt;
&lt;li&gt;Product leverage. If we built X, could we change the product to affect users?&lt;/li&gt;
&lt;li&gt;Data availability. Do we have the data needed? (If not, let’s collect it!)&lt;/li&gt;
&lt;li&gt;Algorithm feasibility. Is it feasible to solve, do we have a best practice, can it scale?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probability and Statistics</title>
      <link>/MacStrelioff/insightstudying/probability-statistics/</link>
      <pubDate>Sat, 21 Sep 2019 11:02:16 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/probability-statistics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#probability-foundations&#34;&gt;Probability Foundations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#probability-rules&#34;&gt;Probability Rules&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probability-functions&#34;&gt;Probability Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expectation&#34;&gt;Expectation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance&#34;&gt;Variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#information-theory&#34;&gt;Information Theory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#causal-inference&#34;&gt;Causal Inference&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions-and-key-terms&#34;&gt;Assumptions and key terms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-methods&#34;&gt;Common Methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#diff-in-diff&#34;&gt;Diff in Diff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#causal-impact&#34;&gt;Causal Impact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#synthetic-controls&#34;&gt;Synthetic Controls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iv-analysis&#34;&gt;IV Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regression-discontinuity&#34;&gt;Regression Discontinuity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-effects-regression&#34;&gt;Fixed Effects Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#first-differences&#34;&gt;First differences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects&#34;&gt;Random Effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#propensity-score-matching&#34;&gt;Propensity Score Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#microsoft-dowhy&#34;&gt;Microsoft DoWhy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#experiment-design&#34;&gt;Experiment Design&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#foundations&#34;&gt;Foundations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concerns&#34;&gt;Concerns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#treatment-units&#34;&gt;Treatment Units&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generative-model-and-adjustment-variables&#34;&gt;Generative Model and Adjustment Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-designs&#34;&gt;Common Designs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assignment-mechanism&#34;&gt;Assignment Mechanism&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#duration&#34;&gt;Duration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#integrety-checks&#34;&gt;Integrety Checks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-versus-bayes-estimators&#34;&gt;Maximum Likelihood versus Bayes’ Estimators&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glms&#34;&gt;GLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-tests&#34;&gt;Common Tests&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#t-test&#34;&gt;t-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#anova&#34;&gt;ANOVA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-model&#34;&gt;Linear Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#z-test-for-proportions&#34;&gt;Z test for proportions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chi-square&#34;&gt;Chi-Square&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-ratio-and-bayes-factors&#34;&gt;Likelihood Ratio and Bayes Factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#always-valid-p-values-ck-optimizely-white-paper&#34;&gt;always valid p-values (ck optimizely white paper)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#different-estimands-for-ci&#34;&gt;Different Estimands For CI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-on-variance&#34;&gt;more on variance&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#emperical-variance-estimation&#34;&gt;Emperical Variance Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mixed-effects-models&#34;&gt;Mixed Effects Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#effiencicy-or-variance-reduction&#34;&gt;Effiencicy or Variance Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glms-1&#34;&gt;GLMs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#causal-inference-methods&#34;&gt;Causal Inference Methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#difference-in-differences&#34;&gt;Difference in Differences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#causal-impact-1&#34;&gt;Causal Impact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#synthetic-controls-1&#34;&gt;Synthetic Controls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#propensity-score-matching-1&#34;&gt;Propensity Score Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-effects-regression-1&#34;&gt;Fixed Effects Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#instrumental-variables&#34;&gt;Instrumental variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regression-discontinuity-1&#34;&gt;Regression Discontinuity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;!--
Fermi problems (order of magnitude)
--&gt;
&lt;!--
- Make YouTube videos on each!
--&gt;
&lt;!--
# What is a probability, and how does that relate to statistics? 

- Probabilities are commonly thought of in three ways; 

1. Purely mathematical
2. Magnitudes of belief
3. Relative frequencies of infinate events

I use probability theory to quantify beliefs and make decisions. 

Logic, about truths of the universe. 
By there is uncertainty in what we know, and uncertainty in our measurements and observations. Statistics as a field is concerned with specifying our knowledge of the laws of the universe and the uncertainty around them. They do this by applying probability theory. 

Probability theory generalizes logic to situations where we aren&#39;t certain. And logic emerges from probability theory when we know things with certainty. 

Hypothesis testing

Statisticians use probability theory to decide on what is and isn&#39;t true
--&gt;
&lt;div id=&#34;probability-foundations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Probability Foundations&lt;/h1&gt;
&lt;!--
Events, event spaces, 
combinatorics (permutations and combinations)
--&gt;
&lt;p&gt;A random variable random variable is a variable with an unknown value, but known possible values. An event is an observed value of a random variable. An event space contains all possible values of the random variable. Probabilities are values assigned to the events in an event space (i.e. the possible values of a random variable) and represent how likely each event is relative to other events in the event space.&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(p(e)\)&lt;/span&gt; will be used to represent the probability of an event. Some special probabilities include the probability of any event in the event space, which is 1. And the probability of any event other than &lt;span class=&#34;math inline&#34;&gt;\(p(e)\)&lt;/span&gt;, known as the compliment of &lt;span class=&#34;math inline&#34;&gt;\(p(e)\)&lt;/span&gt;, denoted with &lt;span class=&#34;math inline&#34;&gt;\(p(\neg e)\)&lt;/span&gt;, is found by;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\neg e) = 1 - p(e)
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;probability-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Probability Rules&lt;/h2&gt;
&lt;p&gt;A joint event refers to two or more events occurring together. They are colloquially talked about as one event ‘and’ another event occurring together. More formally, joint events are called intersections (represented with the &lt;span class=&#34;math inline&#34;&gt;\(\cap\)&lt;/span&gt; symbol) between events. For events &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;, the probability of their joint event will be represented with &lt;span class=&#34;math inline&#34;&gt;\(p(e_1 \cap e_2)\)&lt;/span&gt;. The probability of an intersection of events is the same regardless of which event is considered first;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(e_1 \cap e_2) = p(e_2 \cap e_1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For two events, &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;, their intersection is found by; &lt;span class=&#34;math display&#34;&gt;\[
p(e_1 \&amp;amp; e_2) = p(e_1\cap e_2) = p(e_1|e_2)p(e_2)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(p(e_1|e_2)\)&lt;/span&gt; is a conditional probability, discussed in the next section.&lt;/p&gt;
&lt;p&gt;Conditional events refer to one event, &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt;, after another event, &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;, is known. &lt;span class=&#34;math inline&#34;&gt;\(p(e_1|e_2)\)&lt;/span&gt; represents the probability of &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; given, or after knowing, &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;. These can be defined by rearranging the multiplication rule as follows; &lt;span class=&#34;math display&#34;&gt;\[
p(e_1|e_2)p(e_2)=p(e_1\cap e_2) \Rightarrow p(e_1|e_2)= \frac{p(e_1\cap e_2)}{p(e_2)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Noting that, by the reflexively of joint events and the definition of the multiplication rule, &lt;span class=&#34;math inline&#34;&gt;\(p(e_1 \cap e_2) = p(e_2 \cap e_1)= p(e_2 | e_1) p(e_1)\)&lt;/span&gt;, and so the above equation becomes; &lt;span class=&#34;math display&#34;&gt;\[
p(e_1|e_2)=\frac{p(e_2 | e_1) p(e_1)}{p(e_2)}
\]&lt;/span&gt; This equation is known as Bayes’ Theorem or Bayes’ Rule.&lt;/p&gt;
&lt;p&gt;A union (represented with the &lt;span class=&#34;math inline&#34;&gt;\(\cup\)&lt;/span&gt; symbol) of events refers to at least one of multiple events occurring. For events, &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;, their union would include the probability that &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; occurs, the probability that or &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; occurs, and the probability that both &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; occur. Colloquially this is talked about the probability of &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; ‘or’ &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;. Formally this is expressed and computed as; &lt;span class=&#34;math display&#34;&gt;\[
p(e_1\text{ or } e_2) = p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1 \cap e_2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(p(e_1 \cap e_2)\)&lt;/span&gt; is a joint probability.&lt;/p&gt;
&lt;p&gt;Independence is a common assumption in many statistical techniques. Statisticians assume independence primarily because it simplifies the computation of certain probabilities. Events are said to be independent if knowing one event does not change the probability of the other event. Formally, if events &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are independent, this would mean that; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(e_1|e_2) &amp;amp;= p(e_1) \\
p(e_2|e_1) &amp;amp;= p(e_2)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are independent, then their joint probability simplifies as so; &lt;span class=&#34;math display&#34;&gt;\[
p(e_1 \cap e_2) = p(e_1|e_2)p(e_2) = p(e_1)p(e_2)
\]&lt;/span&gt; Where the last equality is only true if &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are independent (i.e. &lt;span class=&#34;math inline&#34;&gt;\(p(e_1|e_2)=p(e_1)\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Then the probability of their union simplifies to; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(e_1 \cup e_2) &amp;amp;= p(e_1) + p(e_2) - p(e_1|e_2)p(e_2) \\
&amp;amp;= p(e_1) + p(e_2) - p(e_1)p(e_2)
\end{aligned}
\]&lt;/span&gt; Where again, the last part of this equality is only true if &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;p&gt;Events &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are said to be mutually exclusive if the occurrence of either event precludes the occurrence of the other event. Formally mutual exclusivity means that; &lt;span class=&#34;math display&#34;&gt;\[
p(e_1 | e_2) = 0 \text{ and } p(e_2 | e_1) = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If two events are mutually exclusive, then the probability of their joint event is; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(e_1 \cap e_2) &amp;amp;= p(e_1|e_2)p(e_2) \\
&amp;amp;= 0*p(e_2) \\
&amp;amp;= 0
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the probability of their union simplifies to; &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(e_1 \cup e_2) &amp;amp;= p(e_1) + p(e_2) - p(e_1 \cap e_2) \\
&amp;amp;= p(e_1) + p(e_2) - 0 \\
&amp;amp;=p(e_1) + p(e_2)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;!--
## Practice Problems and Walkthroughs

- link to my YouTube videos on these things

Practice problems on the amoeba [here](https://www.quora.com/Bobo-the-amoeba-has-a-25-25-and-50-chance-of-producing-0-1-or-2-offspring-respectively-Each-of-Bobos-descendants-also-have-the-same-probabilities-What-is-the-probability-that-Bobos-lineage-dies-out) and  [here](https://www.quora.com/An-amoeba-has-a-75-chance-of-splitting-in-two-and-25-chance-of-dying-Is-there-an-intuitive-reason-why-the-probability-of-extinction-is-not-1).
--&gt;
&lt;/div&gt;
&lt;div id=&#34;probability-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Probability Functions&lt;/h2&gt;
&lt;!--
Many machine learning algorithms are different ways of estimating f(x)
--&gt;
&lt;p&gt;Here I’ll describe the properties and use cases for probability functions and cumulative probability functions. Conventionally, use &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is used represent a probability function, and &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; to represent a cumulative probability function. &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is commonly called a probability mass function (pmf) if &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is discrete, and a probability density function (pdf) if &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is continuous. &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; are related through integration:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(x) &amp;amp;= \frac{d}{dx}F(x) \\
F(x) &amp;amp;= \int_{-\infty}^{\infty} f(x) d_{x}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For discrete variables, the probability function represents the probability that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; takes a specific value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(f(x) = p(X=x)\)&lt;/span&gt;. For continuous variables, the probability that the variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; takes any particular value is technically &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(p(X=x)=0\)&lt;/span&gt;. However, &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; is still related to probability functions &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; through areas. For an arbitrarily small &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; the probability that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; takes a value between &lt;span class=&#34;math inline&#34;&gt;\(x-\epsilon\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x+\epsilon\)&lt;/span&gt; is the area of the distribution over that range;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p( x - \epsilon&amp;lt; x &amp;lt; x+\epsilon) &amp;amp;= F(x+\epsilon) - F(x-\epsilon) \\
&amp;amp;= \int_{-\infty}^{(x-+\epsilon)}f(x)d_{x} - \int_{-\infty}^{(x-\epsilon)}f(x)d_{x}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Probability functions have to satisfy a few conditions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
Transformations of variables!
--&gt;
&lt;!--
CDF transformation to convert from uniform into any random variable with a CDF
--&gt;
&lt;!--
Link to YouTube video on the circle problem
- circle problem
--&gt;
&lt;!--
### Probability Distributions
Make YouTube videos on each. 
- 1) Walk through properties from Wikipedia!
- 2) Walk through some simulations in Python!
--&gt;
&lt;!--
## Central limit theorem

Sum of any random variable converges to normal distribution. 
- show for binomial case, to justify Z test
--&gt;
&lt;/div&gt;
&lt;div id=&#34;expectation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expectation&lt;/h2&gt;
&lt;!--
expectation, variance, conditional or iterated expectations, law of total variance, variance of multiple variables
--&gt;
&lt;p&gt;For any functions of a randome variable, &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;, the expectation of that function is;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E_{x}(g(x))=\int_{-\infty}^{\infty}g(x)f(x)d_{x}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For discrete random variables, &lt;span class=&#34;math inline&#34;&gt;\(E(x)=\sum_xxp(x)\)&lt;/span&gt;. For continuous random variables, &lt;span class=&#34;math inline&#34;&gt;\(f(x)dx\)&lt;/span&gt; is conceptually &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; since it is the area under an infinately small segment of the probability function if &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; represents the height and &lt;span class=&#34;math inline&#34;&gt;\(dx\)&lt;/span&gt; represents an infinately small width.&lt;/p&gt;
&lt;p&gt;Expectation is a linear operator, so for constants &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and random variables &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(E(ax+by+c) = aE(x)+bE(y)+c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Expectation for joint random variables&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E_{x,y}(g(x,y))=\int_{s=x}\int_{t=y}g(s,t)f(s,t)d_{s}d_{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Marginalizing a distribution from a joint probability function (&lt;span class=&#34;math inline&#34;&gt;\(f(x,y)\)&lt;/span&gt;) or a conditional probability function (&lt;span class=&#34;math inline&#34;&gt;\(f(x|y)\)&lt;/span&gt;): &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f_x(x)&amp;amp;=\int_yf_{x,y}(x,y)d_{y}\\
&amp;amp;=\int_y f_{x|y}(x|y)f_y(y)d_{y}\\
&amp;amp;=E_y(f(x|y))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_expectation&#34;&gt;Law of total expectation, and rough proof&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E_{x}(x)&amp;amp;=E_{y}(E_{x|y}(x|y))\\
&amp;amp;=\int_y\int_{x}xf_{x|y}(x|y)d_{x}f(y)d_{y} \\
&amp;amp;=\int_{x}x\int_{y}f_{x|y}(x|y)f(y)d_{y}d_{x} \\
&amp;amp;=\int_{x}xf(x)d_{x}\\
&amp;amp;=E_{x}(x)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The last steps reflect that the internal integral is just marginalizing across y; &lt;span class=&#34;math inline&#34;&gt;\(f_x(x)=\int_y f_{x|y}(x|y)f_y(y)d_{y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;!--
Linear regression as a conditional expectation
Could show a simple figure with Y~1 and Y~X as example of expectation versus conditional expectation

Conditional expectation 
$$
E(g(x)|Z)=\int_{-\infty}^{\infty}p(x|Z)g(x)dx
$$

in linear regression, the expectation of Y is the linear combination of X and B, since those are deterministic, and the only non-deterministic part, epsilon, has expectation 0. 

- So the property that expectation propogates through sums leads to this result..
---&gt;
&lt;/div&gt;
&lt;div id=&#34;variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance&lt;/h2&gt;
&lt;p&gt;Covariance and variance&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Cov(X,Y)&amp;amp;=E_{x,y}((X-E_{x}(X))(Y-E_{y}(Y)))\\
Var(X)&amp;amp;=Cov(X,X)=E_{x}((X-E_{x}(X))^2) \\
&amp;amp;= E(X^2)-E(X)^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34;&gt;Bias-Variance tradeoff&lt;/a&gt; decomposes the theoretical variance obtained when estimating a function &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt; with a model &lt;span class=&#34;math inline&#34;&gt;\(\hat{g}(x)\)&lt;/span&gt; into three components that can be used to motivated changes to the model. Specifically, the Bias-Variance tradeoff is a decomposition of the expected prediction error; &lt;span class=&#34;math inline&#34;&gt;\(g(x)-\hat{g}(x)\)&lt;/span&gt;. To see it’s relation to variance, first rearrange the variance equation above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Var(X)= E(X^2)-E(X)^2 \\
\Rightarrow E(X^2) = Var(X)+E(X)^2 \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then define the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; to be the errors of the model;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E((g(x)-\hat{g}(x))^2) &amp;amp;= Var((g(x)-\hat{g}(x))^2)+E((g(x)-\hat{g}(x)))^2 \\
&amp;amp;= (g(x)-E(\hat{g}(x)))^2 + E((E(\hat{g}(x))-\hat{g}(x))^2) + Var(g(x))\\
&amp;amp;= Bias(\hat{g}(x))^2 + Var(\hat{g}(x)) + Var(g(x))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here bias represents the squared error between the true function &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt; and the expected model &lt;span class=&#34;math inline&#34;&gt;\(E(\hat{g}(x))\)&lt;/span&gt;. Variance represents the squared error between the expected model and the obtained model. And &lt;span class=&#34;math inline&#34;&gt;\(Var(g(x))\)&lt;/span&gt; represents the true noise or irreducible error in the process &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Variance of linear combinations of random variables:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Var(aX+bY+c)=a^2Var(X)+b^2Var(Y)+2abCov(X,Y)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Variance is also crucial for designing experiments, for example if &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; have different signs and &lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)&amp;gt;0\)&lt;/span&gt;, then the variance between is reduced because the covariance is subtracted off. This is a justification for within-participant designs, where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; represents the first measurement, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; represents a future measurement, &lt;span class=&#34;math inline&#34;&gt;\(a=-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b=1\)&lt;/span&gt; to reflect a difference comparing the second and first measurements (&lt;span class=&#34;math inline&#34;&gt;\(Y-X\)&lt;/span&gt;), and since the measurements come from the same person here, it is reasonable to assume they are correlated (&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)&amp;gt;0\)&lt;/span&gt;). In this case, &lt;span class=&#34;math inline&#34;&gt;\(Var(X,Y)=Var(X)+Var(Y)-Cov(X,Y)\)&lt;/span&gt;, which is not larger than the variance that would be obtained if the measurements were from independent samples.&lt;/p&gt;
&lt;!--
Confidence intervals, prediction intervals
--&gt;
&lt;!--
Variance of a function:
$$
\begin{aligned}
Var(g(x)) &amp;= E((g(x)-E(g(x)))^2)\\
&amp;\approx \left(\frac{d}{dx}g(E(x))\right)^2var(x) 
\end{aligned}
$$
--&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_variance&#34;&gt;Law of total variance&lt;/a&gt;: The variance of a variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be decomposed into the variation in the variable that remains when &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is known or irreducible error (&lt;span class=&#34;math inline&#34;&gt;\(E(Var(X|Y))\)&lt;/span&gt;), and the variation that carries through from the uncertainty in the model estimation &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(Var(E(X|Y)\)&lt;/span&gt;). &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Var(X)&amp;amp;=E(Var(X|Y))+Var(E(X|Y))
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This arises in hierarchical models and prediction intervals.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
x               &amp;amp;\sim N(\mu,\sigma)\\
E(x)            &amp;amp;=\mu\\
Var(x)          &amp;amp;=\sigma^2\\
\hat{\mu}_n     &amp;amp;\sim N\left(\mu,\frac{\sigma}{\sqrt{n}}\right)\\
E(\hat{\mu}_n)  &amp;amp;=\mu\\
Var(\hat{\mu}_n)&amp;amp;=\frac{\sigma^2}{n}\\
E(x_{n+1})      &amp;amp;= E(E(x_{n+1}|\hat{\mu}_n))= E(\hat{\mu}_n)=\mu \\
Var(x_{n+1})    &amp;amp;=E(Var(x_{n+1})) + Var(E(x_{n+1}|\mu_{n})\\
                &amp;amp;=\sigma^2+\frac{\sigma^2}{n}\\
(100-\alpha)PI: &amp;amp; E(x_{n+1}|\hat{\mu}_n) \pm T_\alpha \sqrt{Var(x_{n+1})}\\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;!--
Expectation and variance are the foundation for decision making
- value function v(x) that assigns values to the outcomes x
- If you know the distribution and value function, then you can directly compare choices based on their expected value
--&gt;
&lt;!--
## Models as conditional expectations

Models output an expectation that depends on the model, its structure, its features, and in Bayesian settings it&#39;s priors
--&gt;
&lt;/div&gt;
&lt;div id=&#34;information-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Information Theory&lt;/h2&gt;
&lt;p&gt;Information or Surprise is a measure of how unexpected an event was: &lt;span class=&#34;math display&#34;&gt;\[
I(x)=-log(f(x))=log\left(\frac{1}{f(x)} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Entropy is the expected surprise – higher entropy relates to more uncertainty: &lt;span class=&#34;math display&#34;&gt;\[
H(X)=E(I(x)) = E(-log(f(x)))  = \int_X f(x)I(x)dx = -\int_X f(x)log(f(x))d_x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;K-L Divergance, or relative entropy, is the expected distance between distributions with respect to one of the distributions: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
D_{KL}(f_{x}(x);f_{y}(x)) &amp;amp;= \int_{X} f_{x}(x)log\left(\frac{f_{y}(x)}{f_{x}(x)} \right)d_x\\
&amp;amp;= \int_X f_{x}(x)(log(f_{y}(x))-log(f_{x}(x))d_x \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Divergance is an important concept in machine learning, because it can be a loss function when we are estimating a distribution &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; with a model &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt;. From a Bayesian perspective, Divergance between a prior and posterior measures the information gained by observing the data. Bayesian experimental design focuses on collecting data that maximizes the divergance between the posterior and prior, i.e. the most informative data.&lt;/p&gt;
&lt;p&gt;Mutual Information is the divergance from the joint distribution &lt;span class=&#34;math inline&#34;&gt;\(f_{x,y}(x,y)\)&lt;/span&gt; to the joint when independence is assumed &lt;span class=&#34;math inline&#34;&gt;\(f_x(x)f_y(y)\)&lt;/span&gt; – i.e. a measure of non-independence; &lt;span class=&#34;math display&#34;&gt;\[
I(x;y)=\int_{y} \int_{x} f_{x,y}(x,y)log\left(\frac{f(x,y)}{f(x)f(y)}  \right)d_yd_x
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-inference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Causal Inference&lt;/h1&gt;
&lt;p&gt;The ultimate goal of most statistical work is to discover the causes of some outcome so that the outcome can be controlled. Hence, it is important to understand when causality can be inferred.&lt;/p&gt;
&lt;p&gt;From this perspective, there is a treatment assignment &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; and an outcome of interest that depends on the treatment &lt;span class=&#34;math inline&#34;&gt;\(Y(T)\)&lt;/span&gt;. Ideally, we would be able to compute the expected treatment effeoct for any individual &lt;span class=&#34;math inline&#34;&gt;\(E(Y(T=1)-Y(T=0))\)&lt;/span&gt;, but it is impossible to assign an individual to both variants of the treatment under the exact same conditions – e.g. one must be done at a later time than the other. Causal inference approaches specify the assumptions, models, and designs neeed to make causal statements.&lt;/p&gt;
&lt;p&gt;Most methods like t-tests and ANOVAs compare the observed outcomes in the treatment group to the observed outcomes in the control group. Causal inference methods can give a more powerful comparison between observed outcomes and counterfactual outcomes &lt;a href=&#34;https://www.youtube.com/watch?v=GTgZfCltMm8&#34;&gt;CI video&lt;/a&gt; at 10:00.&lt;/p&gt;
&lt;div id=&#34;assumptions-and-key-terms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumptions and key terms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_i(0),Y_i(1) \perp T_i | X_i\)&lt;/span&gt; Unconfoundedness: conditional on observations (&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;), treatment (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;) and potential outcomes (&lt;span class=&#34;math inline&#34;&gt;\(Y(T)\)&lt;/span&gt;) are independent.&lt;/li&gt;
&lt;li&gt;Endogeneity refers to a relationship between covariates and the error terms, e.g. through unobserved confounds.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;common-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Common Methods&lt;/h2&gt;
&lt;p&gt;Some summarized in this &lt;a href=&#34;https://towardsdatascience.com/causal-inference-using-difference-in-differences-causal-impact-and-synthetic-control-f8639c408268&#34;&gt;Medium Post&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;diff-in-diff&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Diff in Diff&lt;/h3&gt;
&lt;p&gt;Contexts and example ..&lt;/p&gt;
&lt;p&gt;Assumptions and requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires observed data on a control group pre and post treatment&lt;/li&gt;
&lt;li&gt;Assumes the trend (mean?) in the control group is an appropriate proxy for the trend (mean?) in the treatment group, had the treatment not occurred.&lt;/li&gt;
&lt;li&gt;Assumes the temporal dimension isn’t informative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I_{T}&amp;amp;:\text{Indicator for treatment group} \\
I_{Post}&amp;amp;:\text{Indicator for post treatment} \\
Y &amp;amp;= \beta_0 + \beta_1 I_{T} + \beta_2 I_{Post} + \beta_3 I_{T} I_{Post}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-impact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Causal Impact&lt;/h3&gt;
&lt;p&gt;Causal impact was developed by Google, docs &lt;a href=&#34;https://google.github.io/CausalImpact/CausalImpact.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Want to estimate the counterfactual for an observed timeseries, pre and post treatment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assumptions and requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires other time series related to the target time series&lt;/li&gt;
&lt;li&gt;Assumes the other time series are not influenced by the treatment, generally good candidates include; google trends time series, the weather, other countries or markets where no action was taken, unemployment indecies, stock prices, … .&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In pre period, train any kind of model to estimate the target time series as a function of the other time series. The model used in the Causal Impact package is a Bayesian structural time series model.&lt;/li&gt;
&lt;li&gt;Google’s Causal Impact method uses Bayesian structural time series to construct a counterfactual group – an estimate of what the time series would have looked like if the treatment had not been assigned. The method is described in &lt;a href=&#34;https://www.youtube.com/watch?v=GTgZfCltMm8&#34;&gt;this video&lt;/a&gt;. The method estimates a counterfactual using other related time series that were not influenced by the treatment.&lt;/li&gt;
&lt;li&gt;BSTS uses spike and slab prior for feature selection.&lt;/li&gt;
&lt;li&gt;In post period, use the model to estimate a counterfactual time series (i.e. synthetic control).&lt;/li&gt;
&lt;li&gt;Independent Python implementation &lt;a href=&#34;https://github.com/tcassou/causal_impact&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Benefits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provides pointwise estimates of the causal effect over time, as well as a cumulative estimate (summing up the pointwise estimates)&lt;/li&gt;
&lt;li&gt;Bayesian approach, provides credible intervals&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;synthetic-controls&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Synthetic Controls&lt;/h3&gt;
&lt;p&gt;Context:&lt;/p&gt;
&lt;p&gt;Assumptions and requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires multiple related time series&lt;/li&gt;
&lt;li&gt;Assumes a linear combination of the related time sereis is a good proxy for the counterfactual&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regress target time series on related time series&lt;/li&gt;
&lt;li&gt;Use estimates from this model as the counterfactual&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When to use vs causal impact:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The other time series are more conceptually identical - e.g. observed outcomes in untreated market segments subject to the same plausiable confounds – e.g. different nearby counties in a state.&lt;/li&gt;
&lt;li&gt;Causal impact might be better if the time series are thought to be components of the target – stock price, unemployment, page impressions, google trends, … .&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;iv-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;IV Analysis&lt;/h3&gt;
&lt;p&gt;More from &lt;a href=&#34;https://www.youtube.com/watch?v=NLgB2WGGKUw&#34;&gt;Ben Lambert&lt;/a&gt;, good explanation &lt;a href=&#34;https://www.youtube.com/watch?v=OWHCbEP56ms&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=cX5q_dKt6iU&#34;&gt;part II&lt;/a&gt;. Case study using quarter and years of education as &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; in place of education as the instrument [here](&lt;a href=&#34;https://www.youtube.com/watch?v=pI9YGSJ2qPk&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=pI9YGSJ2qPk&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=WjcoHAJ4_Mc&#34;&gt;relation to 2SLS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Mostly from Gelman chapter “Causal inference with more complicated observational designs” chapter from Gina.&lt;/p&gt;
&lt;p&gt;Context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Goal is to find the causal effect of &lt;span class=&#34;math inline&#34;&gt;\(x\rightarrow y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Issue: &lt;span class=&#34;math inline&#34;&gt;\(Cov(x,\epsilon)\neq 0\)&lt;/span&gt;, a feature is correlated to unobserved confounds, so &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{OLS}\)&lt;/span&gt; is unbiased and inconsistent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assumptions and Requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An instrument &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is measured so it can be included in the model.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(z,x)\neq 0\)&lt;/span&gt;, the instrumental variable is related to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(z,\epsilon)=0\)&lt;/span&gt;, the instrument is not correlated with unobserved confounds.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model, 2 stage least squares:&lt;/p&gt;
&lt;p&gt;Done to address bias in &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{OLS}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat{x}&amp;amp;=\gamma z + u \\
\hat{y}&amp;amp;=\beta_{IV} \hat{x} + \epsilon \\
\beta_{IV} &amp;amp;= (Z^TX)^{-1}Z^TY = \frac{Cov(Z,Y)}{Cov(Z,X)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Limitation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{IV}\)&lt;/span&gt; can still be biased, but is at least consistent (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{IV}\overset{p}{\rightarrow} \beta\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This relates to the intent to treat (ITT) estimate, where the effect of assignment to the treatment group is consided &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, but the effect of actually being treated &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; may not be known or estimated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-discontinuity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression Discontinuity&lt;/h3&gt;
&lt;p&gt;Context:&lt;/p&gt;
&lt;p&gt;Some systematic mechanism forces a discontinuity in what would be expected to be a regression line.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E.g. 1: Yelp rounds star ratings so that businesses close in underlying rating (4.49 and 4.5 stars) are assigned to different conditions (4 and 4.5 starts).&lt;/li&gt;
&lt;li&gt;E.g. 2: Schools might filter on exam scores, so that students who are very similar in underlying ability or test score (1 point below the threshold vs at the threshold) might be assigned to different universities.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;fixed-effects-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fixed Effects Regression&lt;/h3&gt;
&lt;p&gt;Context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unobserved heterogineity &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; that is related to features, &lt;span class=&#34;math inline&#34;&gt;\(Cov(\alpha_i,x_{i,t})\neq 0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Issue: &lt;span class=&#34;math inline&#34;&gt;\(Cov(\alpha_i,x_{i,t})\neq 0\)&lt;/span&gt;, this implies that &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; isn’t consistent for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, i.e. the sampling distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; doesn’t converge to &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assumptions and Requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(x_{i,t},u_[i,t])=0\)&lt;/span&gt;, i.e. weak exogeniety&lt;/li&gt;
&lt;li&gt;No perfect correlation between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;These assumptions imply the fixed effects estimates (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{FE}\)&lt;/span&gt;) are consistent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model, from &lt;a href=&#34;https://www.youtube.com/watch?v=sFvV9b1cGFc&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;The procedure is to average over time to get &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_i\)&lt;/span&gt;, and subtract this off, taking advantage of &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i=\bar{\alpha}_i\)&lt;/span&gt; where the later is averaged across time.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\alpha_i &amp;amp;: \text{unobserved hetergenity} \\
Cov(\alpha_i,x_{i,t}) &amp;amp;\neq 0 \\
u_{i,t} &amp;amp;: \text{error} \\
y_{i,t}&amp;amp;=\beta x_{i,t} + \alpha_i + u_{i,t}\\
\bar{y}_{t}&amp;amp;=\beta \bar{x}_{i} + \bar{\alpha}_i + \bar{u}_{i} \\
y_{i,t}-\bar{y}_{t}&amp;amp;=\beta_{FE} (x_{i,t}-\bar{x}_{i}) + (\alpha_i - \bar{\alpha}_i)+ u_{i,t} - \bar{u}_{i} \\
&amp;amp;= \beta_{FE} (x_{i,t}-\bar{x}_{i}) + u_{i,t} - \bar{u}_{i} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;removes anything that is constant over time, meaning effects of any time-constant variables can’t be estimated&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;first-differences&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;First differences&lt;/h3&gt;
&lt;p&gt;Context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;similar to fixed effects, but less asymptotically efficient if there are serially uncorrelated errors &lt;a href=&#34;https://www.youtube.com/watch?v=G7WqK2o474Y&#34;&gt;see this vid&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random Effects&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=bQampZBzU9Q&#34;&gt;description in this video&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Context:&lt;/p&gt;
&lt;p&gt;Assumptions and requirements:&lt;/p&gt;
&lt;p&gt;Model:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-score-matching&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Propensity Score Matching&lt;/h3&gt;
&lt;p&gt;Context:&lt;/p&gt;
&lt;p&gt;Treatment assignment is non-random and might be related to the relationship between treatment and outcome. So you match people on their propensity to be assigned to treatment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;microsoft-dowhy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Microsoft DoWhy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;see &lt;a href=&#34;https://github.com/microsoft/dowhy&#34;&gt;docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;experiment-design&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Experiment Design&lt;/h1&gt;
&lt;div id=&#34;foundations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Foundations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hypotheses&lt;/li&gt;
&lt;li&gt;Treatment and control conditions&lt;/li&gt;
&lt;li&gt;assumptions (including CI assumptions, SUTVA, …?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;concerns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concerns&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;network effects: front-end, it could be from different interactions, back-end it could be from changes in algorithm behavior for units in C based on different behavior from units in T – e.g. if a new recommender for units in T influences them to watch more of x, then x may also get recommended more to those in C.&lt;/li&gt;
&lt;li&gt;learning effects (change aversion, novelty seeking)&lt;/li&gt;
&lt;li&gt;early adopters&lt;/li&gt;
&lt;li&gt;Non-compliance: Those assicned to treatment may not actually experience the treatment.&lt;/li&gt;
&lt;li&gt;Crossover: Those assigned to control might gain access to the treatment.&lt;/li&gt;
&lt;li&gt;Treatment inhomogenaity: Some user segments might respond differently than others.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;treatment-units&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Treatment Units&lt;/h2&gt;
&lt;p&gt;What is the unit at which we assign treatments?&lt;/p&gt;
&lt;p&gt;Ideally independent individuals, but online it is hard to know who is visiting a webpage, or crucial to keep experiences comparable across devices. Also, in networks, individuals are not independent and it is important to keep user experience consistent across connected individuals.&lt;/p&gt;
&lt;p&gt;Proxies for individuals include;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User ID or account – most clearly tied to a user, but&lt;/li&gt;
&lt;li&gt;cookies – device and browser specific, so these could differ across a user’s browsers or devices.&lt;/li&gt;
&lt;li&gt;IP address (device request return address) – device specific, so a user’s experience might differ across devices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In networks, loosely connected clusters of individuals can be used as experimental units (see unofficial google data science blog post on this).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a solution to SUTVA violations, but decreases sample size and power dramatically&lt;/li&gt;
&lt;li&gt;cluster-based, stratified, serial, balanced, …&lt;/li&gt;
&lt;li&gt;propensity scores and matching&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;generative-model-and-adjustment-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generative Model and Adjustment Variables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Specify the hypothesized generative process&lt;/li&gt;
&lt;li&gt;Confounds&lt;/li&gt;
&lt;li&gt;Precision&lt;/li&gt;
&lt;li&gt;Neusance&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;common-designs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Common Designs&lt;/h2&gt;
&lt;p&gt;A/B testing&lt;/p&gt;
&lt;p&gt;A/A testing to estimate variation&lt;/p&gt;
&lt;p&gt;Variance reduction designs (paired designs, matching, …)&lt;/p&gt;
&lt;p&gt;Bandits for limited data or maximizing an objective&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assignment-mechanism&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assignment Mechanism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;maps samples (xi,yi) into treatment or control conditions.&lt;/li&gt;
&lt;li&gt;randomized control trials ideal, not always possible&lt;/li&gt;
&lt;li&gt;other options, …?&lt;/li&gt;
&lt;li&gt;test assignment validity, could use propensity scores or maybe chi square.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;duration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Duration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;learning effects: initial exploration or novelty seeking&lt;/li&gt;
&lt;li&gt;learning effects: initial change aversion&lt;/li&gt;
&lt;li&gt;power analysis for sample size&lt;/li&gt;
&lt;li&gt;time to run to mitigate&lt;/li&gt;
&lt;/ul&gt;
&lt;!---
# Experiment Design

## Objective, demand, and value

Objective is based on a metric (increase clickthrough or revenue per user). 

Demand -- if adding a new user feature, how can demand for the feature be assessed? 

value = benefit - cost. 

Expected value helps with decisions about the size and duration of an experiment

## Constructs, metrics, and scoping

Once an objective is clear, the details of what can be measured need to be flushed out. 

Metrics (fill in from udamy course section on metrics)

Netflix metrics -- streaming hours, retention (users staying on platform), viewing for a title (e.g. effected by artwork -- but may be at the detrement of general viewing?) 

### User flow and target metrics

Think of the sequence of actions a user might take on the site. Experiments can target transtition probabilities between any user/platform states. 

- number of clicks
- time on page
- ... 

### Invariants

Metrics that shouldn&#39;t change or differ across groups. For example, demographic variables should be the same across groups if the randomization or balancing worked properly. Also, many application performance metrics and business metrics should be unchanged, or monitored just in case they change. 

### Confounding variables

Mitigated by randomization. 

### Precision variables

Can also highlight features that one would want to match treatment and control on.

Examples: 

- number of posts
- number of followers
- visibility / impressions

## Metric Validation

User expreience research, retrospective analyses of past data or log files, ... . 

## Conditions

Define the experimental manipulations. 

## Treatment Units

What is the unit at which we assign treatments? 

Ideally independent individuals, but online it is hard to know who is visiting a webpage, or crucial to keep experiences comparable across devices. Also, in networks, individuals are not independent and it is important to keep user experience consistent across connected individuals. 

Proxies for individuals include;

- User ID or account -- most clearly tied to a user, but 
- cookies -- device and browser specific, so these could differ across a user&#39;s browsers or devices.
- IP address (device request return address) -- device specific, so a user&#39;s experience might differ across devices.

In networks, loosely connected clusters of individuals can be used as experimental units (see unofficial google data science blog post on this).

## Assignment Mechanism 

The assignment mechanism samples members from a population and assignes them to conditions. 

How are units assigned to treatments?

Randomized control trials are the ideal, but many issues arise in online experimentation settings. 

(look up desirable properties form causal inference notes)

### Population 

What group is being sampled from?

### Cohorts

Random sample, cluster-based, stratified, serial, balanced, ... 

### Synthetic Control Groups

Propensity matching

### Limitations

- Non-compliance: Those assicned to treatment may not actually experience the treatment. 
- Crossover: Those assigned to control might gain access to the treatment. 
- Treatment inhomogenaity: Some user segments might respond differently than others.

# Implementation

Batched, or real-time

## Size

power, sample size

## Duration

- learning effects: initial exploration or novelty seeking
- learning effects: initial change aversion

Solution: Consider running experiment past any initial observed effect.

### Temporal variation

### Optional stopping

Why it&#39;s an issue for frequentists

Optamizely using a threshold on FDR from likelihood ratio tests in frequentist setting

Bayesian justifications

p-values versus likelihood ratios

## Estimands

- Average treatment effect
- Treatment on treated

## Effiencicy or Variance Reduction

- look up variance reduction in A/B tests, a common method might be to incorporate precision variables like demographic data or user data such as device or browser. 

e.g. with a two sample t-test, $Y_t - Y_{t-1} = \beta_0$, versus a model $Y_t = \beta_0 + \beta_1 Y_{t-1}$

- get table for different tests of means from soc sci 10 notes.

## Checking assignment integrety

- Check demographics across buckets, if randomization worked then demographics should be approximately equally represented in the buckets. 

---&gt;
&lt;/div&gt;
&lt;div id=&#34;integrety-checks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integrety Checks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;check that those assigned to treatment received it&lt;/li&gt;
&lt;li&gt;check that stratifications were implemented correctly&lt;/li&gt;
&lt;li&gt;Check demographics across buckets, if randomization worked then demographics should be approximately equally represented in the buckets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-likelihood-versus-bayes-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum Likelihood versus Bayes’ Estimators&lt;/h2&gt;
&lt;p&gt;likelihood is one term in Bayes’ Theorem&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;glms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;GLMs&lt;/h1&gt;
&lt;p&gt;Again about the variance – this time it takes a form other than normal.&lt;/p&gt;
&lt;p&gt;likelihood, score, fisher information, robust veriance, …?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-tests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Common Tests&lt;/h1&gt;
&lt;div id=&#34;t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;t-test&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;different variance formulations.&lt;/li&gt;
&lt;li&gt;one-sample&lt;/li&gt;
&lt;li&gt;two independent sampels&lt;/li&gt;
&lt;li&gt;two dependent samples&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;anova&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ANOVA&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Model&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;z-test-for-proportions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Z test for proportions&lt;/h2&gt;
&lt;p&gt;Binary data&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chi-square&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chi-Square&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic Regression&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-ratio-and-bayes-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood Ratio and Bayes Factors&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;always-valid-p-values-ck-optimizely-white-paper&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;always valid p-values (ck optimizely white paper)&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;different-estimands-for-ci&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Different Estimands For CI&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Average treatment effect&lt;/li&gt;
&lt;li&gt;Treatment on treated&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;more-on-variance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;more on variance&lt;/h1&gt;
&lt;div id=&#34;emperical-variance-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Emperical Variance Estimation&lt;/h2&gt;
&lt;p&gt;bootstrapping&lt;/p&gt;
&lt;p&gt;A/A testing&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mixed-effects-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mixed Effects Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Maime’s consulting project? Other use cases?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;effiencicy-or-variance-reduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Effiencicy or Variance Reduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;look up variance reduction in A/B tests, a common method might be to incorporate precision variables like demographic data or user data such as device or browser.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g. with a two sample t-test, &lt;span class=&#34;math inline&#34;&gt;\(Y_t - Y_{t-1} = \beta_0\)&lt;/span&gt;, versus a model &lt;span class=&#34;math inline&#34;&gt;\(Y_t = \beta_0 + \beta_1 Y_{t-1}\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;get table for different tests of means from soc sci 10 notes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;glms-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GLMs&lt;/h2&gt;
&lt;!--
models for when the errors aren&#39;t assumed to be normally distributed
---&gt;
&lt;p&gt;Stats: descriptive, R-squared, chi-squared;&lt;/p&gt;
&lt;p&gt;Probability: distributions, CLT, sampling distributions, p-value,&lt;/p&gt;
&lt;p&gt;Stats: k-s, Q-Q plot, hypothesis testing, experimentation;&lt;/p&gt;
&lt;p&gt;Probability: Bayes, bootstrap&lt;/p&gt;
&lt;p&gt;Probability: maximum likelihood estimation; time series analysis (ARIMA models), granger causality&lt;/p&gt;
&lt;p&gt;Dynamic experimentation (multi-armed bandits)&lt;/p&gt;
&lt;p&gt;Unit testing; EDA visualization (seaborn, Plotly, Bokeh)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-inference-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Causal Inference Methods&lt;/h1&gt;
&lt;p&gt;Good medium blog post &lt;a href=&#34;https://towardsdatascience.com/causal-inference-using-difference-in-differences-causal-impact-and-synthetic-control-f8639c408268&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For each mention the assumptions and the model.&lt;/p&gt;
&lt;div id=&#34;difference-in-differences&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Difference in Differences&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-impact-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Causal Impact&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;synthetic-controls-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Synthetic Controls&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-score-matching-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Propensity Score Matching&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;fixed-effects-regression-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed Effects Regression&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;instrumental-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Instrumental variables&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-discontinuity-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Discontinuity&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Wealthfront Notes</title>
      <link>/MacStrelioff/insightstudying/wealthfront-deep-dive/</link>
      <pubDate>Wed, 18 Sep 2019 20:34:23 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/wealthfront-deep-dive/</guid>
      <description>


&lt;!--
LinkedIn summary to consider, form a DS at Wealthfront; 
Skilled in Statistics, Linear Algebra, Data Munging, and Data Visualization. Applications in Regression Analysis, Machine Learning, and Causal Inference. Experience formulating, solving, and communicating the results of impactful business questions. 
--&gt;
&lt;div id=&#34;todo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TODO:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Looking for: “You enjoy mentoring others on subjects such as causal inference, experimental design, data visualization or behavioral psychology”&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Make a few videos on experimentation and causal inference&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Work on: “You subscribe to Wealthfront’s values and mission and can articulate the reasons why, and have an understanding of our product, business model and key metrics”&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;job-description&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Job Description&lt;/h1&gt;
&lt;div id=&#34;projects-tasks-and-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Projects, Tasks, and Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Work closely with a cross-functional growth team of marketers, product managers, and engineers to generate new ideas for growth especially top of funnel, translating open-ended business issues into data questions, identifying practical analytic and A/B testing approaches to validate the ideas, and defining ongoing checks post-implementation to ensure that the growth mechanisms are working as planned. Carry out timely and well-documented analyses that influence our growth and company strategy&lt;/li&gt;
&lt;li&gt;Formally mentor junior colleagues and provide technical guidance to the team&lt;/li&gt;
&lt;li&gt;Continuously look for, and execute upon, opportunities to improve the quality of our data, infrastructure and products&lt;/li&gt;
&lt;li&gt;Understand and communicate a data-driven picture of our clients, product and business (and their interaction) to the company&lt;/li&gt;
&lt;li&gt;Promote the automation of repetitive tasks and the creation of tools over ad hoc analyses, making your insights available to anyone that can benefit from them&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ds-role&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;DS Role&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;You subscribe to Wealthfront’s values and mission and can articulate the reasons why, and have an understanding of our product, business model and key metrics&lt;/li&gt;
&lt;li&gt;4+ years of experience operating successfully within a world-class data science team or academic program&lt;/li&gt;
&lt;li&gt;Ability to access, transform, visualize, and model datasets with minimal engineering support, and communicate your results clearly, both verbally and in writing&lt;/li&gt;
&lt;li&gt;A firm foundation, rooted in experience, of the basic tool-box of regression and classification models, as well as causal inference, experimental design and time series modeling&lt;/li&gt;
&lt;li&gt;You can effectively prioritize and objectively edit your own work&lt;/li&gt;
&lt;li&gt;You care about producing reproducible work and have picked up tools to this end&lt;/li&gt;
&lt;li&gt;You enjoy mentoring others on subjects such as causal inference, experimental design, data visualization or behavioral psychology&lt;/li&gt;
&lt;li&gt;You have built something first hand that you’re proud of&lt;/li&gt;
&lt;li&gt;You are currently learning something outside of your immediate line of work&lt;/li&gt;
&lt;li&gt;Programming competency in R, Python, and SQL is a plus&lt;/li&gt;
&lt;li&gt;BS, MS, or PhD in computer science, mathematics, economics or related field&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Process:&lt;/h1&gt;
&lt;div id=&#34;phone-call-with-wealthfront-chief-data-officer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Phone call with Wealthfront Chief Data Officer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/vineet-singh-8ba416/&#34;&gt;VINEET SINGH&lt;/a&gt; - PhD in AI from Stanford in 1988.&lt;/li&gt;
&lt;li&gt;Extensive experience with marketing and statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;notes from call&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;started as wealth management product&lt;/li&gt;
&lt;li&gt;financial planning product, now made avilable to everyone, can specify goals for retirement, college saving, home buying, … .&lt;/li&gt;
&lt;li&gt;have all data on this, and can project future wealth.&lt;/li&gt;
&lt;li&gt;high yield cash account product, beginning of a banking services product&lt;/li&gt;
&lt;li&gt;CD, debit, credit card, etc.&lt;/li&gt;
&lt;li&gt;self-driving money: idea that they can automate finances for people. to meet short term and long terms savings needs.&lt;/li&gt;
&lt;li&gt;system can optimize how money from a paycheck or other accounts should flow between cash, investments, savings, based on these goals.&lt;/li&gt;
&lt;li&gt;accounts for all tax rules, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Team:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;170 people in company&lt;/li&gt;
&lt;li&gt;DS team more diverse (9 people (should be doubling lately))&lt;/li&gt;
&lt;li&gt;economists, statisticians, ML, data science, financial, … .&lt;/li&gt;
&lt;li&gt;Teams;&lt;/li&gt;
&lt;li&gt;investment algorithms&lt;/li&gt;
&lt;li&gt;financial planning optimization&lt;/li&gt;
&lt;li&gt;analytics (product, market, risk), (~ 4 people)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;data products, production ready systems&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Investing in growth: paid marketing, messaging within product to “cross sell”, referrals.&lt;/li&gt;
&lt;li&gt;Almost no paid marketing at all, have had some since introducing the cash account.&lt;/li&gt;
&lt;li&gt;Still want to apply a science to making it more effective.&lt;/li&gt;
&lt;li&gt;looking for ideas on growing cash account via a referral program.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;hiring one growth DS for across those areas&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- PROCESS --&gt;
&lt;p&gt;Process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this initial meeting&lt;/li&gt;
&lt;li&gt;technical phone screen: more of a filter, statistics and analytics Hypothesis testing, experimental design (first few items I have listed on methods), nothing like RL and contextual bandits, 1 hour long&lt;/li&gt;
&lt;li&gt;homework assignment: programming, 24 hours – can use Python or R, some analytic questions and need to provide some recommendations&lt;/li&gt;
&lt;li&gt;longer 5 hour interview&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Me: - finance as a hobby – Quantopian - HaaS freelance work&lt;/p&gt;
&lt;!-- Increase savings! --&gt;
&lt;ul&gt;
&lt;li&gt;Considering Nudges, e.g. maybe showing counterfactuals like how much interest a person would have gotten if they spent X less.&lt;/li&gt;
&lt;li&gt;I like the figure showing wealth accumulation – add credible intervals around it if it’s based on uncertain investments?&lt;/li&gt;
&lt;li&gt;Add oldified images of a user later on the timeline, maked that future self more concrete and increases saving behavior?&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- work life --&gt;
&lt;ul&gt;
&lt;li&gt;What do project pipelines look like – who is involved, how are tasks ideated and delegated?&lt;/li&gt;
&lt;li&gt;steakholders: marketing team, paid marketing team, ‘growth team’&lt;/li&gt;
&lt;li&gt;growth team: a product manager, designer, engineers, and another person, it’s just been formed.&lt;/li&gt;
&lt;li&gt;** First work on conversion from cash client to investment client.&lt;/li&gt;
&lt;li&gt;** and on paid marketing side, optimizing marketing on facebook, and Google ad words, and working on attribution to allocate marketing budget.&lt;/li&gt;
&lt;li&gt;** Lower customer acquisition costs.&lt;/li&gt;
&lt;li&gt;Identify cash clients who would have biggest benefit from opening investment account.&lt;/li&gt;
&lt;li&gt;regular check in with head of marketing every couple weeks&lt;/li&gt;
&lt;li&gt;Growth DS can look at all of these options (come up with ideas)&lt;/li&gt;
&lt;li&gt;Don’t want relationship to be where DS just does what marketing team asks. So DS isn’t expected to be marketing expert, but should come up with their own ideas and refine in collaboration with the other teams/partners.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;questions;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More informaiton about the role?&lt;/li&gt;
&lt;li&gt;Open projects or current directions?&lt;/li&gt;
&lt;li&gt;How big is the DS team?&lt;/li&gt;
&lt;li&gt;Could you tell me a little about your team specifically?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;phone-call-with-ben&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Phone call with Ben&lt;/h2&gt;
&lt;!-- about me --&gt;
&lt;ul&gt;
&lt;li&gt;PredictIt, finding arbitrage opportunities, programming them&lt;/li&gt;
&lt;li&gt;Strategy inference, JAGS model to get probability of a strategy being followed on each trial&lt;/li&gt;
&lt;li&gt;Using conjugacy to makde models update fast with streams of data – Tweet model, RL model for dissertation work&lt;/li&gt;
&lt;li&gt;RL model - an agent is learning a function to plan actions, for this function I uses gaussian basis functions and Bayesian logistic regression&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Questions --&gt;
&lt;ul&gt;
&lt;li&gt;Which team are you on?&lt;/li&gt;
&lt;li&gt;General DS team, each DS owned a specific area. Ben owned financial planning team (path model, projection on dashboard).&lt;/li&gt;
&lt;li&gt;A/B experimentation and user demographics, trying to figure out what properties of clients lead to an outcome they like.&lt;/li&gt;
&lt;li&gt;A/B encouraged to come up with own experiments – Company has a path they want to follow, quarterly objectives for products or things they want to deploy. Those are tested, e.g. predicting off actions if they’re going to do X or Y… .&lt;/li&gt;
&lt;li&gt;Have a lot of client data. DS extract insights from this data&lt;/li&gt;
&lt;li&gt;Have some DE responsibilities, Spark pipelines, Shiny.&lt;/li&gt;
&lt;li&gt;Scala, production level code.&lt;/li&gt;
&lt;li&gt;Hadoop, use AWS, often writing ETLs done on .. ?.&lt;/li&gt;
&lt;li&gt;doubling down on paid marketing bc it had a great response on the cash app.&lt;/li&gt;
&lt;li&gt;Frequentist much easier even if interpretation harder..&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Company founded by economists&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Econ - causal inference?&lt;/li&gt;
&lt;li&gt;Ad attribution?&lt;/li&gt;
&lt;li&gt;have 3rd party aps for ad attribution&lt;/li&gt;
&lt;li&gt;has been attribution issues when people come through multiple referral pages.&lt;/li&gt;
&lt;li&gt;working with impact radious to detect when a user clicks on multiple ads&lt;/li&gt;
&lt;li&gt;&lt;p&gt;different ways to weigh the ads&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;What are some of your projects?&lt;/li&gt;
&lt;li&gt;first Spark pipeline took a month&lt;/li&gt;
&lt;li&gt;other weeks when running an experiment to make sure data quality issues have been addressed.&lt;/li&gt;
&lt;li&gt;have inferastructure to run A/B experiment&lt;/li&gt;
&lt;li&gt;Do analyses that A/B platform can’t account for&lt;/li&gt;
&lt;li&gt;Do it in R markdown, present in HTML to various teams.&lt;/li&gt;
&lt;li&gt;lot of marketing requests in terms of data quality and impact of marketing experiments&lt;/li&gt;
&lt;li&gt;running queries, in redshift.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Monday roadmap meeting makes plan for week. Venieet helps prioritize.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Work culture? Is the workload managable?&lt;/li&gt;
&lt;li&gt;Once a week with manager, weekly lunch where one DS gives a presentation, and financial meetings.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;growth will have a DS, someone from marketing, someone from DE, … .&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;What are some upcoming projects?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Your research sounds really cool – do you do similar work (i.e. develop samplers) at Wealthfront?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Been really picky with decisions&lt;/li&gt;
&lt;li&gt;been looking for people with a few years of industry experience&lt;/li&gt;
&lt;li&gt;DS has been growing, so maybe opps for general DS roles&lt;/li&gt;
&lt;li&gt;Ben had phone interviews with&lt;/li&gt;
&lt;li&gt;Technical phone screens have a component to show interest / what you can bring to table&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Educated enthusasiasm, showing you’re not shopping around, show you dug deep and understand product and why it’s exciting&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;use rmd for HW interview&lt;/li&gt;
&lt;li&gt;&lt;p&gt;want to use data to grow the company&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;definitely a need for generalists&lt;/li&gt;
&lt;li&gt;but ads now seem to have a qualifier at the end, require T shaped candidate&lt;/li&gt;
&lt;li&gt;&lt;p&gt;had interns this year – one for research one for DS&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;quantity vs quality?&lt;/li&gt;
&lt;li&gt;PhD stands out – not just someone who did a Kaggle project or a sentiment analyses.&lt;/li&gt;
&lt;li&gt;Have owned actual research&lt;/li&gt;
&lt;li&gt;Not just a toy dataset, …&lt;/li&gt;
&lt;li&gt;Get data understand biases&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For Sr. need ownership.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Kinda flexible in terms of what the role is&lt;/li&gt;
&lt;li&gt;Have been some highly qualified people who did presentations&lt;/li&gt;
&lt;li&gt;Might be asked growth related questions&lt;/li&gt;
&lt;li&gt;Could be a general DS role too&lt;/li&gt;
&lt;li&gt;&lt;p&gt;People can get Sr roles out of academia if qualifications fit well..&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Wealthfront came in through Insight.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First time didn’t take anyone, took Ben, then didn’t take anyone.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;educated enthusasiasm, good answers for why I’m interested in product/company, answer that differs for them vs other investing firms&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;SQL or deplyr,&lt;/li&gt;
&lt;li&gt;&lt;p&gt;joins, etc in deplyr.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Used STAN in postdoc for sampling from ~500 params&lt;/li&gt;
&lt;li&gt;Lots of tasks are in prediction realm rather than inference realm..&lt;/li&gt;
&lt;li&gt;Predict good outcome based on user behavior&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If opportunity arose to talk about RL work, that’d be good!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;went on 3 onsites, Wealthfront was only offer&lt;/li&gt;
&lt;li&gt;Bosh, Uber, were other onsites&lt;/li&gt;
&lt;li&gt;Uber one part was desasterous.. Didn’t undertand what was being asked&lt;/li&gt;
&lt;li&gt;Speaking from experience is what gets Sr roles.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Slipping up ok if they’re able to catch themselves / adjust quickly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;technical-phone-screen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Technical phone screen&lt;/h2&gt;
&lt;p&gt;Interviewer: &lt;a href=&#34;https://www.linkedin.com/in/allen-sirolly-89b0b639/&#34;&gt;Allen Sirolly&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;From recruiter: “Our technical phone interview is meant to get a sense of the types of research or data problems that excite you. We will inquire about projects you have completed in the past; specifically, we want to hear about your personal contributions to the project and gauge your ability to give background on and extensions to your previous work. You are not permitted to use Google or other reference materials during the interview.”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sense of problems that excite you&lt;/li&gt;
&lt;li&gt;Inquire about contributions to past projects, understanding of past projects&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- my Projects --&gt;
&lt;ul&gt;
&lt;li&gt;Insight&lt;/li&gt;
&lt;li&gt;&lt;p&gt;HaaS&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Quantopian; wanted to create self-driving money for myself&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;replicated XIV&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Primarily an income investor. Created a strategy conceptually similar to risk parity – I treated dividends as a measure of risk and weighted stocks inversely to their dividend yields. Quantopian had a bug in dividends paid, so I submitted a ticket showing this with AAPL stock, and stopped using that platform.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Passion for automated financial decision making: PredictIt formula for guarenteed profits that accounted for relationships between prices, probability theory, and PredictIt fees. This is similar to finding an arbitrage relationship (relations between prices and probability theory) and accounting for taxes (fees).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- fit for growth team --&gt;
&lt;ul&gt;
&lt;li&gt;HaaS work on fb ads and google analytics (talk about these tools)&lt;/li&gt;
&lt;li&gt;Insight work on modeling user conversion from a free-trial to subscriber&lt;/li&gt;
&lt;li&gt;YT channel, casually testing thumbnails&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- I like the Products --&gt;
&lt;ul&gt;
&lt;li&gt;Path: Financial planning service that shows a projection of wealth and goals (homebuying, retirement, …)&lt;/li&gt;
&lt;li&gt;Free financial advice in blogs (read buy vs rent, played with ourse purchase planning tool)&lt;/li&gt;
&lt;li&gt;Investment management and automating established methods (risk parity, modern portfolio theory, .. )&lt;/li&gt;
&lt;li&gt;Portfolio line of credit (Margin Lending)&lt;/li&gt;
&lt;li&gt;Rolling out direct deposit, debit cards&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Other excitement --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I also like the principled investment philosophy (Modern protfolio theory, accounting for taxes, … )&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Personal finance is something that I think about a lot, and a theme in my hobby projects (Quantopian) and research (PredictIt).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;** excitement: One of my Quantopian ideas was conceptually similar to a wisk parity strategy – Dividends are a proxy for risk, so I weighted stocks proportional to the reciprocal of their dividend; (&lt;span class=&#34;math inline&#34;&gt;\(w_i = \frac{1/d_i}{\sum_i 1/d_i}\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.prnewswire.com/news-releases/wealthfront-cuts-risk-parity-expense-ratio-in-half-to-0-25-300631933.html&#34;&gt;Cheap risk parity fund has /$500 mil assets under management&lt;/a&gt;, the risk parity method equalizes the risk contributions of asset classes within a portfolio&lt;/li&gt;
&lt;li&gt;** I love the vision of self-driving money, this kind of automation is something I’d want to build for myself if I had the time.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.wealthfront.com/introducing-free-financial-planning/&#34;&gt;Democratizing quality financial advice&lt;/a&gt;; I liked the &lt;a href=&#34;https://www.wealthfront.com/home-guide&#34;&gt;home guide&lt;/a&gt;, I’ve been thinking about renting vs buying for a while now. My BaC card gives 3/% cash back on travel, and airbnb counts for that – so it’s like a 3/% discount on rent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Personal finance is something that I think about a lot, and a theme in my hobby projects (Quantopian) and research (PredictIt).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Vs Bettermint --&gt;
&lt;ul&gt;
&lt;li&gt;Wealthfront has better automation, Bettermint pivoted to human advisors&lt;/li&gt;
&lt;li&gt;Self driving money is something I’ve been working on making for myself through Quantopian and PredictIt.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Questions / Interest --&gt;
&lt;ul&gt;
&lt;li&gt;Which team are you on?&lt;/li&gt;
&lt;li&gt;analytics,&lt;/li&gt;
&lt;li&gt;working on?&lt;/li&gt;
&lt;li&gt;building out ab testing,&lt;/li&gt;
&lt;li&gt;support product team for cash product&lt;/li&gt;
&lt;li&gt;interesting ad hoc project – would like to know what portion of are&lt;/li&gt;
&lt;li&gt;so that if a competitor comes out with anotehr high level savings account, what fraction would they expect to lose..&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;difference between savings features, product offerings, ..&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;know where clients deposit money from.&lt;/li&gt;
&lt;li&gt;know if money is deposited from a high yield account&lt;/li&gt;
&lt;li&gt;also have detailed client banking data from a 3rd partner, so can see what accounts they transfer from&lt;/li&gt;
&lt;li&gt;want to see how elastic demand is&lt;/li&gt;
&lt;li&gt;discrete choice models&lt;/li&gt;
&lt;li&gt;&lt;p&gt;random utility might be a first implementation&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;What is your tech stack?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R for ad hoc and exploration, pipelines in Scala and Spart run on AWS, looking to transition to Python.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;What is your like ‘methods stack’, so what are the statistical tools that you use frequently?&lt;/li&gt;
&lt;li&gt;in house libraries to do survival analysis for client churn&lt;/li&gt;
&lt;li&gt;haven’t done much ML in past, haven’t used sk learn much&lt;/li&gt;
&lt;li&gt;CRAN packages/libraries?&lt;/li&gt;
&lt;li&gt;can’t randomize on price (eg discount)&lt;/li&gt;
&lt;li&gt;don’t have any network effects.. Other than close friends, family (not a great question on my part)&lt;/li&gt;
&lt;li&gt;Frequentist, Bayesian, Causal inference (methods for addressing violations of true randomization e.g. early adopters, noncompliance or contamination, no possible control group (counterfactuals, synthetic controls, causal impact from Google))&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if you make an update available and someone doesn’t upgrade, then the difference you detect might be a different between early adopters and late adopters, rather than between treatment (new app) and control (old app) conditions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vineet had mentioned working on converting from cash account users to investment management users – Are there currently attempts at doing this?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Next steps?&lt;/li&gt;
&lt;li&gt;if we decide to move forward, a HW, SQL modeling.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;invite to onsite.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;growth DS to provide useful data input to growth team&lt;/li&gt;
&lt;li&gt;grow users&lt;/li&gt;
&lt;li&gt;&lt;p&gt;margine fraction of clients wealth they give&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;in past DS hasn’t taken leading role, might change, work in close collaboration with product team&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Value adds / ideas--&gt;
&lt;ul&gt;
&lt;li&gt;Why switch from self-management to their portfolio management?&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Maybe a tool to compare a person’s portfolio to Wealthfront’s portfolio? Then a person should switch if switching is easy and their portfolio&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;underperforms or&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;has higher uncertainty or&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;a lower SHARPE?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Which blog articles motivate investing?&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Exploratory: See which users viewed a blog post&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Experimental: Suggest blog posts to users (Treatment)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;focus on Treatment on Treated vs. users who were suggested other posts and viewed them&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;controls for engagement (viewing suggestion)?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;controls for financial motivation (crossover effects i.e. viewing the blog post when it is ont suggestsd (assigned C experienced T))?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;reflection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reflection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Talked about predictit project, quantopian a little (didn’t mention the dividend risk parity strategy), and he asked about the dynamic bayesian logistic regression model from ARL. I fumbled a bit explaining the context of the dblr.&lt;/li&gt;
&lt;li&gt;He mentioned discrete choice models, so I mentioned a background in it and asked if he was using random utility models, he said he was just learning about them.&lt;/li&gt;
&lt;li&gt;Overall could work on running the conversation more, talking about my projects and accomplishments.&lt;/li&gt;
&lt;li&gt;Allen mentioned estimation of demand elasticity to see how many users might move assets if a better interest rate account hit the market, and random utility models to study the choices that users make in the app.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;thank-you-email&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thank-You email&lt;/h3&gt;
&lt;p&gt;Hi Allen,&lt;/p&gt;
&lt;p&gt;Thank you for your call yesterday! I enjoyed hearing a little more about the projects at Wealthfront, and particularly your work with discrete choice models sounded like an interesting direction that would be fun to discuss further.&lt;/p&gt;
&lt;p&gt;Best, Mac&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preparation&lt;/h1&gt;
&lt;!-- Followup from PM phone call --&gt;
&lt;div id=&#34;ideas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ideas&lt;/h2&gt;
&lt;!-- PROJECT DESCRIPTIONS --&gt;
&lt;/div&gt;
&lt;div id=&#34;project-descriptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Project Descriptions:&lt;/h2&gt;
&lt;div id=&#34;haas-freelance-work&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;HaaS freelance work&lt;/h3&gt;
&lt;p&gt;Situation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Haas stands for housing as a service, it’s a membership-based coliving startup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Task:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Want to increase memberships through facebook and google ads.&lt;/li&gt;
&lt;li&gt;I’m leveraging a little SEO experience to help with designing ads;&lt;/li&gt;
&lt;li&gt;Brighter photos, photos with faces, and color schemes that contrast FB’s color scheme of white and blue,&lt;/li&gt;
&lt;li&gt;thse features are associated with higher CTR for YouTube thumbnails.&lt;/li&gt;
&lt;li&gt;Becoming familiar with Google Analytics and Facebook Ad Analytics to run A/B tests with these ads&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Action:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;See which drive more interest in CTR, and more conversions in applications for membership, and which attract the demographics that my client wants to attract.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;insight-project&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Insight Project&lt;/h3&gt;
&lt;p&gt;Situation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 week consulting project&lt;/li&gt;
&lt;li&gt;Startup focused on digital workspaces (e.g. google docs)&lt;/li&gt;
&lt;li&gt;Wanted to understand user base.&lt;/li&gt;
&lt;li&gt;Wanted to be able to predict who would convert from a free trial to a paying customer.&lt;/li&gt;
&lt;li&gt;(skip) Wanted to attribute conversion to product features with conversion.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Task:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data:&lt;/li&gt;
&lt;li&gt;Company demographic information, company size and revenue.&lt;/li&gt;
&lt;li&gt;Used this to communicate conversion rates by demographic to heads of different teams using tables and figures.&lt;/li&gt;
&lt;li&gt;Account activity data which was a sum of actions taken within an account each day.&lt;/li&gt;
&lt;li&gt;I transformed this into cumulative sum instead of daily sum to increase interpretability and modeling ease.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(skip) Action: (skip) - Used cross sections of cumulative account activity data after 7 days since the account was created. (skip) - Cast as classification problem, used SMOTE to address class imbalance, (skip) - used 5-fold CV to evaluate models on AUC, (skip) - ended with a RF algorithm for these predictions.&lt;/p&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Delivered presentation and slide deck with tables and figures to a group of team leaders.&lt;/li&gt;
&lt;li&gt;Delivered a Python script to implement the model, and a slide deck describing the modeling procedure and performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;predictit-project&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PredictIt project&lt;/h3&gt;
&lt;p&gt;Situation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prediction markets aggregate information and incentivize rational behavior, so one would expect the prices to be accurate.&lt;/li&gt;
&lt;li&gt;At an individual level, lots of fallacies have been established in lab settings.&lt;/li&gt;
&lt;li&gt;I wanted to know if any fallacies would manifest in market prices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conjunction fallacy: “Trump” traded higher hand “Republican” for extended period of time.&lt;/li&gt;
&lt;li&gt;Disjunction fallacy: Most market prices add up to something greater than $1, but because of fees it’s often not profitable to take advantage of this.&lt;/li&gt;
&lt;li&gt;CPT subjective probability function: Overpricing of low probability events and Underpricing of high probability events&lt;/li&gt;
&lt;li&gt;&lt;p&gt;– notably this was highest as expiration approached, a context that is more similar to lab studies that don’t have delayed rewards.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we’re working on automating trades based on these and other indights.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;quantopian&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quantopian:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;recreated XIV, fund that trades VIX futures weighted so that the effective days to expiration is approximately 30.&lt;/li&gt;
&lt;li&gt;mean revision&lt;/li&gt;
&lt;li&gt;moving average crossover&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;child-bandit-modeling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Child bandit modeling&lt;/h3&gt;
&lt;p&gt;Situation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wanted to know what strategies they followed, and if their strategy changed over time.&lt;/li&gt;
&lt;li&gt;Kids repeatedly chose between two gambling machines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Action:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I developed a model that inferred the probability of following each strategy on each trial&lt;/li&gt;
&lt;li&gt;Used Bayes’ Rule to smooth out these inferences / accumulate information over trials&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kids do seem to switch strategies, probably don’t act optimally, and have different strategies depending on age (switchy vs sticky)&lt;/li&gt;
&lt;li&gt;Result: A lot of experiments focus on which strategy best fits all data across trials – the approach that I took allows for changes in strategies, and the exciting result was an ability to track latent decision making strategies over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Value:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implications for user intent modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- JUST IN CASE --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ci-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CI methods&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;diff in diff&lt;/li&gt;
&lt;li&gt;causal impact&lt;/li&gt;
&lt;li&gt;synthetic control&lt;/li&gt;
&lt;li&gt;IV analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-dive&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deep Dive&lt;/h2&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Summary (2-3 sentences):&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;products&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Products&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Path: Financial planning service that shows a projection of wealth and goals (homebuying, retirement, …)&lt;/li&gt;
&lt;li&gt;Free financial advice in blogs&lt;/li&gt;
&lt;li&gt;Investment management&lt;/li&gt;
&lt;li&gt;Portfolio line of credit (Margin Lending)&lt;/li&gt;
&lt;li&gt;Rolling out direct deposit, debit cards&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.prnewswire.com/news-releases/wealthfront-research-shows-clients-who-regularly-engage-with-automated-financial-advice-save-more-300723506.html&#34;&gt;article&lt;/a&gt;: Path usage associated with higher savings, meaning better outcomes for savers (but maybe the savers have higher income / can save more already?)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.prnewswire.com/news-releases/wealthfront-cuts-risk-parity-expense-ratio-in-half-to-0-25-300631933.html&#34;&gt;Cheap risk parity fund has /$500 mil assets under management&lt;/a&gt;, the risk parity method equalizes the risk contributions of asset classes within a portfolio&lt;/li&gt;
&lt;li&gt;** excitement: One of my Quantopian ideas was conceptually similar to a wisk parity strategy – Dividends are a proxy for risk, so I weighted stocks proportional to the reciprocal of their dividend; (&lt;span class=&#34;math inline&#34;&gt;\(w_i = \frac{1/d_i}{\sum_i 1/d_i}\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.wealthfront.com/introducing-free-financial-planning/&#34;&gt;Democratizing quality financial advice&lt;/a&gt;; I liked the &lt;a href=&#34;https://www.wealthfront.com/home-guide&#34;&gt;home guide&lt;/a&gt;, I’ve been thinking about renting vs buying for a while now. My BaC card gives 3/% cash back on travel, and airbnb counts for that – so it’s like a 3/% discount on rent.&lt;/li&gt;
&lt;li&gt;** Like the vision of self-driving money, this kind of automation is something I’s want to build for myself if I had the time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Vs Bettermint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wealthfront has better automation, Bettermint double backed to human advisors&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Self driving money is something I’ve been working on making for myself through Quantopian and PredictIt.&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;users&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Users&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;use-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use cases&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;dynamics-network-marketplace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dynamics (network, marketplace, … )&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;user-journey-and-user-flows&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;User Journey and User Flows&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;monetization-strategies&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monetization Strategies&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;major-metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Major metrics&lt;/h3&gt;
&lt;p&gt;Growth / retention&lt;/p&gt;
&lt;p&gt;Engagement&lt;/p&gt;
&lt;p&gt;Monetization&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;possible-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Possible Questions&lt;/h3&gt;
&lt;p&gt;How can you help drive key metrics?&lt;/p&gt;
&lt;p&gt;What behaviors should the UI drive and how can you make product recommendations?&lt;/p&gt;
&lt;p&gt;What data/features would you potentially have access to?&lt;/p&gt;
&lt;p&gt;How are certain data products built (recommenders, content feeds)?&lt;/p&gt;
&lt;p&gt;How would a site/app reduce fraud?&lt;/p&gt;
&lt;p&gt;Based on team you are interviewing for&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;signup-process&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Signup process:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One page for all “Legal Stuff” questions, instead of a page for each question..&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Is there any dropoff on each question? I’m expecting the dropoff for Legal stuff to be lower if it were one page than compared to it being a page for each question.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;important-metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Important Metrics:&lt;/h3&gt;
&lt;p&gt;Growth:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Weekly new users&lt;/li&gt;
&lt;li&gt;Month over month active account growth&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Engagement or value:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total deposits&lt;/li&gt;
&lt;li&gt;Total assets tracked&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;culture-and-vision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Culture and Vision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;“We’re here because more people across the economic spectrum deserve to live secure and rewarding lives.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;business-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Business Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Offer a high yield savings account&lt;/li&gt;
&lt;li&gt;Offer funds managed by the author of ‘A Random Walk Down Wall Street’&lt;/li&gt;
&lt;li&gt;Use Modern Portfolio Theory, smart beta and tax-loss harvesting for larger accounts.&lt;/li&gt;
&lt;li&gt;Collect a 0.25% fee on managed funds&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;glassdoor-interview-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Glassdoor interview questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;probably puzzle questions.&lt;/li&gt;
&lt;li&gt;behavior : go through the resume.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;competition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Competition&lt;/h3&gt;
&lt;p&gt;Wealthfront predacessor was &lt;a href=&#34;https://venturebeat.com/2019/09/16/google-schedules-pixel-4-hardware-event-for-october-15/&#34;&gt;KaChing&lt;/a&gt;, a social network and platform for users to pick a fantasy stock portfolio, and investors to allocate funds acording to the picks of high skill users. KaChing had software that would evaluate the skill of it’s users.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.betterment.com/&#34;&gt;Betterment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.investopedia.com/wealthfront-vs-betterment-4587963&#34;&gt;Wealthfront vs Betterment&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>/MacStrelioff/insightstudying/machine-learning/</link>
      <pubDate>Tue, 17 Sep 2019 14:21:49 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/machine-learning/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#considerations&#34;&gt;Considerations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bias-variance-trade-off&#34;&gt;Bias-variance trade-off&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overfitting-and-underfitting&#34;&gt;Overfitting and Underfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cost-functions-and-metrics&#34;&gt;Cost Functions and Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering&#34;&gt;Feature Engineering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#seperability&#34;&gt;Seperability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions-about-form-of-relationship&#34;&gt;Assumptions about form of relationship&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unsupervised&#34;&gt;Unsupervised&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#clustering&#34;&gt;Clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#k-means&#34;&gt;k-means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agglomerative&#34;&gt;Agglomerative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devisive&#34;&gt;Devisive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-decomposition&#34;&gt;Matrix Decomposition&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#pca-principle-components-analysis&#34;&gt;PCA Principle Components Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#svd-singular-value-decomposition&#34;&gt;SVD Singular Value Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-based&#34;&gt;Text-based&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lda-latent-direchilote-analysis&#34;&gt;LDA Latent Direchilote Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lsa-latent-semantic-analysis&#34;&gt;LSA Latent Semantic Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#supervised&#34;&gt;Supervised&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#regression&#34;&gt;Regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear&#34;&gt;Linear&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#support-vector-machine-svm&#34;&gt;Support Vector Machine (SVM)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification&#34;&gt;Classification&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#naive-bayes&#34;&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discriminant-analysis&#34;&gt;Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#knn&#34;&gt;KNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ensemble-methods&#34;&gt;Ensemble Methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#trees-and-forests&#34;&gt;Trees and Forests&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#regression-1&#34;&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification-1&#34;&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#open-ended-template&#34;&gt;Open ended Template&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#raw-data-structure&#34;&gt;Raw data structure?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#define-target-variable&#34;&gt;Define target variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#preprocession&#34;&gt;Preprocession,&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering-1&#34;&gt;Feature engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance-metrics-and-how-they-apply&#34;&gt;Performance metrics and how they apply&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cautions&#34;&gt;Cautions:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imagine-rolling-the-model-out&#34;&gt;Imagine rolling the model out&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-explainability&#34;&gt;Model explainability&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#shapely-additive&#34;&gt;Shapely additive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#locally-linear&#34;&gt;Locally linear&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;considerations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Considerations&lt;/h1&gt;
&lt;div id=&#34;bias-variance-trade-off&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bias-variance trade-off&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;overfitting-and-underfitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overfitting and Underfitting&lt;/h2&gt;
&lt;p&gt;Cross Validation; Classification&lt;/p&gt;
&lt;p&gt;Assess by comparing a performance metric on training and testing data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cost-functions-and-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cost Functions and Metrics&lt;/h2&gt;
&lt;p&gt;Modeling: Validation metrics,&lt;/p&gt;
&lt;p&gt;metrics (precision, recall, F1)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Precision_and_recall&#34;&gt;metrics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comparison of prec, rec, F1, &lt;a href=&#34;https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
acc  &amp;amp;= \frac{tp+tn}{tp+tn+fp+fn} \\
prec &amp;amp;= \frac{tp}{tp+fp}\\
rec  &amp;amp;= \frac{tp}{tp+fn}\\
F1   &amp;amp;= 2\frac{prec*rec}{prec+rec}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;R squared, RMSE&lt;/p&gt;
&lt;p&gt;Likelihood&lt;/p&gt;
&lt;p&gt;regularisation (ridge, lasso),&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Engineering&lt;/h2&gt;
&lt;div id=&#34;seperability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Seperability&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-about-form-of-relationship&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assumptions about form of relationship&lt;/h3&gt;
&lt;p&gt;Process: feature selection, data cleaning / imputation with common pitfalls, model training, bias-variance trade-off&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unsupervised&lt;/h1&gt;
&lt;div id=&#34;clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clustering&lt;/h2&gt;
&lt;div id=&#34;k-means&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;k-means&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;agglomerative&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Agglomerative&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;devisive&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Devisive&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrix Decomposition&lt;/h2&gt;
&lt;div id=&#34;pca-principle-components-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PCA Principle Components Analysis&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;svd-singular-value-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SVD Singular Value Decomposition&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;text-based&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Text-based&lt;/h2&gt;
&lt;div id=&#34;lda-latent-direchilote-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LDA Latent Direchilote Analysis&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;lsa-latent-semantic-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LSA Latent Semantic Analysis&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Supervised&lt;/h1&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression&lt;/h2&gt;
&lt;div id=&#34;linear&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linear&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;support-vector-machine-svm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Support Vector Machine (SVM)&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;div id=&#34;naive-bayes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Naive Bayes&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;discriminant-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Discriminant Analysis&lt;/h3&gt;
&lt;div id=&#34;lda&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;LDA&lt;/h4&gt;
&lt;p&gt;When to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;qda&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;QDA&lt;/h4&gt;
&lt;p&gt;When to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;knn&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;KNN&lt;/h3&gt;
&lt;p&gt;When to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;When to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ensemble-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ensemble Methods&lt;/h1&gt;
&lt;p&gt;boosting, bagging&lt;/p&gt;
&lt;p&gt;XGBoost, NLP (bag of words, vector-space models, sentiment analysis), Rec systems, Collab filtering, Optimization, XGBoost&lt;/p&gt;
&lt;div id=&#34;trees-and-forests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trees and Forests&lt;/h2&gt;
&lt;p&gt;Gini Impurity: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
m:&amp;amp;\text{ Region} \\
k:&amp;amp; \text{ Class} \\
G =&amp;amp; \Sigma_k \hat{p}_{m,k}(1-\hat{p}_{m,k})
\\=&amp;amp; 1 - \Sigma_k \hat{p}_{m,k}^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Decrease in Impurity &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
t:&amp;amp;\text{ Node} \\
N_t:&amp;amp;\text{ Samples at node }t \\
s_t:&amp;amp;\text{ Split }t \text{ into } t_L,t_R\\
\Delta i(s_t,t)=&amp;amp; i(t) - \frac{N_L}{N_t} i(t_L) - \frac{N_R}{N_t} i(t_R)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Mean Decrease in Impurity: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
T:&amp;amp; \text{Trees} \\
N_T:&amp;amp; \text{Number of trees} \\
t:&amp;amp;\text{ Nodes} \\
s_t:&amp;amp;\text{ Split }t \text{ into } t_L,t_R\\
v(s_t):&amp;amp;\text{Variable split on at } s_t \\
p(t):&amp;amp; \text{Proportion of total samples at node } t \\
Imp(X_m)=&amp;amp;\frac{1}{N_T} \Sigma_{T}\Sigma_{t\in T:v(s_t)\in X_m} p(t) \Delta i(s_t,t)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e&#34;&gt;Good blog on analysis of feature importance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also see &lt;a href=&#34;https://github.com/slundberg/shap&#34;&gt;SHAP&lt;/a&gt; for interpreting predictions from tree models!&lt;/p&gt;
&lt;p&gt;Benefits of trees over OLS&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quora.com/How-does-a-random-forest-fix-regression-problems-non-normality-heteroscedasticity-multicollinearity-outliers-missing-values-and-categorical-variables&#34; class=&#34;uri&#34;&gt;https://www.quora.com/How-does-a-random-forest-fix-regression-problems-non-normality-heteroscedasticity-multicollinearity-outliers-missing-values-and-categorical-variables&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;regression-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Classification&lt;/h3&gt;
&lt;p&gt;Deep learning, Rec systems, Content and collaborative filtering, Optimization, Probabalistic programming&lt;/p&gt;
&lt;div id=&#34;assessment&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Assessment&lt;/h4&gt;
&lt;p&gt;Confusion matrix, F1, precision-recall curve&lt;/p&gt;
&lt;p&gt;ROC, AUC&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;open-ended-template&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Open ended Template&lt;/h1&gt;
&lt;div id=&#34;raw-data-structure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Raw data structure?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Table of information about consumer&lt;/li&gt;
&lt;li&gt;Table of transaction history&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;define-target-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Define target variable&lt;/h2&gt;
&lt;p&gt;e.g. Fraud would be if a consumer calls and labels a transaction as fraud.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocession&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocession,&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EDA - distributions, correlations&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature engineering&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-metrics-and-how-they-apply&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance metrics and how they apply&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Precision&lt;/li&gt;
&lt;li&gt;Recall&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;cautions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cautions:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Don’t mention models you aren’t familiar with&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;imagine-rolling-the-model-out&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imagine rolling the model out&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What issues might arise?&lt;/li&gt;
&lt;li&gt;What actions do you take in production?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-explainability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model explainability&lt;/h2&gt;
&lt;div id=&#34;shapely-additive&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Shapely additive&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;locally-linear&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Locally linear&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;eg &lt;a href=&#34;https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Limitations&lt;/h3&gt;
&lt;p&gt;Limitations described a little in the FLowcast paper &lt;a href=&#34;https://flowcast.ai/Flowcast_whitepaper_-_Explainability.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;biased for collinear features?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>YouTube Strategy</title>
      <link>/MacStrelioff/unlisted/youtube_strategy/</link>
      <pubDate>Sun, 08 Sep 2019 00:38:42 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/youtube_strategy/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#planned-videos&#34;&gt;Planned Videos&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#about-me-channel-video&#34;&gt;“About Me” Channel Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-science-interview-walkthrough-series&#34;&gt;Data Science Interview Walkthrough Series&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-scientist-interview-walkthrough-intro&#34;&gt;“Data Scientist Interview Walkthrough: Intro”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-scientist-interview-walkthrough-probability-question-set-4&#34;&gt;Data Scientist Interview Walkthrough: Probability Question Set 4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-do-you-want-this-role&#34;&gt;Why do you want this role?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#current-topics-in-data-science&#34;&gt;Current Topics in Data Science&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#explain-some-research-at-pintrest&#34;&gt;Explain some research at Pintrest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-science-topics-and-concepts&#34;&gt;Data Science Topics and Concepts&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-probability-and-what-is-a-probability-function&#34;&gt;What is Probability and What is a Probability Function?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-a-power-analysis-explained-at-3-levels-of-difficulty&#34;&gt;What is a Power Analysis? (Explained at 3 levels of difficulty)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-a-sampling-distribution&#34;&gt;What is a sampling distribution?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ab-testing-versus-bandits&#34;&gt;A/B testing versus bandits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-i-get-free-bitcoin-with-this-simple-python-script-and-you-can-too&#34;&gt;How I Get Free Bitcoin With This Simple Python Script (And you can too!)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#behavioral-question-series&#34;&gt;Behavioral question series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probability-distributions-series&#34;&gt;Probability Distributions Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ml-algorithm-series&#34;&gt;ML algorithm series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-data-scientists-do&#34;&gt;What Data Scientists Do&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rejection-challenge&#34;&gt;100-Rejection challenge&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#general-strategies-and-guides&#34;&gt;General Strategies and Guides&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#meta&#34;&gt;Meta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seo&#34;&gt;SEO&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#keywords&#34;&gt;Keywords&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#title&#34;&gt;Title:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#description&#34;&gt;Description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tags&#34;&gt;Tags&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#higher-ranking&#34;&gt;Higher Ranking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ctr&#34;&gt;CTR&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#thumbnails&#34;&gt;Thumbnails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#brand-consistency&#34;&gt;Brand Consistency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#within-videos&#34;&gt;Within Videos&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#duration&#34;&gt;Duration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hook-and-intro&#34;&gt;Hook and Intro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#content&#34;&gt;Content&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#end-screen&#34;&gt;End Screen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#general-tips&#34;&gt;General tips&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finished&#34;&gt;Finished:&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-scientist-interview-walkthrough-probability-part-1&#34;&gt;Data Scientist Interview Walkthrough: Probability, Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-scientist-interview-walkthrough-probability-part-2&#34;&gt;Data Scientist Interview Walkthrough: Probability, Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-scientist-interview-walkthrough-probability-question-set-3&#34;&gt;Data Scientist Interview Walkthrough: Probability Question Set 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;planned-videos&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Planned Videos&lt;/h1&gt;
&lt;!--
- Post the finished videos on my website under the video tab
--&gt;
&lt;div id=&#34;about-me-channel-video&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“About Me” Channel Video&lt;/h2&gt;
&lt;p&gt;Why?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make a landing video for my website that builds credibility (PhD, masters, multiple projects) and likability (goal is to empower people to break down barriers)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Behavioral, “Tell me about yourself”:&lt;/p&gt;
&lt;p&gt;What they’re looking for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;get from Lacy’s pages, and from Deniz’s youtube videos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;About Me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PhD.. , masters in statistics (borrow from cover letters?)&lt;/li&gt;
&lt;li&gt;Through grad school I pursued any project that was interesting to me – It was my time to develop valuable skills and learn what I wanted to do.&lt;/li&gt;
&lt;li&gt;fast list of projects (decision making, prediction markets, statistics, AI and models of the mind, … )&lt;/li&gt;
&lt;li&gt;relate to this job at this company&lt;/li&gt;
&lt;li&gt;talk about YouTube channel and growth strategy&lt;/li&gt;
&lt;li&gt;demonstrate the value that I add to produce positions&lt;/li&gt;
&lt;li&gt;decision making, so studying people and the patterns or experiences that shape their behavior&lt;/li&gt;
&lt;li&gt;Amazed to learn that there’s a huge interplay between psychology and computer science – reinforcement learning Skinner in the 30’s, turned into a plethora of models in computer science, which are now being used to explain human behavior, and neural correlates of key values in those models are being discovered.&lt;/li&gt;
&lt;li&gt;Overall I was more interested in naturally occurring datasets, and experimental design – how we structude and verify our beliefs, and then use those beliefs to guide our actions.&lt;/li&gt;
&lt;li&gt;So here at X that might look like using naturally occurring user data to develop and validate models that inform choices about product design.&lt;/li&gt;
&lt;li&gt;I really like diving into a new theory or model and gaining an redicovering the intuition from some first principles.&lt;/li&gt;
&lt;li&gt;I also love finding connections between seemingly desparate fields or ideas. This is one of the reasons I was drawn to math and statistics – the same formal patterns and expressions are manifested in so many diverse applications.&lt;/li&gt;
&lt;li&gt;During graduate school I contributed to a number of projects (behavioral econ, strategy inference, RL, … ) and learned about theories that have real applications – how people make decisions with tradeoffs, how specific ways of delivering reward can influence behavior.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I’m also deeply interested in the right way to justify a decision or belief – this is something that propelled my interest in experimentation, Bayesian statistics, and causal inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;My mission is to break down barriers, mainly by democratizing education.&lt;/li&gt;
&lt;li&gt;About me: UCI, recorded lectures for students, stats masters, data scientist, …&lt;/li&gt;
&lt;li&gt;&lt;p&gt;About this channel: I’ll primarily use this channel to publish educational resources on statistics and programming, mainly for aspiring data scientists. I’ll also use this channel as a platform to discuss hobby projects related to data science.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-science-interview-walkthrough-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Science Interview Walkthrough Series&lt;/h2&gt;
&lt;p&gt;I realized there was a lot of material to help prepare for software engineering videos, but very little for data scientist interviews. So I’m compiling videos of interview questions my firends and I have been asked in data sicence interviews, and questions that I’ve gotten from data scientists who are currently interviewing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Target Audience: People who want to know more about data scientist interviews&lt;/li&gt;
&lt;li&gt;Marketing: Post on Facebook, LinkedIn data science interest groups&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Value:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“I have a masters in statistics and a PhD in cognitive science” (can I add results to this?) My credability: Do an ‘about me’ video where I talk about my work at UCI, masters degree in statistics.&lt;/li&gt;
&lt;li&gt;“To make these videos the mose valuable use of your time, I’m focusing exclusively on questions and content that have come up during interviews.”&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;data-scientist-interview-walkthrough-intro&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;“Data Scientist Interview Walkthrough: Intro”&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;When I was studying to be a data scientist, I realized that there were lots of resources with loads of material that won’t actually help in an interview because they were too vague or too theoretical, or just contained very little material that actually comes up in an interview.&lt;/li&gt;
&lt;li&gt;I also noticed that there are plenty of walkthroughs for the problems that come up in software engineering interviews, but nothing like that for data science interviews.&lt;/li&gt;
&lt;li&gt;So with this series, I’m creating videos that will walk you through the questions that are currently coming up in silicone valley data scientist interviews&lt;/li&gt;
&lt;li&gt;To make this the most valuable use of your time, I’ll be walking through questions that have come up in my interviews, or in my friends interviews, as well as questions that I gotten from data scientists and product managers who are actively interviewing.&lt;/li&gt;
&lt;li&gt;This way these videos are going to be the most relevant and helpful resource for you if you want to learn what it takes to excel in a data science interview.&lt;/li&gt;
&lt;li&gt;So.. what are these videos?&lt;/li&gt;
&lt;li&gt;Each video will start with a question. I’d recommend pausing the video and trying the question yourself for a few minutes, and then watching my walkthrough for the question.&lt;/li&gt;
&lt;li&gt;As I walk through the questions, I’ll mention the concepts that the questions are testing for so that you can note them and dive into those later.&lt;/li&gt;
&lt;li&gt;Now, some of the best questions start easy and become progressively harder, so what that means here is that, if you don’t watch to the end then you might miss surprises that an interviewer can throw at you,&lt;/li&gt;
&lt;li&gt;that the people who aren’t watching my videos won’t be expecting.&lt;/li&gt;
&lt;li&gt;So for now, I would recommend watching the videos, trying the problems, and seeing how I walkthrough solutions, and watching through the end so that there aren’t any unexpected surprises&lt;/li&gt;
&lt;li&gt;If you find this helpful or interesting, like this video so that I know that the work I put into this series is having a good impact for you and other people&lt;/li&gt;
&lt;li&gt;Also, if there are questions that you have encountered and want answered, comment them below so that I can get you a walkthrough!&lt;/li&gt;
&lt;li&gt;And finally be sure to subscribe if you want access to the most recent walkthroughs&lt;/li&gt;
&lt;li&gt;Thank you, and I hope you enjoy these videos!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-scientist-interview-walkthrough-probability-question-set-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Scientist Interview Walkthrough: Probability Question Set 4&lt;/h3&gt;
&lt;p&gt;Thumbnail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Me centered, looking perplexed in a direction of the title, with a green/blue background?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I was asked this question during a technical phone screen for a large, popular tech company that you know.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Outro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So I have a few things to say before I end this video.&lt;/li&gt;
&lt;li&gt;First, if you have any questions, feel free to leave them in the comment section so that they can be addressed.&lt;/li&gt;
&lt;li&gt;Second, these videos actually take time to make, time that I could spend doing things that are more fun like drinking coffee.&lt;/li&gt;
&lt;li&gt;I’m deciding whether I should make more videos like this, so if you liked it, please “make it official” and click the like button on YouTube.&lt;/li&gt;
&lt;li&gt;Finally, try subscribing if you want to see the latest videos like this&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-do-you-want-this-role&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why do you want this role?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;see &lt;a href=&#34;https://www.youtube.com/watch?v=taHSZEhTzPc&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=taHSZEhTzPc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tip #1: Understand their PAIN and how to solve it.&lt;/li&gt;
&lt;li&gt;Tip #2: Show the right demeanour.&lt;/li&gt;
&lt;li&gt;Tip #3: Tell stories.&lt;/li&gt;
&lt;li&gt;Tip #4: Sell yourself.&lt;/li&gt;
&lt;li&gt;Tip #5: Be confidently YOU.&lt;/li&gt;
&lt;li&gt;and this &lt;a href=&#34;https://www.youtube.com/watch?v=RiKXKYNlwFQ&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=RiKXKYNlwFQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Also maybe a professional development workshop on this topic? -&lt;a href=&#34;https://www.youtube.com/watch?v=URs2vLAPNT0&#34;&gt;general on behavioral interviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dENi7K2lX4U&#34;&gt;general on rapport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OyV8LELM_HM&#34;&gt;general on being liked&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;current-topics-in-data-science&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Current Topics in Data Science&lt;/h2&gt;
&lt;div id=&#34;explain-some-research-at-pintrest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explain some research at Pintrest&lt;/h3&gt;
&lt;p&gt;See:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://labs.pinterest.com/user/themes/pin_labs/assets/paper/pinning-www17.pdf&#34;&gt;Predicting Intent Using Activity Logs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pintrest:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Content sharing platform, and image based search engine&lt;/li&gt;
&lt;li&gt;Prior research found user types; 1) casual browsers, 2) responding to a specific goals&lt;/li&gt;
&lt;li&gt;Prior research found different temporal horizons, e.g. using Pintrest to plan dinner versus using Pintrest to plan vacations.&lt;/li&gt;
&lt;li&gt;On Pintrest users view pins (content) to boards (collections).&lt;/li&gt;
&lt;li&gt;Pins have 33 categories (DIY, food and drink, …)&lt;/li&gt;
&lt;li&gt;Users can look at closeups of Pins, which provides additional information&lt;/li&gt;
&lt;li&gt;Users can click through to visit a site affiliated with a pin&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Problem and volue of a solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Users can use a product with different intents, and the material that a user finds valuable may depend on their intent.&lt;/li&gt;
&lt;li&gt;Being able to infer a user’s intent from their behavior during a session would enable a personalized experience aligned with a user’s goals.&lt;/li&gt;
&lt;li&gt;Example: Someone looking for dinner recepies might want something they can quickly make with common ingredients, while someone looking for potluck ideas might want more unique dishes, and someone who is just browsing food might prefer a variety of exotic food images.&lt;/li&gt;
&lt;li&gt;User intent can vary with each session, so quickly identifying intent within a session can help with changing the interface and content shown to the user in real time based on their inferred intent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Current paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Goals: Understand how intent informs behavior, and whether intent can be inferred from behavior.&lt;/li&gt;
&lt;li&gt;Two dimensions of intent:&lt;/li&gt;
&lt;li&gt;Goal specificity: whether the intent is stimulus specific (glutin free chocolate chip recepies) or vague (just browsing to pass time).&lt;/li&gt;
&lt;li&gt;and temporal range: Time horizon for goal completion (tonight, next week, next month, …).&lt;/li&gt;
&lt;li&gt;In psych, motivation is defined as a directing and activating or invigorating force. The directional component maps onto what Pintrest is calling goal specificity, and the invigorating component might map onto the inverse of what Pintrest refers to as temporal range.&lt;/li&gt;
&lt;li&gt;In psych, there’s a notion of goal-directed and habitual behavior. The former is driven by a specific desired outcome (similar to goal specificity here) while the later is a less mindful response to stimuli (eg boredom triggers a passive arousal seeking).&lt;/li&gt;
&lt;li&gt;In social psych, attitudes have been thought about along a gradient of specificity – so maybe things like time-related words in a query can be used to identify goal-directed users.&lt;/li&gt;
&lt;li&gt;Shopping websites can be used for a specific purchase (goal-directed) or in an experiential way (habitual).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample: 5369 females mean age 33.5, 564 males mean age 38.6, all from US and on Pintrest during July 2016&lt;/li&gt;
&lt;li&gt;Survey asking;&lt;/li&gt;
&lt;li&gt;7 point likert scale: “Are you visiting Pintrest with a goal in mind?”&lt;/li&gt;
&lt;li&gt;“When are you planning to act on what you’re looking for today”&lt;/li&gt;
&lt;li&gt;Question about Pintrest specific motivations (eg finding DIY ideas)&lt;/li&gt;
&lt;li&gt;“What are you looking for on Pintrest today?” Response options were based on Pintrest categories&lt;/li&gt;
&lt;li&gt;Focused on data in the session immediately following the survey&lt;/li&gt;
&lt;li&gt;primarily analyzed first 10 minutes because this window had peak performance, around 27% of sessions had fewer than 10 minutes&lt;/li&gt;
&lt;li&gt;Over 850,000 behavioral events (views, closeups, searches) across 5933 users within 10 minutes.&lt;/li&gt;
&lt;li&gt;Also analyzed behavior from when the user first created account until a week after survey completion&lt;/li&gt;
&lt;li&gt;used Holm-corrected p-values, a method that is more powerful than a the conservative Bonferroni correction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mitigating limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;coverage bias: mitigated by uniformly sampling from users for inclusion&lt;/li&gt;
&lt;li&gt;participation bias: compared activity of users who completed the survey (addigned survey, took survey) to those who saw the survey popup but did not complete the survey (assigned survey, did not take survey).&lt;/li&gt;
&lt;li&gt;Survey takers had been using Pintrest longer, and had more saved pins. Inferred from this that engaged users were&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Findings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Goal-specific users are: more focused, search more (1.1 vs 0.4 searches), spend more time browsing specific categories of content in detail, more likely to reference saved content, less likely to return in next 7 days, less likely to attribute their visit to boredom (2% vs 25% for habitual), more likely to attribute visit to making something (26% vs 5% for habitual)&lt;/li&gt;
&lt;li&gt;users with short term goals: specific categories of content, likely to reference saved content, less likely to save new content, goal-specific users more likely to act in short term, habitual users more unsure of taking actions&lt;/li&gt;
&lt;li&gt;Demographic differences: females more likely to be goal-specific (49% vs 40%), males unsure of taking action, older users more short-term goals, younger users more unsure of taking action, food, drink, DIY searches more likely to be goal-specific and act in short term, travel, entertainment more likely to be habitual and likely to act in long term, intent moderates the type of recepies users save&lt;/li&gt;
&lt;li&gt;Model can predict goal specificity and temporal range.&lt;/li&gt;
&lt;li&gt;Current session data is most informative, historical activity data also helpful&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://labs.pinterest.com/user/themes/pin_labs/assets/paper/understanding-purchasing-at-pinterests.pdf&#34;&gt;Understanding behaviors that lead to purchasing&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-science-topics-and-concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Science Topics and Concepts&lt;/h2&gt;
&lt;div id=&#34;what-is-probability-and-what-is-a-probability-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is Probability and What is a Probability Function?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;notes from prob and stats page&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-power-analysis-explained-at-3-levels-of-difficulty&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is a Power Analysis? (Explained at 3 levels of difficulty)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Explain a power analysis (start complicated, get simple to keep people watching till the end)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;an analysis done to find a sample size&lt;/li&gt;
&lt;li&gt;formula based on t-distribution example&lt;/li&gt;
&lt;li&gt;formula based on P(..)&lt;/li&gt;
&lt;li&gt;General definition and power functions&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;mix Peter’s github repo / simulations with an explanation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-sampling-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is a sampling distribution?&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ab-testing-versus-bandits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A/B testing versus bandits&lt;/h2&gt;
&lt;!---
todo:

- Bank teller problem 
- random number generator from a uniform random number generator
- estimate median from Google
- variances of different experiment designs
- multinomial and guessers &#39;favorite color&#39; and a proportion of users who guess randomly
- bandits vs A/B tests, particularly for lots of conditions
- LI: android you can randomize, iphone you can&#39;t (synthetic control)
- Chris Alborn&#39;s demos?
---&gt;
&lt;/div&gt;
&lt;div id=&#34;how-i-get-free-bitcoin-with-this-simple-python-script-and-you-can-too&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How I Get Free Bitcoin With This Simple Python Script (And you can too!)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;can make video, then make it public later.. (after job searching ends)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Video on my script for clicking the roll button on freebitco.com&lt;/li&gt;
&lt;li&gt;Simplify the script to be a python script that can be run from the terminal with one command.&lt;/li&gt;
&lt;li&gt;In the video, show people how to download the script from my GitHub and run the script from terminal :D&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add referral code so that I get a cut of the coins generated by anyone watching my video&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;CTR:&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Thumbnail: Colorful background, with me looking and pointing at a screenshot of the python scrip &amp;amp; maybe text that says free on it?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Advertising:&lt;/li&gt;
&lt;li&gt;“Sharing this video is like giving your friends free money!”&lt;/li&gt;
&lt;li&gt;Incentivize: The more people who consistently do this, the more of the referral bonous I’ll give back.&lt;/li&gt;
&lt;li&gt;Show an example of how much I could give back if they run this consistently.&lt;/li&gt;
&lt;li&gt;My goal is to make a certain amount each week, so I’ll modify the referral bonus kickbacks based on that.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;So share this video to increase your kickback.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;behavioral-question-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Behavioral question series&lt;/h2&gt;
&lt;p&gt;Make videos based on the questions / resources from Lacy ..&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;probability-distributions-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Probability Distributions Series&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pick common distributions, describe them from the Wikipedia page&lt;/li&gt;
&lt;li&gt;Show their probability functions&lt;/li&gt;
&lt;li&gt;Show (derive?) their CDF&lt;/li&gt;
&lt;li&gt;Derive their MGFs&lt;/li&gt;
&lt;li&gt;Derive score and Fisher Information&lt;/li&gt;
&lt;li&gt;Describe / show conjugacy properties?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ml-algorithm-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ML algorithm series&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;include scripts, and put the scripts on my GitHub for download (then I can cite GH DLs on my CV)&lt;/li&gt;
&lt;li&gt;Explanations, intuitions of different ML algorithms&lt;/li&gt;
&lt;li&gt;Strengths and weaknesses of different ML algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-data-scientists-do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What Data Scientists Do&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Break down some blog posts from big companies (Google, Netflix, Instacart, …)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;rejection-challenge&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;100-Rejection challenge&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-vZXgApsPCQ&#34;&gt;TED Talk on this&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=w-HYZv6HzAs&#34;&gt;self confidence&lt;/a&gt;: repetition, managing self-talk, ….&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;general-strategies-and-guides&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;General Strategies and Guides&lt;/h1&gt;
&lt;div id=&#34;meta&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Meta&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;schema.. an html code snippit that gives google info about a video?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;go to html code, ask dennis to do it for you… or google it lol&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;playlists good for watch time – this might be in place of a long video..&lt;/li&gt;
&lt;li&gt;videos with individual titles to funnel people in&lt;/li&gt;
&lt;li&gt;keyword is the the search query, er the target search query&lt;/li&gt;
&lt;li&gt;pick a target keyword for each video&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Youtube algo inferring search intent&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;seo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SEO&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.briggsby.com/reverse-engineering-youtube-search&#34;&gt;good overview article&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;keywords&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Keywords&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Search for similar videos, check their keywords using Google’s ‘inspect source’.&lt;/li&gt;
&lt;li&gt;Search those keywords on &lt;a href=&#34;https://app.vidiq.com/channels/86a5e064-1aba-406e-99c5-f2f9ce27be30/seo/keyword-research&#34;&gt;vidIQ&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;keyword is the the search query, er the target search query&lt;/li&gt;
&lt;li&gt;Use vidiq to determine search volume potential on video keywords&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Google search volume is not always indicative of interest in a topic on Youtube so it’s not a good idea to rely on it for choosing video keywords&lt;/li&gt;
&lt;li&gt;Keywords that have video carousels in the Google SERPS can increase CTR of a video and pull in more views&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;title&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;instead of ‘part 1, ..’ find seperate videos that can rank on their own, maybe try ‘question 1’ so they sound self-contained&lt;/li&gt;
&lt;li&gt;title case – capitalize first letter of all words in title&lt;/li&gt;
&lt;li&gt;Video is shorter than 60 characters so it isn’t cut off in the SERP&lt;/li&gt;
&lt;li&gt;Video contains your target keyword&lt;/li&gt;
&lt;li&gt;Remove excess jargon&lt;/li&gt;
&lt;li&gt;See SEO Framework/YouTube Distribution section - in the expanded section in middle of page - for examples.&lt;/li&gt;
&lt;li&gt;Do not include episode numbers for your videos that are in playlists. CTR and views might go down as episode numbers go up and the playlist is already directing the order in which your audience views so there’s no need to really include the numbers&lt;/li&gt;
&lt;li&gt;Briggsby found that titles with 47-48 characters perform best. This has not been reflected in our own videos so far but this is still a good target range to adhere to as a best practices.&lt;/li&gt;
&lt;li&gt;Front-load your keyword in your title if possible. Youtube gives more weight to words that come at the beginning of the title. Don’t do this if it comes at the expense of the flow of your title or users’ understanding of your title.&lt;/li&gt;
&lt;li&gt;Note that your title in the video description can be different than what appears in the thumbnail (to make the title visually more succinct)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;description&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Description&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in description, overview of everything, links of anything you want to push people to, timestamps for parts of the video.&lt;/li&gt;
&lt;li&gt;longer descriptions better for ranking bc youtube, sweet spot ~350 words.&lt;/li&gt;
&lt;li&gt;Include your target keyword in first line and give an overview of what topics your video will cover&lt;/li&gt;
&lt;li&gt;Descriptions that consist of 200-350 may to improve rankings&lt;/li&gt;
&lt;li&gt;Your description should not give away so much of the content of your video that people no longer need to watch to get the bulk of the information&lt;/li&gt;
&lt;li&gt;Link at the front, so that it isn’t cut off&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Including time stamps in your video can help prevent people who just want to understand one topic from your video from bouncing off the page when they don’t get the answers they want immediately (this is good for longer videos)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add summary with timecodes in the description &lt;a href=&#34;https://creatoracademy.youtube.com/page/lesson/edu-discovery?cid=educational-channel&amp;amp;hl=en#strategies-zippy-link-2&#34;&gt;timecodes&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From &lt;a href=&#34;https://www.briggsby.com/reverse-engineering-youtube-search&#34;&gt;this blog&lt;/a&gt; - Brief CTA for a relevant next video to drive session length and related videos. - Subscribe CTA to help build your distribution power. - 50 to 150 words in the description to summarize and describe your video, mentioning a broad match variation of your keyword 1 to 3 times, depending on length. - List additional videos to watch, focused on your best content about the same or a similar subject matter as this video, creating recommendation clusters. - A brief bio and/or list of social accounts, which can help build your distribution power. - Avoid exceeding 400 words without a good reason to do so, because stuffing or going broadly off topic can harm performance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tags&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tags&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Look at “view page source” and find their keywords.&lt;/li&gt;
&lt;li&gt;type key words in youtube search, and add the things that it suggests.&lt;/li&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;keyword&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;long tail keywords&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;2-3 broadest category ones&lt;/li&gt;
&lt;li&gt;&lt;p&gt;focus on 6-8 tags&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Include 6-8 tags&lt;/li&gt;
&lt;li&gt;Use this formula: 1st tag - target keyword, 2nd + 3rd tags - synonyms for your target keyword, 4th + 5th tags - related long tail keywords, 6th + 7th keywords - broad categories that your video fits into&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use ‘page inspector’ option in Chrome to look up keywords used in high traffic videos on related topics! This increases your chances of being placed as a suggested video for that video&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;higher-ranking&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Higher Ranking&lt;/h3&gt;
&lt;p&gt;Good advice from &lt;a href=&#34;https://www.youtube.com/watch?v=xdcG1M6dy58&#34;&gt;joma here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;video guidelines for higher ranking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;add 5-10s pauses to let users try the problems on their own (and to increase view time)&lt;/li&gt;
&lt;li&gt;ask users to subscribe&lt;/li&gt;
&lt;li&gt;sell likes and subscriptions:&lt;/li&gt;
&lt;li&gt;“As you can imagine, it takes a lot of time and effort to make these videos. If you found this content helpful at all, please click the like button to let me know. Also, if there are interview questions that you’ve gotten, or concepts you would like an explanation on, feel free to comment below so that I know which videos to prioritize in the future. Finally, if you want to stay up to date on interview walkthroughs for common and recent interview questions, then click subscribe to stay on top of the latest interview topics.”&lt;/li&gt;
&lt;li&gt;“Unfortunately, interviews are largely luck in terms of whether you’ve seen the question before.&lt;/li&gt;
&lt;li&gt;In this series I walk through problems that reflect those that are currently being asked in data scientist interviews at highly competitive companies.&lt;/li&gt;
&lt;li&gt;After watching these interview question walkthroughs, you’ll be able to confidently brease through your technical interviews.&lt;/li&gt;
&lt;li&gt;Many resources, can be overwhelming and ultimately a waste of time because much of that material won’t ever come up.&lt;/li&gt;
&lt;li&gt;My goal is to make these videos the highest return on your time by focusing on content that my friends and I have seen in interviews.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ctr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CTR&lt;/h2&gt;
&lt;div id=&#34;thumbnails&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thumbnails&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tubefilter.com/2014/06/19/youtube-thumbnails-definitive-guide/&#34;&gt;guide here&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Face in a small frame over written math&lt;/li&gt;
&lt;li&gt;Face large, with text formulas around it&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a background photo, choose something with blues, greens, and high contrast, and maybe face shapes e.g. search: blue green smoke black background face&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From Jessica:&lt;/li&gt;
&lt;li&gt;Use bold text that can be read in a variety of different sizes&lt;/li&gt;
&lt;li&gt;Use high resolution pictures&lt;/li&gt;
&lt;li&gt;Use colors that contrast against the Youtube website color scheme of red and white&lt;/li&gt;
&lt;li&gt;Yellow, green, orange, purple pink and blue&lt;/li&gt;
&lt;li&gt;Use high contrast photos&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Many experts recommend including people/faces in thumbnails to increase CTR. However, we tried this with the Kanban series and did not see CTR increase so the jury is out.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find a high contrast background, and maybe in ppt, put a title in white font on a black box in the center?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;brand-consistency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Brand Consistency&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;within-videos&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Within Videos&lt;/h2&gt;
&lt;div id=&#34;duration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Duration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Do an incognito tab search in Youtube for your target keyword and note the average duration of the top three videos. This should give you a good idea of an appropriate duration for the type of content you’re creating&lt;/li&gt;
&lt;li&gt;Videos should be at least 4:30 long&lt;/li&gt;
&lt;li&gt;According to Briggsby: The average duration of videos that rank in the top 5 is 11 minutes and 44 seconds, and there appears to be a positive relationship between video length and rank performance.&lt;/li&gt;
&lt;li&gt;Audiences also tend to “like” videos more that are in the 10-16 minute range&lt;/li&gt;
&lt;li&gt;However, keep in mind that you should tailor your video duration to be appropriate for the type of content you are creating and that’s why you should always be sure to note how long the videos are that are ranking for your target keyword&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;hook-and-intro&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hook and Intro&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Context: “This video is part of a series where I work though common and current Data Science interview questions. If you’d like to see more, feel free to like so that I know the these videos are having an impact, and to subscribe if you want to see the latest videos.”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://creatoracademy.youtube.com/page/lesson/edu-video-formats?cid=educational-channel&amp;amp;hl=en#strategies-zippy-link-2&#34;&gt;Credibility&lt;/a&gt; “I have a PhD in Cognitive Science and a Masters in Statistics, currently a fellow in a sillicon valley data science bootcamp, where I’m actively interviewing, and networking with data scientists who conduct interviews.”&lt;/li&gt;
&lt;li&gt;Need / Value: “I realized other content sucks, and this is the best use of your time.”&lt;/li&gt;
&lt;li&gt;Legitimacy: “This is a question that I got from an interview”&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;start with an overview of the video.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;start with what they want for the hook – then ask to like/subscribe at the end for the people that obviously like.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Hook your audience in the first 10 seconds but don’t give away the answer to the central question of the video that early or your audience will bounce&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You shouldn’t string them along for too long because you want your video to get to the point, but you don’t want to lose them within the first 10 seconds&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Playlists can help increase the amount of time that your viewers stay on Youtube and, in turn, improve your rankings&lt;/li&gt;
&lt;li&gt;However, be sure that you keep your playlists focused around one highly-specific topic. If you switch up the topic too much, we see high drop-off and videos end up looking ineffective to the Youtube algorithm.&lt;/li&gt;
&lt;li&gt;Additionally, if you’re creating a playlist, make sure that videos can stand alone as well within the context of the playlist.&lt;/li&gt;
&lt;li&gt;If they’re ranking for a standalone keyword, they won’t always appear within the context of your video and you don’t want people to bounce because they feel like they need to have watched 4 or 5 previous videos in order to understand the one that they landed on&lt;/li&gt;
&lt;li&gt;Writing a script is highly recommended.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By sharing your script and refining your opening, you avoid the risk of shooting footage without a clear, strong opening&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;content&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Content&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;biggest spikes are when there’s new visual information..&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Mention emotions and feelings through a video “At this point you feel like you’re doing well, then …”&lt;/li&gt;
&lt;li&gt;Speak in a desirable way – engaging or calm.&lt;/li&gt;
&lt;li&gt;Emphasize value of the content at every step.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pause for them to try and also to increase view time metric&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;end-screen&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;End Screen&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://support.google.com/youtube/answer/6388789?hl=en&#34;&gt;Some advice from YouTube&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a custom end screen with bold text to clearly indicate your calls to action&lt;/li&gt;
&lt;li&gt;Next video to watch and subscriber buttons are good ones&lt;/li&gt;
&lt;li&gt;If possible have your end screen pop up on the side while your video is still going on, instead of coming on against a static screen. According to Youtube expert Tim Schmoyer, this can increase engagement and prevent people from clicking away from your video&lt;/li&gt;
&lt;li&gt;Focus on your two most important call to actions instead of bombarding your viewers with too many options&lt;/li&gt;
&lt;li&gt;Make sure to hold your “subscribe” element in your end screen (see end screen example here) for at least 5 seconds before cutting away.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;general-tips&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;General tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Write everything out (maybe in a slightly vague way), and then spend time explaining the terms. From analytics, long periods of writing seem to hurt user retention.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;finished&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Finished:&lt;/h1&gt;
&lt;div id=&#34;data-scientist-interview-walkthrough-probability-part-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Scientist Interview Walkthrough: Probability, Part 1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Coins in a bag&lt;/li&gt;
&lt;li&gt;Probability rules&lt;/li&gt;
&lt;li&gt;Bayes’ Theorem, Bayes Factor&lt;/li&gt;
&lt;li&gt;Frequentist Hypothesis Testing&lt;/li&gt;
&lt;li&gt;Likelihood Ratio&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-scientist-interview-walkthrough-probability-part-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Scientist Interview Walkthrough: Probability, Part 2&lt;/h3&gt;
&lt;p&gt;Geometric distribution problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This video is part of a series where I work though common and current Data Science interview questions.&lt;/li&gt;
&lt;li&gt;Subscribe if you want to see the latest videos.&lt;/li&gt;
&lt;li&gt;And if this sounds interesting or helpful, give this video a like so that I know the these videos are having an impact,&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-scientist-interview-walkthrough-probability-question-set-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Scientist Interview Walkthrough: Probability Question Set 3&lt;/h3&gt;
&lt;p&gt;Thumbnail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Me centered, looking perplexed in a direction of the title, with the problem full screen?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There’s two questions in this video.&lt;/li&gt;
&lt;li&gt;The first was from an onsite interview at a late stage startup.&lt;/li&gt;
&lt;li&gt;The second was from an actual test that a company gave as a screen.&lt;/li&gt;
&lt;li&gt;Here they are:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Outro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So I have a few things to say before I end this video.&lt;/li&gt;
&lt;li&gt;First, if you have any questions, feel free to leave them in the comment section so that they can be addressed.&lt;/li&gt;
&lt;li&gt;Second, these videos actually take time to make, time that I could spend doing things that are more fun like drinking coffee.&lt;/li&gt;
&lt;li&gt;I’m deciding whether I should make more videos like this, so if you liked it, please “make it official” and click the like button on YouTube.&lt;/li&gt;
&lt;li&gt;Finally, try subscribing if you want to see the latest videos like this&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Behavioral Interviewing</title>
      <link>/MacStrelioff/insightstudying/behavioral-interviews/</link>
      <pubDate>Thu, 05 Sep 2019 13:59:25 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/behavioral-interviews/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#principles&#34;&gt;Principles&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-behavioral-questions&#34;&gt;Common Behavioral Questions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tell-me-a-little-about-your-background.&#34;&gt;“Tell me a little about your background.”&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#child-bandits&#34;&gt;Child bandits:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tell-me-about-the-project-you-worked-on-at-insight.&#34;&gt;“Tell me about the project you worked on at Insight.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-are-you-interested-in-a-career-as-a-data-scientist&#34;&gt;“Why are you interested in a career as a data scientist?”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tell-me-about-your-research-talk-to-me-about-your-last-role.&#34;&gt;“Tell me about your research / talk to me about your last role.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-are-you-interested-in-joining-this-company&#34;&gt;“Why are you interested in joining this company?”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#talk-about-a-time-you-were-on-a-team-that-struggled-with-effective-communication.&#34;&gt;“Talk about a time you were on a team that struggled with effective communication.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tell-me-about-a-time-you-made-a-mistake-at-work.&#34;&gt;“Tell me about a time you made a mistake at work.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#talk-about-a-situation-that-required-you-to-explain-your-technical-work-to-someone-outside-your-industry-or-field.&#34;&gt;“Talk about a situation that required you to explain your technical work to someone outside your industry or field.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#talk-about-a-time-you-were-asked-to-manage-an-important-project.&#34;&gt;“Talk about a time you were asked to manage an important project.”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#briefcase-document&#34;&gt;Briefcase Document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thank-you-email&#34;&gt;Thank-You Email&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linkedin-thank-you-email&#34;&gt;LinkedIn Thank-You Email&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#recruiters&#34;&gt;Recruiters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#first-interviewer&#34;&gt;First Interviewer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gigi-zhang&#34;&gt;Gigi Zhang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#caitlyn&#34;&gt;Caitlyn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#yelp-thank-you-email&#34;&gt;Yelp Thank-You Email&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cold-recruiter-email&#34;&gt;Cold Recruiter Email&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#email-to-robinhood-recruiter&#34;&gt;Email To RobinHood recruiter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;!--
Try: 

- bring a notebook and use that instead of the whiteboard. -- keeps seeting closer / more engaging, also keeps a log of what was covered in the interview.

In my personal life, I manage a YouTube channel. Recently I&#39;ve pulled my viewer data from the YouTube API and found that my viewers are most engaged Sunday-Tuesday. I&#39;m acting on this insight by scheduling my videos for release on Sundays.

--&gt;
&lt;!---
Decent cover letter:
Wealthfront&#39;s vision of automating personal finance is very exciting to me. I have been personally interested in investing since I was 16 years old, actively trading since I was 18 years old, and I&#39;ve recently been learning about automated trading and portfolio management particularly for income investing. 

In addition to the vision, the job requirements align with the tasks that I like the most within the realm of data science. I&#39;m deeply interested in experimentation, analytics, and data-driven decision making. Throughout my PhD in an experimental psychology lab, my interest in optimizing experimental designs and conclusions drove me to pursue a concurrent MS in Statistics and to peruse the literature on topics like methods for valid inference with optional stopping. I also use analytics to guide decisions in my personal life. As a recent example, I&#39;ve pulled viewership data for my channel from YouTube&#39;s API and found that my viewers are more engaged early in the week -- this guided a decision to schedule videos for release on Sundays. 

Overall I&#39;m excited by the vision of automating swaths of personal finance to optimize passive income, and have applied many of the relevant skills through my research and in my personal life.

I hope you are interested in the skills and projects outlined in my resume, and I look forward to hearing from you if you think I could add value to the team!
---&gt;
&lt;div id=&#34;principles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Principles&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Interviews are conversations, not tests&lt;/li&gt;
&lt;li&gt;Use stories to establish a connection and demonstrate fit for a role and company&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tell me about yourself, your academic research, why data science?; 30 second elevator pitch vs. 2 minute phone screen/on-site&lt;/p&gt;
&lt;p&gt;Why this company? what value can you add? Tell me about a time when…&lt;/p&gt;
&lt;p&gt;STAR chart (Situation, Task, Action, Result)&lt;/p&gt;
&lt;p&gt;Mock interviews: Practice, practice, practice&lt;/p&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;(from Cracking the Coding Interview, pp. 16-17 on Product Manager interviews)&lt;/p&gt;
&lt;p&gt;PM Areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Handeling ambiguity: When faced with an ambigious problem, they evaluate you on seeking information, prioritizing the most important parts, solving the problem in a structured way&lt;/li&gt;
&lt;li&gt;Customer focused attitide: Identifying user segments or intents, understanding who the customer is and how they use the product.&lt;/li&gt;
&lt;li&gt;Customer focus technical: some understanding of the technical aspects of the product&lt;/li&gt;
&lt;li&gt;Communication: Particularly able to explain concepts to broad audiences&lt;/li&gt;
&lt;li&gt;Passion for technology: enthusiasm in describing prior work, excited and passionate about the company and role&lt;/li&gt;
&lt;li&gt;Leadership: handle conflicts well, take initiative, understand people, people like working with you.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dev Lead areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Teamwork / Leadership: filtering people who are too arrogant or passive.&lt;/li&gt;
&lt;li&gt;Prioritization: understanding what parts of a project are critical, and what timelines are possible with limited resources&lt;/li&gt;
&lt;li&gt;Communication: You can be friendly and engaging when communicating to senior, junior, and people outside your team.&lt;/li&gt;
&lt;li&gt;“Getting Things Done”: Balance between preparing for a project and implementing it. Ability to structure a problem and motivate people to accomplish goals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-behavioral-questions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Common Behavioral Questions&lt;/h1&gt;
&lt;div id=&#34;tell-me-a-little-about-your-background.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Tell me a little about your background.”&lt;/h2&gt;
&lt;p&gt;This question gives an opportunity to demonstrate clear communication, curiosity, enthusiasm, transferrable skills, and value adds. This is also an opportunity to align past experience with the current job opportunity.&lt;/p&gt;
&lt;p&gt;Why is it asked?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To assess presentation skills; clear, professional, charismatic, likable, impressive&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Goals of this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Impress them&lt;/li&gt;
&lt;li&gt;Highlight what you can contribute (stories related to what they want)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note – As you answer, they are imagining working with you.&lt;/p&gt;
&lt;p&gt;Why should you care?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First impressions are powerful. If you bore or annoy them, they may not be as focused throughout the rest of your interview.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Why they ask:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Want to know what you consider important in your background&lt;/li&gt;
&lt;li&gt;Want to see presentation skills&lt;/li&gt;
&lt;li&gt;Want to know what it will be like to work with you&lt;/li&gt;
&lt;li&gt;Large part of jobs is meetings and working on teams&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bonus: ‘personally’ … .&lt;/p&gt;
&lt;p&gt;Goal:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Current role, background, ambitions and fit&lt;/li&gt;
&lt;li&gt;Relevant experience, methodologies, and results&lt;/li&gt;
&lt;li&gt;Problem, curiosity, value, project&lt;/li&gt;
&lt;li&gt;key achievements, relevant to job, demonstrate qhy you are qualified.&lt;/li&gt;
&lt;li&gt;chrisma&lt;/li&gt;
&lt;li&gt;proof of performance&lt;/li&gt;
&lt;li&gt;Start a conversation&lt;/li&gt;
&lt;li&gt;Coherence&lt;/li&gt;
&lt;li&gt;Entertaining&lt;/li&gt;
&lt;li&gt;focused, concise, top selling points&lt;/li&gt;
&lt;li&gt;personality&lt;/li&gt;
&lt;li&gt;interest in oppertunity&lt;/li&gt;
&lt;li&gt;natural&lt;/li&gt;
&lt;li&gt;spontanious&lt;/li&gt;
&lt;li&gt;not stiff – ‘real you’&lt;/li&gt;
&lt;li&gt;comfort speaking&lt;/li&gt;
&lt;li&gt;confident, compelling&lt;/li&gt;
&lt;li&gt;enthusasiasm for the position&lt;/li&gt;
&lt;li&gt;show you’ll represent company professionally&lt;/li&gt;
&lt;li&gt;charismatic and likable&lt;/li&gt;
&lt;li&gt;speak smoothly&lt;/li&gt;
&lt;li&gt;nervous&lt;/li&gt;
&lt;li&gt;impressiveness / leadership&lt;/li&gt;
&lt;li&gt;communication, language&lt;/li&gt;
&lt;li&gt;tell a professional work story&lt;/li&gt;
&lt;li&gt;engaging&lt;/li&gt;
&lt;li&gt;compelling&lt;/li&gt;
&lt;li&gt;clear&lt;/li&gt;
&lt;li&gt;complete&lt;/li&gt;
&lt;li&gt;create connection&lt;/li&gt;
&lt;li&gt;create conversation&lt;/li&gt;
&lt;li&gt;relate to job description – issue: often times the job description is vague or unaligned with the job..&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Content:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Current role&lt;/li&gt;
&lt;li&gt;background&lt;/li&gt;
&lt;li&gt;ambition and goals&lt;/li&gt;
&lt;li&gt;relevant experience&lt;/li&gt;
&lt;li&gt;methods used / skills&lt;/li&gt;
&lt;li&gt;results&lt;/li&gt;
&lt;li&gt;why you’re qualified – impressive, interesting experience&lt;/li&gt;
&lt;li&gt;key accomplishments&lt;/li&gt;
&lt;li&gt;proof of performance (not summary of job duties, unique things you did)&lt;/li&gt;
&lt;li&gt;accomplishments, competencies&lt;/li&gt;
&lt;li&gt;what you want: this position really excites me&lt;/li&gt;
&lt;li&gt;powerpoint presentation with background and results&lt;/li&gt;
&lt;li&gt;company name&lt;/li&gt;
&lt;li&gt;title&lt;/li&gt;
&lt;li&gt;number of years&lt;/li&gt;
&lt;li&gt;job responsibilities&lt;/li&gt;
&lt;li&gt;mini work snapshots for every position&lt;/li&gt;
&lt;li&gt;achievement oriented&lt;/li&gt;
&lt;li&gt;accomplishment: anything that saved time, money, profitibility, quantifyable results&lt;/li&gt;
&lt;li&gt;what you know about the role.&lt;/li&gt;
&lt;li&gt;why you are right fit: “overall … I am confident I’d be able to succeed because…”&lt;/li&gt;
&lt;li&gt;self-aware&lt;/li&gt;
&lt;li&gt;transparent (trustworthy)&lt;/li&gt;
&lt;li&gt;potential leader&lt;/li&gt;
&lt;li&gt;tell your story, one that leads clearly to their offer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My Perspective:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is an interviewer thinking when they ask this?&lt;/li&gt;
&lt;li&gt;They are imagining themselves working with you. Do you do cool things? Do you do relevant things?&lt;/li&gt;
&lt;li&gt;The behavioral and technical questions help to get a complete picture of what it would be like to work with you.&lt;/li&gt;
&lt;li&gt;Will you make life easier or harder, … ?&lt;/li&gt;
&lt;li&gt;Instead of making it relevant to the particular job description, make it relevnat to qualities you’d like in a colleague, and mention that explicately: “I like to dive into new things and figure them out. So during my PhD I worked on a variety of projects … .”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open: I recently graduated from UC Irvine, where I studied statistics and cognitive science. I worked on a few lines of research, the one that was most interesting to me came out of an interest in prediction markets and arbitrage.&lt;/li&gt;
&lt;li&gt;Problem: So prediction markets are a platform where people can buy and sell contracts on events that pay a dollar if the event occurs and nothing if it doesn’t occur. So for example, one market might be Who will win the 2020 election, and contracts might be ‘Bernie Sanders’, ‘Andrew Yang’, ‘Donnald Trump’, … . Since the contracts pay out $1, the theory is that the price should estimate the underlying probability of the event occuring. And since the market is aggregating information across individuals, it should be a pretty good estimate. However, individuals have shown a number of biases in probabilistic reasoning in lab studies, so I was wondering if these same biases would exist in prediction market prices. If they did, then I could make profitable trading algorithms.&lt;/li&gt;
&lt;li&gt;So I partnered with a large online prediction market to get access to their data, and found evidence of some biases consistent with those observed in lab studies.&lt;/li&gt;
&lt;li&gt;Trajectory: Overall, I realized that I enjoy these kinds of impactful analyses of real-world data, moreso than theory building through laboratory studies, so I decided to bridge into a career in data science, and joined Insight to become part of a network of data scientists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example Answer;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Current role: “I’m currently a Fellow at Insight Data Science. During this, I worked on a consulting project Overall my client was really happy with my work.”&lt;/li&gt;
&lt;li&gt;Background: “Prior to Insight, I … . I hold a PhD in Cognitive Science from an experimental psychology lab, and a masters in statistics.”&lt;/li&gt;
&lt;li&gt;Ambitions:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Problem: Concepts and context&lt;/li&gt;
&lt;li&gt;Curiosity: Open question&lt;/li&gt;
&lt;li&gt;Value: Why should people care&lt;/li&gt;
&lt;li&gt;Project Outline: Methods, results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open: I recently graduated from UC Irvine, where I studied statistics and cognitive science. I worked on a few lines of research, the one that was most interesting to me came out of an interest in prediction markets and arbitrage.&lt;/li&gt;
&lt;li&gt;Problem: So prediction markets are a platform where people can buy and sell contracts on events that pay a dollar if the event occurs and nothing if it doesn’t occur. So for example, one market might be Who will win the 2020 election, and contracts might be ‘Bernie Sanders’, ‘Andrew Yang’, ‘Donnald Trump’, … . Since the contracts pay out $1, the theory is that the price should estimate the underlying probability of the event occuring. And since the market is aggregating information across individuals, it should be a pretty good estimate. However, individuals have shown a number of biases in probabilistic reasoning in lab studies, so I was wondering if these same biases would exist in prediction market prices. If they did, then I could make profitable trading algorithms.&lt;/li&gt;
&lt;li&gt;So I partnered with a large online prediction market to get access to their data, and found evidence of some biases consistent with those observed in lab studies.&lt;/li&gt;
&lt;li&gt;Trajectory: Overall, I realized that I enjoy these kinds of impactful analyses of real-world data, moreso than theory building through laboratory studies, so I decided to bridge into a career in data science, and joined Insight to become part of a network of data scientists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Communication: Clearly explain prediction market mechanics.&lt;/li&gt;
&lt;li&gt;Enthusasism: Interest in markets and complex problems.&lt;/li&gt;
&lt;li&gt;Transferrable skills: Mention some of the models used.&lt;/li&gt;
&lt;li&gt;Value adds: Realized prediction markets are biased close to expiration – their signal is weaker than previously thought. Also able to profit literally by rolling out trading algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;child-bandits&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Child bandits:&lt;/h3&gt;
&lt;p&gt;Bandit tasks are a common paradigm for studying individual level decision making and making claims about the way people make decisions. But in lots of contexts, the results you get when you addregate over trials can be different from the actual result on any trial. So I developed a model to infer on a trial by trial level, what decision making strategy kids were using to choose between two ‘bandits’ that gave out stickers. – I have an interest in using more customized models and Bayesian inference to overcome issues that arise from aggregating data at too high a level. If you had averaged over trials, then there may have been evidence for one or another model, but the dynamic changes in decision making strategies would have gone unnoticed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Similar approaches could be useful for dynamic experimental designs, OR in the context of Netflix, for personalization. If we could dynamically infer something like the strategy or emotional state of a user, then we could use that inference to personalize our recommended content or things like cover art or other aesthetic aspects of Netflix.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-me-about-the-project-you-worked-on-at-insight.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Tell me about the project you worked on at Insight.”&lt;/h2&gt;
&lt;p&gt;This question gives an opportunity to demonstrate what you’ll be capable of on Day 1.&lt;/p&gt;
&lt;p&gt;Template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open: At Insight I worked on a consulting project for an external company. This company acquired users by offering a free trial, after which users would subscribe.&lt;/li&gt;
&lt;li&gt;Purpose: Goals of the project were to to 1) identify, early on, who would become a subscriber, and 2) identify how the successful users were using the product.&lt;/li&gt;
&lt;li&gt;Approach: I usually think about a generative process. Here I framed the problem as a classification problem and thought about using LDA or QDA because I thought the distributions of features would be different based on the classes.&lt;/li&gt;
&lt;li&gt;Methods: Later thought about other algorithms because this was primarily a prediction problem, and because the assumption of normally distributed features was violated. So I tried tree-based algorithms and those performed the best across all metrics – F1 score, accuracy, precision, and recall.&lt;/li&gt;
&lt;li&gt;Challenges: Non-normal features (could break LDA, QDA)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How could I have used a mixed effects model here? (See Maime’s blog?)&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clarity:&lt;/li&gt;
&lt;li&gt;Relevance: Do the skills add value to the company?&lt;/li&gt;
&lt;li&gt;Logic: Were sensible solutions tried in a sensible order?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-are-you-interested-in-a-career-as-a-data-scientist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Why are you interested in a career as a data scientist?”&lt;/h2&gt;
&lt;p&gt;This is an opportunity to show that becoming a data sceientist at the company is the logical next career move, and to demonstrate that you know what to expect in the new role.&lt;/p&gt;
&lt;p&gt;Outline of topics that may resonate with a hiring manager:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Skills, Technical: Tools, technologies, methods you have used.&lt;/li&gt;
&lt;li&gt;Skills, Conceptual: Your approach to understanding problems and navigating to solutions&lt;/li&gt;
&lt;li&gt;Interests: What are you eager to do in this next phase of your professional life?&lt;/li&gt;
&lt;li&gt;Interests, Broad: Knowledge of problems tackled by data professionals in this industry&lt;/li&gt;
&lt;li&gt;Interests, Specific: Knowledge of technical challenges that will be faced by someone in this role&lt;/li&gt;
&lt;li&gt;Growth, Skills: What you’re excited to learn in the next few years&lt;/li&gt;
&lt;li&gt;Growth, Trajectory: Relate this job to an ultimate career trajectory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example answer: When I was in my PhD, I worked on a variety of projects and realized that I most enjoyed the ones that derived some impactful analysis from larger scale data – namely the prediction market project.&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Educated Enthusasiam:&lt;/li&gt;
&lt;li&gt;Professional Goals:&lt;/li&gt;
&lt;li&gt;Niche area:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-me-about-your-research-talk-to-me-about-your-last-role.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Tell me about your research / talk to me about your last role.”&lt;/h2&gt;
&lt;p&gt;Same as in ‘tell me about your background..’&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Skills:&lt;/li&gt;
&lt;li&gt;Relevance: Show how previous work relates to the responsibilities of the current role.&lt;/li&gt;
&lt;li&gt;Clarity:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-are-you-interested-in-joining-this-company&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Why are you interested in joining this company?”&lt;/h2&gt;
&lt;p&gt;This is an opportunity to demonstrate knowledge of and interest in the current role.&lt;/p&gt;
&lt;p&gt;Depends on deep-dive.&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Industry knowledge:&lt;/li&gt;
&lt;li&gt;Company knowledge:&lt;/li&gt;
&lt;li&gt;Potential for impact:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;talk-about-a-time-you-were-on-a-team-that-struggled-with-effective-communication.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Talk about a time you were on a team that struggled with effective communication.”&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Situation: Where / when did this happen? What were you working on, and with whom?&lt;/li&gt;
&lt;li&gt;Task: What were you responsible for? What challenges did you face?&lt;/li&gt;
&lt;li&gt;Action: How did you decide the appropriate response to the situation? How did you implement it?&lt;/li&gt;
&lt;li&gt;Result: How do you know your solution was successful?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Takeaways: What did you learn from this situation? What would you do the same or differently next time?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Can’t think of one.. In community college I worked as a TA for my mentor, and I lead a research project which is very rare in community college settings. She was extremely supportive and I think communication and goals were clear across the team – I would make outlines / agendas for team meetings.&lt;/li&gt;
&lt;li&gt;At UC Davis, for my Bachelors, my mentor was really bad with email. But he was nice, and his office door was always open, so whenever I needed something I would stop by and chat.&lt;/li&gt;
&lt;li&gt;At Irvine people were always available via email or even phone if needed.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also I had taken a consulting class as part of the stats masters, and a portion of that emphasized the importance of clear communication with clients, so in consulting relationships I maintain clear goals and deliverables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowledge of company values around communication:&lt;/li&gt;
&lt;li&gt;Your principles for communication:&lt;/li&gt;
&lt;li&gt;Awareness of problem:&lt;/li&gt;
&lt;li&gt;Initiative:&lt;/li&gt;
&lt;li&gt;Lessons Learned:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-me-about-a-time-you-made-a-mistake-at-work.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Tell me about a time you made a mistake at work.”&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Situation: Where/when did this happen? What were you working on, and with whom?&lt;/li&gt;
&lt;li&gt;Task: What were you responsible for? What challenges did you face?&lt;/li&gt;
&lt;li&gt;Action: How did you decide the appropriate response to the situation? How did you implement it?&lt;/li&gt;
&lt;li&gt;Result: How do you know your solution was successful?&lt;/li&gt;
&lt;li&gt;Takeaways: What did you learn from this situation? What would you do the same or differently next time?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Forgot to add code to save a questionnaire at the end of an experiment. From that point on I ran myself and analyzed my data before running any real subjects – so kind of like I learned the importance of unit tests from that experience.&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your principles for mistakes:&lt;/li&gt;
&lt;li&gt;Knowledge of company values around rapid iteration, learning through failure, and troubleshooting&lt;/li&gt;
&lt;li&gt;Judgment: How did you explain the mistake (misunderstanding, incomplete information, technical errors)? Was it due to carelessness.&lt;/li&gt;
&lt;li&gt;Responsibility: Did you make excuses, or make solutions?&lt;/li&gt;
&lt;li&gt;Wisdom: Are you likely to avoid a similar mistake in the future? Can lessons here apply more broadly?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;talk-about-a-situation-that-required-you-to-explain-your-technical-work-to-someone-outside-your-industry-or-field.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Talk about a situation that required you to explain your technical work to someone outside your industry or field.”&lt;/h2&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ability to communicate with stakeholders:&lt;/li&gt;
&lt;li&gt;Impact: Understanding of what the other party needed to know&lt;/li&gt;
&lt;li&gt;Adaptability: Techniques (analogy, visualization, …) used to simply communicate ideas. How did you decide what to include. Did you clarify that the explanation made sense.&lt;/li&gt;
&lt;li&gt;Attitude: Will you patiently explain your ideas?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;talk-about-a-time-you-were-asked-to-manage-an-important-project.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Talk about a time you were asked to manage an important project.”&lt;/h2&gt;
&lt;p&gt;PredictIt proejct, since that’s one that I really drove.&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowledge of company values on leadership&lt;/li&gt;
&lt;li&gt;Trust: Why were you chosen to lead?&lt;/li&gt;
&lt;li&gt;Respect and responsibility:&lt;/li&gt;
&lt;li&gt;Delegation:&lt;/li&gt;
&lt;li&gt;People management:&lt;/li&gt;
&lt;li&gt;Executive decision-making:&lt;/li&gt;
&lt;li&gt;Team dynamics:&lt;/li&gt;
&lt;li&gt;Potential: Strengths and weaknesses in leading.&lt;/li&gt;
&lt;li&gt;Interest in management roles:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;briefcase-document&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Briefcase Document&lt;/h1&gt;
&lt;p&gt;This idea also came from &lt;a href=&#34;https://towardsdatascience.com/3-strategies-to-guarantee-a-data-science-job-with-no-experience-68d85b345f21&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Template:&lt;/p&gt;
&lt;p&gt;[COMPANY NAME] Data Science Action Plan&lt;/p&gt;
&lt;p&gt;Goal:&lt;/p&gt;
&lt;p&gt;Pain-Points (Speculated):&lt;/p&gt;
&lt;p&gt;Project #1:&lt;/p&gt;
&lt;p&gt;Desired Outcome&lt;/p&gt;
&lt;p&gt;Pain Points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pp1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Proposed Experiment&lt;/p&gt;
&lt;p&gt;Resource Requirements:&lt;/p&gt;
&lt;p&gt;Add 3 projects as part of a deep dive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;thank-you-email&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Thank-You Email&lt;/h1&gt;
&lt;div id=&#34;linkedin-thank-you-email&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LinkedIn Thank-You Email&lt;/h3&gt;
&lt;div id=&#34;recruiters&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Recruiters&lt;/h4&gt;
&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Everyone was nice and seemed to be working on interesting things!&lt;/p&gt;
&lt;p&gt;Best, Mac&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-interviewer&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;First Interviewer&lt;/h4&gt;
&lt;p&gt;Hi Joonhyung,&lt;/p&gt;
&lt;p&gt;Have you heard of &lt;a href=&#34;https://www.rimeto.com/#profiles&#34;&gt;Rimeto&lt;/a&gt;? It sounds very similar to the org chart product that you described during my interview on Thursday.&lt;/p&gt;
&lt;p&gt;I’m really excited about work across multiple teams, so I could imagine myself using such a product to meet other employees who are working on interesting projects.&lt;/p&gt;
&lt;p&gt;Best, Mac&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gigi-zhang&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Gigi Zhang&lt;/h4&gt;
&lt;p&gt;Hi Gigi,&lt;/p&gt;
&lt;p&gt;Best, Mac&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caitlyn&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Caitlyn&lt;/h4&gt;
&lt;p&gt;Hi Caitlin,&lt;/p&gt;
&lt;p&gt;I hope you had a nice weekend. I was excited to hear a little about your work on Thursday, and wish we had more time for discussion.&lt;/p&gt;
&lt;p&gt;I came across some exciting papers by Jian Wang when she was at LinkedIn on what seems to be relevant work.&lt;/p&gt;
&lt;p&gt;Has your group published anything that I can follow up with?&lt;/p&gt;
&lt;p&gt;All the best, Mac&lt;/p&gt;
&lt;p&gt;Hi Caitlin,&lt;/p&gt;
&lt;p&gt;I thought the work on your team sounded interesting, and I would have liked to hear more about it if there had been time for discussion.&lt;/p&gt;
&lt;p&gt;I came across some papers by Jian Wang when she was at LinkedIn on what seems to be relevant work.&lt;/p&gt;
&lt;p&gt;Has your group published anything that I can follow up with?&lt;/p&gt;
&lt;p&gt;Best, Mac&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;work focus sounded interesting&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.www2015.it/documents/proceedings/proceedings/p1209.pdf&#34;&gt;one paper; User Latent Preference Model for Better Downside Management in Recommender Systems&lt;/a&gt; is conceptually close to the approach I was trying to take for the email question.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;yelp-thank-you-email&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Yelp Thank-You Email&lt;/h3&gt;
&lt;p&gt;Hi (),&lt;/p&gt;
&lt;p&gt;Thank you for the interview prep information, and for helping to move the process along quickly!&lt;/p&gt;
&lt;p&gt;I had a great time meeting the team on Friday! I also enjoyed thinking through the typical challenges that they seem to work on at Yelp.&lt;/p&gt;
&lt;p&gt;Let me know if you need anything from me, and I look forward to hearing back about potential next steps later this week.&lt;/p&gt;
&lt;p&gt;Thank you, Mac&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cold-recruiter-email&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cold Recruiter Email&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/3-strategies-to-guarantee-a-data-science-job-with-no-experience-68d85b345f21&#34;&gt;Blog on reaching out here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;“Hey (Recruiter),&lt;/p&gt;
&lt;p&gt;I know you’re really busy and you get a lot of emails. Found (Company) through angellist. Already applied for the Data Scientist position but wanted to reach out directly! Promise this email will be &amp;lt; 60 seconds.&lt;/p&gt;
&lt;p&gt;I’m Jeff and I’m a Data Scientist. A few things about me, I built the Data Scientist in Python &amp;amp; R track for dataquest.io, used by hundreds of thousands of students and by companies such as Airbnb, Spotify, Uber etc so I excel in communicating technically complex topics to nontechnical audiences. On the technical side, I built “The Booki Monster” which is a monster that eats books and summarizes them using LDA &amp;amp; Doc2Vec. App here(it takes a sec to load, so just be patient!) and explanation here. I also built the “Date-a Miner”, a convolutional neural network that automates online dating, by predicting the likelihood of me liking a profile and swiping for me.&lt;/p&gt;
&lt;p&gt;In addition, I’m obsessed with figuring out how to learn things fast. As a result, I’m also working on an ambitious side-project called Month to Master where I attempt to master one difficult skill each month. Skills such as going from zero to conversational in Spanish in one month, memorizing the order of a 52 deck of cards in 3 minutes, completing 27 consecutive pull-ups etc.&lt;/p&gt;
&lt;p&gt;Thanks again! I totally understand if you’re busy so no rush on reply. If you feel like there is a possible fit on your end, would love to further this conversation!&lt;/p&gt;
&lt;p&gt;Hope you have a great week! Attached resume.&lt;/p&gt;
&lt;p&gt;Best, Mac&amp;quot;&lt;/p&gt;
&lt;div id=&#34;email-to-robinhood-recruiter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Email To RobinHood recruiter&lt;/h3&gt;
&lt;p&gt;Hey Chinsin,&lt;/p&gt;
&lt;p&gt;I know you’re really busy and you get a lot of emails. I found the Data Scientist role at Robinhood through LinkedIn and already applied, but wanted to reach out directly! Promise this email will be &amp;lt; 60 seconds.&lt;/p&gt;
&lt;p&gt;I’m Mac and I’m a Data Science Fellow at Insight. I actively trade on Robinhood and am avidly interested in democratizing access and informaiton around trading. On the technical side, I recently graduated from a PhD program in Cognitive Science and a Masters program in Statistics, where I worked on many projects related to decision making and latent decision making strategies.&lt;/p&gt;
&lt;p&gt;Thanks again! If you feel like there is a possible fit on your end, I would love to further this conversation!&lt;/p&gt;
&lt;p&gt;Hope you have a great week! And feel free to reach out if you have any questions about my experience or interests.&lt;/p&gt;
&lt;p&gt;Best, Mac&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Product Sense</title>
      <link>/MacStrelioff/insightstudying/business-sense/</link>
      <pubDate>Tue, 20 Aug 2019 12:30:10 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/business-sense/</guid>
      <description>


&lt;div id=&#34;youtube-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;YouTube Example&lt;/h1&gt;
&lt;p&gt;The YouTube recommendation algorithm favors videos with high watchtime.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://creatoracademy.youtube.com/page/lesson/discovery?cid=get-discovered&amp;amp;hl=en&#34;&gt;video&lt;/a&gt; on the algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;titles&lt;/li&gt;
&lt;li&gt;thumbnails&lt;/li&gt;
&lt;li&gt;descriptions&lt;/li&gt;
&lt;li&gt;how other viewers seem to enjoy it&lt;/li&gt;
&lt;li&gt;likes/dislikes&lt;/li&gt;
&lt;li&gt;comments&lt;/li&gt;
&lt;li&gt;view duration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=gTrLniP5tSQ&amp;amp;list=PLpjK416fmKwQK6_REczCaBQ1x1jyHvZAF&#34;&gt;series&lt;/a&gt; on how search and discovery works.&lt;/p&gt;
&lt;p&gt;Search:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;videos ranked match between query and title, description&lt;/li&gt;
&lt;li&gt;tip: can make videos on trending content.&lt;/li&gt;
&lt;li&gt;tip: can use google to find common search terms.&lt;/li&gt;
&lt;li&gt;engagement and watch time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suggested videos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can be from any channel&lt;/li&gt;
&lt;li&gt;suggest viewers watch another video&lt;/li&gt;
&lt;li&gt;playlists, links, cards, boxes, screens to suggest your other videos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Home screen:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;performance of video&lt;/li&gt;
&lt;li&gt;personalization, video’s match for a viewer’s previous videos watched, and which videos they’ve already seen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Trending:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;specific to viewer’s country&lt;/li&gt;
&lt;li&gt;combines popularity with novelty&lt;/li&gt;
&lt;li&gt;considers rate of growth in views&lt;/li&gt;
&lt;li&gt;includes creater on the rise&lt;/li&gt;
&lt;li&gt;favors sharable videos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Subscribers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;biggest fans&lt;/li&gt;
&lt;li&gt;subscriptions tab shows videos from channels that a viewer is subscribed to&lt;/li&gt;
&lt;li&gt;subscription content also appears in home screen&lt;/li&gt;
&lt;li&gt;can encourage subscriptions by asking users to subscribe, and convaying the value to them&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notifications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sent to subscribers when a new upload is made public&lt;/li&gt;
&lt;li&gt;stochastic, so it helps to jumpstart viewership but isn’t the best in the long run&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-types-and-strategies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Problem Types and Strategies&lt;/h1&gt;
&lt;div id=&#34;new-product&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New Product&lt;/h2&gt;
&lt;p&gt;Overall product sense is about what to build and why.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Overarching goals / stage of the company&lt;/li&gt;
&lt;li&gt;Contextualize: Draw a user funnel or user flow for the specific product&lt;/li&gt;
&lt;li&gt;Draw a dashboard, then populate with platform and product metrics&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;overall-resources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overall resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLOhHNjZItNnM5r2JCX9KzrFH5WYWKrpAA&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/playlist?list=PLOhHNjZItNnM5r2JCX9KzrFH5WYWKrpAA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-dives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Deep-Dives&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/insightdatascience.com/interviewstrategies/interviews/interview-logistics/deep-dives?authuser=0&#34;&gt;Deep Dive guide&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;platform-concepts-and-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Platform Concepts and Metrics&lt;/h1&gt;
&lt;div id=&#34;users&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Users&lt;/h2&gt;
&lt;div id=&#34;user-funnel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;User Funnel&lt;/h3&gt;
&lt;p&gt;Churn,&lt;/p&gt;
&lt;p&gt;funnel analysis&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;user-flow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;User Flow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traffic source: Where users come from (search, referral, … )&lt;/li&gt;
&lt;li&gt;number new visits, percent new visits&lt;/li&gt;
&lt;li&gt;Page visits: users arriving at page&lt;/li&gt;
&lt;li&gt;Bounce rate: the percentage of visitors to a particular website who navigate away from the site after viewing only one page.&lt;/li&gt;
&lt;li&gt;Pages viewed / visit&lt;/li&gt;
&lt;li&gt;Time on pages&lt;/li&gt;
&lt;li&gt;Time on site&lt;/li&gt;
&lt;li&gt;Conversion (purchase, reservation, end-goal action)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;lifetime-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lifetime Value&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kpis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;KPIs&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mixpanel.com/topics/important-user-engagement-metrics-apps/&#34;&gt;Engagement Metrics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Active user metrics are especially important for monetization with ad revenue.&lt;/p&gt;
&lt;p&gt;Daily active users: &lt;span class=&#34;math display&#34;&gt;\[
DAU = n_{users,day}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Average daily active users: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
ADAU &amp;amp;= \frac{\text{total number of active users in a month}}{\text{days in the average month}}\\
&amp;amp;= \frac{\sum_{days}^{n_{days}} DAU_{day}}{n_{days}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Monthly active users: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
MAU &amp;amp;= \frac{\text{total number of active users in a year}}{12} \\
&amp;amp;= \frac{\sum_{days}^{365} DAU_{day}}{n_{months}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Stickiness: Here the numerator is a measure of activity within a month, and the denominator is a measure of activity within a year. &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S &amp;amp;= \frac{ADAU}{MAU} \\ 
  &amp;amp;= \frac{\frac{1}{n_{days}}\sum_{days}^{n_{days}} DAU_{day}}{\frac{1}{n_{months}}\sum_{days}^{365}DAU_{day}}\\
  &amp;amp;= \frac{n_{months}\sum_{days}^{n_{days}}DAU_{day}}{n_{days}\sum_{days}^{365}DAU_{day}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.popcornmetrics.com/5-user-engagement-metrics-for-growth/&#34;&gt;alternatively&lt;/a&gt;, stickiness could be measured as &lt;span class=&#34;math inline&#34;&gt;\(DAU\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(MAU\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Daily active users: number of users in a day&lt;/li&gt;
&lt;li&gt;Active users: Average Daily Active Users = monthly users / days in average month&lt;/li&gt;
&lt;li&gt;Active users: Monthly Active Users = users in a year / months in year&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stickiness: Average Daily Active Users / Monthly Active Users =&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Support ticiets: new tickets + unresolved tickets&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Time to ticket resolution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.klipfolio.com/resources/kpi-examples&#34; class=&#34;uri&#34;&gt;https://www.klipfolio.com/resources/kpi-examples&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;monetization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monetization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;customer lifetime value (LTV)&lt;/li&gt;
&lt;li&gt;monthly recurring revenue (MRR)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;product-concepts-and-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Product Concepts and Metrics&lt;/h1&gt;
&lt;div id=&#34;demand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Demand&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;engagement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Engagement&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;clicks&lt;/li&gt;
&lt;li&gt;bounce rate on subsequent page – used combined with clicks to mitigate click bait&lt;/li&gt;
&lt;li&gt;Shares&lt;/li&gt;
&lt;li&gt;Upvotes&lt;/li&gt;
&lt;li&gt;DAU&lt;/li&gt;
&lt;li&gt;time of sessions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Value&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Shares&lt;/li&gt;
&lt;li&gt;Upvotes&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;add-from-meeting-with-alejandro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Add from meeting with Alejandro&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;User data (ways users use the app)&lt;/li&gt;
&lt;li&gt;Usage data (ways the app interacts with devices, networks, servers, product versions, …)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;old-notes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;OLD NOTES:&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;insight-topics-list&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Insight topics list:&lt;/h1&gt;
&lt;p&gt;engagement, search engine optimization (SEO), key-performance indicators (KPIs),&lt;/p&gt;
&lt;p&gt;Experiment design, A/A and A/B testing, power analysis;&lt;/p&gt;
&lt;p&gt;ML focused business case study (features, algorithms, validation, value), business focused data challenges&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;template&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Template&lt;/h1&gt;
&lt;div id=&#34;value-of-a-new-feature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Value of a new feature&lt;/h2&gt;
&lt;p&gt;e.g. higher engagement, better labels for training data or other user behaivors&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deamand-for-a-feature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deamand for a feature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Surveys&lt;/li&gt;
&lt;li&gt;Focus Groups&lt;/li&gt;
&lt;li&gt;UX interviews / shadows&lt;/li&gt;
&lt;li&gt;Retrospective analysis (looking at past user behavior)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;treatment-design&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Treatment Design&lt;/h2&gt;
&lt;p&gt;How exactly will this be implemented in the treatment group?&lt;/p&gt;
&lt;p&gt;Specify treatment and control groups.&lt;/p&gt;
&lt;div id=&#34;issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Networks: Users that interact might be in different conditions&lt;/li&gt;
&lt;li&gt;Learning effects: Users might respond differently initially than they do asymptotically. Can run experiments longer until the treatment effect seems to stabalize.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;constructs-and-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Constructs and Metrics&lt;/h2&gt;
&lt;p&gt;Engagement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lift&lt;/li&gt;
&lt;li&gt;CTR&lt;/li&gt;
&lt;li&gt;Duration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Monetization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CTR&lt;/li&gt;
&lt;li&gt;revenue per user&lt;/li&gt;
&lt;li&gt;add revenue&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Guard Rail Metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;metric-evaluation-and-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metric evaluation and selection&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What if we optimize for … ?: The choice of metrics determines what the platform is being optimized for, so think about what will happen if the platform is optimized for the chosen metric.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Method&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Size&lt;/h3&gt;
&lt;p&gt;Power analysis&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;duration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Duration&lt;/h3&gt;
&lt;p&gt;Bandits versus A/B tests,&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-considerations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other considerations:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Heterogenous treatment effects&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-question-formats&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example Question Formats&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;mock-questions-and-templates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mock questions and templates&lt;/h1&gt;
&lt;div id=&#34;monitoring-twitter-serach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monitoring Twitter serach&lt;/h2&gt;
&lt;p&gt;Metrics:&lt;/p&gt;
&lt;p&gt;Measure health:&lt;/p&gt;
&lt;p&gt;Working well + being used …&lt;/p&gt;
&lt;p&gt;Track number of users using search!&lt;/p&gt;
&lt;p&gt;Consider people who have used the search once.. (can assess need)&lt;/p&gt;
&lt;p&gt;Use of search&lt;/p&gt;
&lt;p&gt;# users that use search&lt;/p&gt;
&lt;p&gt;Usage (prop of users that interact with search)&lt;/p&gt;
&lt;p&gt;# users using search / # active users&lt;/p&gt;
&lt;p&gt;Proportion of times a user comes and uses search&lt;/p&gt;
&lt;p&gt;Time interval(s):&lt;/p&gt;
&lt;p&gt;Daily&lt;/p&gt;
&lt;p&gt;Weekly&lt;/p&gt;
&lt;p&gt;# number of search sessions&lt;/p&gt;
&lt;p&gt;Total search volume (server traffic, … – important!)&lt;/p&gt;
&lt;p&gt;Quality of search&lt;/p&gt;
&lt;p&gt;Chose any suggestion&lt;/p&gt;
&lt;p&gt;Rank of suggestion that they chose&lt;/p&gt;
&lt;p&gt;Look at total number of searches, and searches in different categories / segmentations&lt;/p&gt;
&lt;p&gt;Depends on infrastructure / ease of cutting later&lt;/p&gt;
&lt;p&gt;– favorites, retweets, … Other metrics from a deep-dive.&lt;/p&gt;
&lt;p&gt;CUTS IN DATA&lt;/p&gt;
&lt;p&gt;Think of groups that might be using feature differently&lt;/p&gt;
&lt;p&gt;New vs old users&lt;/p&gt;
&lt;p&gt;Device type (iphone vs android)&lt;/p&gt;
&lt;p&gt;Browser&lt;/p&gt;
&lt;p&gt;Conditions where the feature may be behaving differently (languages, countries, …) ..&lt;/p&gt;
&lt;p&gt;Note: countries would have different bandwidths (country easier to measure than bandiwdth itself)&lt;/p&gt;
&lt;p&gt;Language that the search is using.&lt;/p&gt;
&lt;p&gt;SPIKE IN THE METRIC # of searches&lt;/p&gt;
&lt;p&gt;Bug&lt;/p&gt;
&lt;p&gt;Event&lt;/p&gt;
&lt;p&gt;Think of more people on platform moving with searches…&lt;/p&gt;
&lt;p&gt;# clicks and # searches – if it’s a bug, then clicks prob wouldn’t also spike.&lt;/p&gt;
&lt;p&gt;Impressions, clicks, …&lt;/p&gt;
&lt;p&gt;Top of funnel,&lt;/p&gt;
&lt;p&gt;Logging in, going to search, writing stuff, hitting search .&lt;/p&gt;
&lt;p&gt;‘Ok lets write down some ideas and see how we feel about them … ‘&lt;/p&gt;
&lt;p&gt;Looking for intuition about how things will move and interact?&lt;/p&gt;
&lt;p&gt;Looking for:&lt;/p&gt;
&lt;p&gt;Organized in thinking&lt;/p&gt;
&lt;p&gt;Incl understanding of problem space&lt;/p&gt;
&lt;p&gt;Communicating thinking well&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://datamasked.com/&#34;&gt;Giulio Palombo’s collection of business focused data challenges&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Company deep dives; data+schema brainstorming; ML algorithms; Insights;&lt;/p&gt;
&lt;p&gt;Company blogs are a great resource: check out how StitchFix, Instacart use data science.&lt;/p&gt;
&lt;p&gt;More company deep dives!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Twitter, Poisson Processes, and Conjugacy</title>
      <link>/MacStrelioff/data-science/twitter-poisson-processes-and-conjugacy/</link>
      <pubDate>Sat, 17 Aug 2019 19:56:03 +0000</pubDate>
      
      <guid>/MacStrelioff/data-science/twitter-poisson-processes-and-conjugacy/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background-and-setup&#34;&gt;Background and Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#twitter-data&#34;&gt;Twitter Data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#pulling-data-from-twitters-api&#34;&gt;Pulling Data from Twitter’s API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#working-with-twitter-status-objects&#34;&gt;Working with Twitter Status objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#embed-a-status&#34;&gt;Embed a Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convert-a-status-to-a-dict&#34;&gt;Convert a Status to a dict&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#access-status-attributes-how-i-accessed-the-data-used-below&#34;&gt;Access Status Attributes (How I accessed the data used below)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#poisson-process&#34;&gt;Poisson Process&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions&#34;&gt;Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#specification-and-properties&#34;&gt;Specification and Properties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-the-homogeneity-assumption&#34;&gt;Checking the Homogeneity Assumption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-the-exponential-distribution-of-intervals&#34;&gt;Checking the exponential distribution of intervals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-of-tweet-fequency-using-conjugacy&#34;&gt;Model of Tweet Fequency Using Conjugacy&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#inference-on-tweet-rate-lambda-over-time&#34;&gt;Inference On Tweet Rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; Over Time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predicting-number-of-tweets-in-interval-s&#34;&gt;Predicting Number Of Tweets In Interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;background-and-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background and Setup&lt;/h2&gt;
&lt;p&gt;In this notebook I focus on explaining Poisson processes and conjugacy applied to my Twitter activity.&lt;/p&gt;
&lt;p&gt;To get started, I followed directions from three main sources that walked through the &lt;code&gt;twitter&lt;/code&gt; and &lt;code&gt;python-twitter&lt;/code&gt; libraries, and described how to apply for Twitter API access and use the keys;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;python-twitter&lt;/code&gt;: Blogpost &lt;a href=&#34;https://medium.com/@YashSharma8388/collecting-data-from-twitter-using-python-twitter-library-and-twitter-api-42376c68d910&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tweepy&lt;/code&gt;: Blogpost &lt;a href=&#34;https://medium.com/@ssola/playing-with-twitter-streaming-api-b1f8912e50b0&#34;&gt;here&lt;/a&gt; and docs &lt;a href=&#34;http://docs.tweepy.org/en/v3.4.0/api.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Additional information on obtaining API keys and authenticating Twitter connections in a blogpost &lt;a href=&#34;https://medium.com/@fbilesanmi/how-to-login-with-twitter-api-using-python-6c9a0f7165c5&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I include my code to import the required libraries and set up API access keys, though the data used here were pulled and saved before writing the notebook. The main focus is on understanding Poisson processes and adaptive modeling of such processes using conjucacy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;reticulate&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Setup
# for working with timestamps
import pandas as pd
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
# for basic math
import numpy as np
# for plotting
import matplotlib.pyplot as plt
import seaborn as sns

# for working with distributions
from scipy.stats import expon,gamma,poisson,nbinom

# for general web data pulling
import requests

# for pulling tweets from Twitter API
import twitter

# keys for twitter API
# (removed for this public document)
api = twitter.Api(consumer_key=&amp;#39;&amp;#39;,
              consumer_secret=&amp;#39;&amp;#39;,
              access_token_key=&amp;#39;&amp;#39;,
              access_token_secret=&amp;#39;&amp;#39;)

# for saving and loading Python objects like dicts
import pickle

def save_obj(obj, name):
    with open(name + &amp;#39;.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name):
    with open(name + &amp;#39;.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        return pickle.load(f)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;twitter-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Twitter Data&lt;/h2&gt;
&lt;div id=&#34;pulling-data-from-twitters-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pulling Data from Twitter’s API&lt;/h3&gt;
&lt;p&gt;First I pulled data using the code below. For this code to work, your API keys will need to be specified in the setup above. To conceil my keys, I ran the commented code below earlier and saved the timeline object. The uncommented code loads my timeline and looks at the first element. The timeline is represented as a list of Status objects like the one output by the code below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# # Twitter handel to pull data from
# handle = &amp;#39;@macstrelioff&amp;#39;
# 
# # get timeline
# timeline=api.GetUserTimeline(screen_name = handle, 
#                     count=200, # 200 is maximum 
#                     include_rts=True, 
#                     trim_user=True, 
#                     exclude_replies=False)
# 
# # save timeline object
# save_obj(timeline,&amp;#39;timeline_macstrelioff_20190406&amp;#39;)

# load timeline object
timeline=load_obj(&amp;#39;timeline_macstrelioff_20190406&amp;#39;)
timeline[0] # most recent tweet status object&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Status(ID=1113860296458756097, ScreenName=None, Created=Thu Apr 04 17:46:03 +0000 2019, Text=&amp;quot;@vboykis df.dropna(how=&amp;#39;brute force&amp;#39;) https://t.co/QOULUc5a0u&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-twitter-status-objects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Working with Twitter Status objects&lt;/h3&gt;
&lt;p&gt;I cover three ways to work with Status objects. 1. display the Status as a tweet! 2. Convert the Status to a dictionary and access values from keys 3. Access values directly as attributes of the Status&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;embed-a-status&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Embed a Status&lt;/h3&gt;
&lt;p&gt;First, many the Status attributes (&lt;code&gt;created_at&lt;/code&gt;, &lt;code&gt;favorite_count&lt;/code&gt;, &lt;code&gt;text&lt;/code&gt;, …) can be cleanly displayed as a tweet embedded in a notebook. Below I create function that takes a username and tweet ID then, using Twitter’s embedding API, displayes the tweet as it would be seen on Twitter. (Note: this will only work properly if the Python kernel is trusted)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# for displaying tweets based on username and tweet_id
class disp_tweet(object):
    def __init__(self, user_name, tweet_id):
        # see: https://dev.twitter.com/web/embedded-tweets
        api = &amp;#39;https://publish.twitter.com/oembed?url=https://twitter.com/&amp;#39;+ \
               user_name + &amp;#39;/status/&amp;#39; + tweet_id
        response  = requests.get(api)
        self.text = response.json()[&amp;quot;html&amp;quot;]

    def _repr_html_(self):
        return self.text&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;disp_tweet(user_name=&amp;#39;macstrelioff&amp;#39;,tweet_id=&amp;#39;981338927419109376&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; &amp;lt;__main__.disp_tweet object at 0x1a27c4bb00&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If run from a Jupyter notebook, this should embed a tweet as below;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
My desk is covered in random papers. It is the support of a stationery distribution.
&lt;/p&gt;
— mac strelioff (&lt;span class=&#34;citation&#34;&gt;@macstrelioff&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/macstrelioff/status/981338927419109376?ref_src=twsrc%5Etfw&#34;&gt;April 4, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;convert-a-status-to-a-dict&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Convert a Status to a dict&lt;/h3&gt;
&lt;p&gt;Status objects have a bound method, &lt;code&gt;.AsDict()&lt;/code&gt;, that will convert them to a Python dictionary. This way the structure of the information is easily seen. In the code below, I convert the first status to a dictionary and output it contents.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(timeline[0].AsDict())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; {&amp;#39;created_at&amp;#39;: &amp;#39;Thu Apr 04 17:46:03 +0000 2019&amp;#39;, &amp;#39;favorite_count&amp;#39;: 20, &amp;#39;hashtags&amp;#39;: [], &amp;#39;id&amp;#39;: 1113860296458756097, &amp;#39;id_str&amp;#39;: &amp;#39;1113860296458756097&amp;#39;, &amp;#39;in_reply_to_screen_name&amp;#39;: &amp;#39;vboykis&amp;#39;, &amp;#39;in_reply_to_status_id&amp;#39;: 1113822568211996672, &amp;#39;in_reply_to_user_id&amp;#39;: 19304217, &amp;#39;lang&amp;#39;: &amp;#39;da&amp;#39;, &amp;#39;media&amp;#39;: [{&amp;#39;display_url&amp;#39;: &amp;#39;pic.twitter.com/QOULUc5a0u&amp;#39;, &amp;#39;expanded_url&amp;#39;: &amp;#39;https://twitter.com/macstrelioff/status/1113860296458756097/photo/1&amp;#39;, &amp;#39;id&amp;#39;: 1113860285566046210, &amp;#39;media_url&amp;#39;: &amp;#39;http://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg&amp;#39;, &amp;#39;media_url_https&amp;#39;: &amp;#39;https://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg&amp;#39;, &amp;#39;sizes&amp;#39;: {&amp;#39;thumb&amp;#39;: {&amp;#39;w&amp;#39;: 150, &amp;#39;h&amp;#39;: 150, &amp;#39;resize&amp;#39;: &amp;#39;crop&amp;#39;}, &amp;#39;large&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;}, &amp;#39;medium&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;}, &amp;#39;small&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;}}, &amp;#39;type&amp;#39;: &amp;#39;animated_gif&amp;#39;, &amp;#39;url&amp;#39;: &amp;#39;https://t.co/QOULUc5a0u&amp;#39;, &amp;#39;video_info&amp;#39;: {&amp;#39;aspect_ratio&amp;#39;: [125, 99], &amp;#39;variants&amp;#39;: [{&amp;#39;bitrate&amp;#39;: 0, &amp;#39;content_type&amp;#39;: &amp;#39;video/mp4&amp;#39;, &amp;#39;url&amp;#39;: &amp;#39;https://video.twimg.com/tweet_video/D3U6BzqUUAIwYEC.mp4&amp;#39;}]}}], &amp;#39;retweet_count&amp;#39;: 2, &amp;#39;source&amp;#39;: &amp;#39;&amp;lt;a href=&amp;quot;http://twitter.com/download/android&amp;quot; rel=&amp;quot;nofollow&amp;quot;&amp;gt;Twitter for Android&amp;lt;/a&amp;gt;&amp;#39;, &amp;#39;text&amp;#39;: &amp;quot;@vboykis df.dropna(how=&amp;#39;brute force&amp;#39;) https://t.co/QOULUc5a0u&amp;quot;, &amp;#39;urls&amp;#39;: [], &amp;#39;user&amp;#39;: {&amp;#39;id&amp;#39;: 70255183, &amp;#39;id_str&amp;#39;: &amp;#39;70255183&amp;#39;}, &amp;#39;user_mentions&amp;#39;: [{&amp;#39;id&amp;#39;: 19304217, &amp;#39;id_str&amp;#39;: &amp;#39;19304217&amp;#39;, &amp;#39;name&amp;#39;: &amp;#39;Vicki Boykis&amp;#39;, &amp;#39;screen_name&amp;#39;: &amp;#39;vboykis&amp;#39;}]}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;created_at&amp;#39;: &amp;#39;Thu Apr 04 17:46:03 +0000 2019&amp;#39;,
 &amp;#39;favorite_count&amp;#39;: 20,
 &amp;#39;hashtags&amp;#39;: [],
 &amp;#39;id&amp;#39;: 1113860296458756097,
 &amp;#39;id_str&amp;#39;: &amp;#39;1113860296458756097&amp;#39;,
 &amp;#39;in_reply_to_screen_name&amp;#39;: &amp;#39;vboykis&amp;#39;,
 &amp;#39;in_reply_to_status_id&amp;#39;: 1113822568211996672,
 &amp;#39;in_reply_to_user_id&amp;#39;: 19304217,
 &amp;#39;lang&amp;#39;: &amp;#39;da&amp;#39;,
 &amp;#39;media&amp;#39;: [{&amp;#39;display_url&amp;#39;: &amp;#39;pic.twitter.com/QOULUc5a0u&amp;#39;,
   &amp;#39;expanded_url&amp;#39;: &amp;#39;https://twitter.com/macstrelioff/status/1113860296458756097/photo/1&amp;#39;,
   &amp;#39;id&amp;#39;: 1113860285566046210,
   &amp;#39;media_url&amp;#39;: &amp;#39;http://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg&amp;#39;,
   &amp;#39;media_url_https&amp;#39;: &amp;#39;https://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg&amp;#39;,
   &amp;#39;sizes&amp;#39;: {&amp;#39;thumb&amp;#39;: {&amp;#39;w&amp;#39;: 150, &amp;#39;h&amp;#39;: 150, &amp;#39;resize&amp;#39;: &amp;#39;crop&amp;#39;},
    &amp;#39;large&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;},
    &amp;#39;medium&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;},
    &amp;#39;small&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;}},
   &amp;#39;type&amp;#39;: &amp;#39;animated_gif&amp;#39;,
   &amp;#39;url&amp;#39;: &amp;#39;https://t.co/QOULUc5a0u&amp;#39;,
   &amp;#39;video_info&amp;#39;: {&amp;#39;aspect_ratio&amp;#39;: [125, 99],
    &amp;#39;variants&amp;#39;: [{&amp;#39;bitrate&amp;#39;: 0,
      &amp;#39;content_type&amp;#39;: &amp;#39;video/mp4&amp;#39;,
      &amp;#39;url&amp;#39;: &amp;#39;https://video.twimg.com/tweet_video/D3U6BzqUUAIwYEC.mp4&amp;#39;}]}}],
 &amp;#39;retweet_count&amp;#39;: 2,
 &amp;#39;source&amp;#39;: &amp;#39;&amp;lt;a href=&amp;quot;http://twitter.com/download/android&amp;quot; rel=&amp;quot;nofollow&amp;quot;&amp;gt;Twitter for Android&amp;lt;/a&amp;gt;&amp;#39;,
 &amp;#39;text&amp;#39;: &amp;quot;@vboykis df.dropna(how=&amp;#39;brute force&amp;#39;) https://t.co/QOULUc5a0u&amp;quot;,
 &amp;#39;urls&amp;#39;: [],
 &amp;#39;user&amp;#39;: {&amp;#39;id&amp;#39;: 70255183, &amp;#39;id_str&amp;#39;: &amp;#39;70255183&amp;#39;},
 &amp;#39;user_mentions&amp;#39;: [{&amp;#39;id&amp;#39;: 19304217,
   &amp;#39;id_str&amp;#39;: &amp;#39;19304217&amp;#39;,
   &amp;#39;name&amp;#39;: &amp;#39;Vicki Boykis&amp;#39;,
   &amp;#39;screen_name&amp;#39;: &amp;#39;vboykis&amp;#39;}]}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;access-status-attributes-how-i-accessed-the-data-used-below&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Access Status Attributes (How I accessed the data used below)&lt;/h3&gt;
&lt;p&gt;Since I’m interested in modeling expected number of tweets in a week, the most relevant attribute is the timestamps in the &lt;code&gt;created_at&lt;/code&gt; attribute. These attributes can be accessed directly from the Status object. Below I make a list of the times at which each tweet was created and check the first element of that list;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get list of time stamps
times = [pd.Timestamp(tweet.created_at) for tweet in timeline]
times.reverse() # sort s.t. times[0] is lowest, times[-1] is highest
times[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Timestamp(&amp;#39;2018-11-01 05:56:58+0000&amp;#39;, tz=&amp;#39;tzutc()&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a list of timestamps. By default, times from the twitter API are localized to the UTC timezone. Below I convert these to my local time in California;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;times = [time.tz_convert(&amp;quot;America/Los_Angeles&amp;quot;) for time in times]
times[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Timestamp(&amp;#39;2018-10-31 22:56:58-0700&amp;#39;, tz=&amp;#39;America/Los_Angeles&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a list of timestamps in local time! To get a sense of the duration over which this data spans, below I compute the time difference between the frist and last timestamp;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;times[-1]-times[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Timedelta(&amp;#39;154 days 11:49:05&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Woah, almost 155 days of my twitter activity!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson Process&lt;/h2&gt;
&lt;p&gt;A Poisson process is a common framework for modeling events that occurr in time or space. In this context, tweets are being created over time and we are interested in modeling the rate at which tweets are created in order to predict how many tweets will be created in a week.&lt;/p&gt;
&lt;div id=&#34;assumptions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assumptions&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No more than one event can occur at a single point in time.
&lt;ul&gt;
&lt;li&gt;This can be violated when a user publishes a thread of multiple tweets at once. This can be fixed by recoding threads as a single status.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Independence: The interval lengths for each event are not influenced by any other event.
&lt;ul&gt;
&lt;li&gt;This can be violated if, instead of using Twitter’s thread option, a user ends a tweet with “…” to indicate that they will soon create another tweet. In this case, there are some tweets that imply a shorter interval before the next tweet.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Homogeneity: The distribution of intervals is the same throughout the entire process.
&lt;ul&gt;
&lt;li&gt;I probe this assumption in depth below, and it almost certainly violated.&lt;/li&gt;
&lt;li&gt;There are methods for modeling inhomogeneous Poisson processes, but I ignore those here.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;specification-and-properties&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specification and Properties&lt;/h3&gt;
&lt;p&gt;In this context tweet events are occurring across time. I index tweets with &lt;span class=&#34;math inline&#34;&gt;\(i\in\{1,...,N\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the total number of tweets observed. Each tweet is created at a time, &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;, and the next tweet is observed after an interval &lt;span class=&#34;math inline&#34;&gt;\(s_{i}\)&lt;/span&gt;. That is, if tweet &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is created at time &lt;span class=&#34;math inline&#34;&gt;\(t_{i-1}\)&lt;/span&gt; then tweet &lt;span class=&#34;math inline&#34;&gt;\(i+1\)&lt;/span&gt; is created at time &lt;span class=&#34;math inline&#34;&gt;\(t_{i}=t_{i-1}+s_{i}\)&lt;/span&gt;. The interval between each tweet is &lt;span class=&#34;math inline&#34;&gt;\(s_i = t_{i}-t_{i-1} = (t_{i-1}+s_i)-t_{i-1}\)&lt;/span&gt;. The assumptions of a Poisson process permit the following distributions for three interesting features of this scenario.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The distribution of time between events, &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt;, is exponential; &lt;span class=&#34;math inline&#34;&gt;\(s_i\sim Expo(\lambda) \Rightarrow p(s_i|\lambda) = \lambda e^{-\lambda s_i}\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a parameter that describes the tweet rate.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Given an interval of length &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, the distribution of the count of events in that interval, &lt;span class=&#34;math inline&#34;&gt;\(c|s\)&lt;/span&gt;, is Poisson; &lt;span class=&#34;math inline&#34;&gt;\(c|s\sim Poisson(\lambda s) \Rightarrow p(c|s,\lambda) = \frac{(\lambda s)^{c}e^{-\lambda s}}{c!}\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;The count of events, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, in a fixed interval depends both on the rate of the events, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, and the duration of the interval, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The distribution of the total interval required for &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; events, &lt;span class=&#34;math inline&#34;&gt;\(s|c\)&lt;/span&gt;, is gamma; &lt;span class=&#34;math inline&#34;&gt;\(s|c \sim Gamma(c,\lambda) \Rightarrow p(s|c,\lambda) = \frac{\lambda^c}{\Gamma(c)}(s)^{c-1}e^{-\lambda s}\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;The interval, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, required for a fixed number of events depends on both the rate of the events, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, and the number of events, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;More information on these three kinds of distributions, and ways to implement them in Python, can be found in the &lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/stats.html&#34;&gt;scipy documentation on statistical functions&lt;/a&gt;. General information on each of these distributions can be found on the Wikipedia page for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Exponential_distribution&#34;&gt;exponential&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Poisson_distribution&#34;&gt;Poisson&lt;/a&gt;, or &lt;a href=&#34;https://en.wikipedia.org/wiki/Gamma_distribution&#34;&gt;gamma&lt;/a&gt; distribution. A key difference between the standard uses of these distributions and their roles in a Poisson process is that the rate parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is also scaled by the duration of an interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; when constructing a distribution for the count of events in interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; (2, above) or the duration of the interval required for &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; events (3, above).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-the-homogeneity-assumption&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Checking the Homogeneity Assumption&lt;/h3&gt;
&lt;p&gt;The homogeneity assumption strictly requires that tweet rates are constant across time. This would generate data that are uniform across meaningful intervals such as time in a week or time in a day. To check homogeneity, below I convert the timestamps into the hour within a week, minute within a day, and minute within an hour, and plot tweet counts across these representations of time.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# convert to hours in a week
hour_of_week     = [t.weekday()*24+t.hour+t.minute/60 for t in times]
minute_of_day    = [t.hour*60+t.minute+t.second/60 for t in times]
minute_of_hour   = [t.minute+t.second/60 for t in times]
plt.figure(figsize=(10,4));
plt.hist(hour_of_week,bins=80);&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.title(&amp;quot;My Tweet Counts By Hour Of Week&amp;quot;);
plt.ylabel(&amp;quot;Count&amp;quot;);
plt.xlabel(&amp;quot;Hour In Week&amp;quot;);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The histogram above indicates that there might be some hours of the week that have a higher rate than others. For example, I don’t seem to tweet much early on Sunday (hours 0-5), but I do seem to tweet a lot during the day on Sunday (around hours 6-20). Below I use rug plots, which represent a tweet event with a vertical line near the x-axis, and an imposed kernel density estimate, which is a continuous version of a histogram. I remake this plot in terms of hours within a week, minutes within a day, and minutes within an hour. If the tweet rate (&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) were homogeneous, then the kernel density estimate would be approximately flat.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def rug_plot_and_density(dat,bw,xlab,xlim):
    # rug plot + density
    plt.figure(figsize=(10,4))
    sns.distplot(dat, hist = False, kde = True, rug = True,
                 color = &amp;#39;darkblue&amp;#39;, 
                 kde_kws={&amp;#39;linewidth&amp;#39;: 3,&amp;quot;bw&amp;quot;:bw},
                 rug_kws={&amp;#39;color&amp;#39;: &amp;#39;black&amp;#39;})
    # formatting
    plt.title(&amp;#39;Tweet Density By &amp;#39;+xlab)
    plt.xlabel(xlab)
    plt.ylabel(&amp;#39;Kernel Density&amp;#39;)
    plt.xlim(xlim);
    plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# data and plot formatting arguments
dats = (hour_of_week,minute_of_day,minute_of_hour)
xlabs=(&amp;#39;Hour of Week&amp;#39;,&amp;#39;Minute of Day&amp;#39;,&amp;#39;Minute of Hour&amp;#39;)
xlims=([0,24*7],[0,24*60],[0,60]);
bws  = (4,40,4)

# make plots
for dat,xlab,xlim,bw in zip(dats,xlabs,xlims,bws):
    rug_plot_and_density(dat=dat,bw=bw,xlab=xlab,xlim=xlim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-13-2.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-13-3.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Are very sensitive to the choice of the bandwidth parameter that determines the window over which to aggregate events (similar to bin size when using a histogram). I’m using these plots to demonstrate possible violations of homogeneity that I would follow up on in a real analysis, but will not follow up on here.&lt;/p&gt;
&lt;p&gt;From the top plot, there seems to be two patterns. First, a series of peaks and troughs that roughly correspond to daytime and night-time hours. I probably tweet with a higher frequency when I am awake, rather than asleep – meaning that &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; may depend on time within a day. Second, a generally lower kernel density estimate during the middle than the edges. I may tweet more during the weekends (edge hours) than week days (middle hours).&lt;/p&gt;
&lt;p&gt;From the middle plot that displays tweet frequencies by minutes within a day, there again seem to be two trends. First, I rarely tweet before minute 400 (around 6:40AM). Second, I have peaks around minute 600 (10:00AM), 1000 (4:40PM), and 1350 (10:00PM). This might be related to the times that I take a break from working. I generally take a break around 5:00PM, and usually take another break before bed around 9:00-10:00PM.&lt;/p&gt;
&lt;p&gt;The bottom plot displays tweet frequencies by minutes within an hour. This seems more flat overall longer periods of time, but I may strangely tend to tweet more during the first half of hours.&lt;/p&gt;
&lt;p&gt;Overall, there are many reasons that the homogeneity assumption may be violated. For cases like this, the tweet rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; can be modeled as a function of time. However, to keep this example simple, I’ll ignore possible violations and proceede as if &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; were a constant with respect to time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-the-exponential-distribution-of-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Checking the exponential distribution of intervals&lt;/h3&gt;
&lt;p&gt;Let’s the distribution of the time between tweets, &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt;. If the assumptions of the Poisson process were satisfied, then the intervals between tweets would follow an exponential distribution. Below I compute the number of seconds between tweets and display each value as a black dash on the x-axis. I overlay a histogram, a kernel density, and a exponential density based on the observed mean interval.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# compute intervals between tweets
ss = [times[i]-times[i-1] for i in range(1,len(times))];
# convert from Timedelta to total time in seconds
ss = [si.total_seconds() for si in ss]

# histogram and density plot
plt.figure(figsize=(10,4))
sns.distplot(ss, hist = True, kde = True, rug = True,
             color = &amp;#39;darkblue&amp;#39;, bins=100,
             hist_kws={&amp;#39;color&amp;#39;:[0,.7,.5,.5],&amp;#39;label&amp;#39;:&amp;#39;Histogram&amp;#39;},
             kde_kws={&amp;#39;linewidth&amp;#39;: 3,&amp;quot;bw&amp;quot;:60*60,&amp;#39;label&amp;#39;:&amp;#39;Kernel Density&amp;#39;},
             rug_kws={&amp;#39;color&amp;#39;: &amp;#39;black&amp;#39;})
plt.xlim([0,680000]);&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.title(&amp;#39;Distribution of intervals between tweets&amp;#39;)
plt.xlabel(&amp;#39;Seconds&amp;#39;)
plt.ylabel(&amp;#39;Frequency&amp;#39;)

# overlay an exponential density
tmp_rate=np.mean(ss)
tmpx = np.linspace(0,680000,680000*5)
tmpy = expon.pdf(tmpx,scale=tmp_rate)
plt.plot(tmpx,tmpy,color=[.7,0,0,1],linewidth=3,label=&amp;#39;Exponential Density&amp;#39;);
# add legend
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on the relative heights of the kernel density and the exponential density, there seem to be more short intervals, fewer moderate length intervals, and more long intervals relative to the exponential distribution. The mean and standard deviation of an exponential distribution should be the same value. Below I check the standard deviaion and mean of the observed intervals.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# check standard deviation
np.std(ss),np.mean(ss),np.std(ss)/np.mean(ss)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; (90150.9122414953, 67076.1055276382, 1.3440093388300445)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The observed standard deviation is about 1.34 times larger (variance is about 1.80 times larger) than it would be if the data were exponentially distributed with the observed mean. While this could be accounted for with an overdispersion parameter, I will ignore this issue here for the sake of having a simple and fast online model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-of-tweet-fequency-using-conjugacy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model of Tweet Fequency Using Conjugacy&lt;/h2&gt;
&lt;p&gt;First, I’ll assume (despite the overdispersion) that the intervals between tweets follow an exponential distribution;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
s_i|\lambda &amp;amp;\sim Expo(\lambda) \\
\Rightarrow p(s_i|\lambda) &amp;amp;= \lambda e^{-s_i \lambda}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To account for uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, I’ll use a Gamma distribution with shape &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and rate &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda &amp;amp;\sim Gamma(\alpha,\beta) \\ 
\Rightarrow p(\lambda|\alpha,\beta) &amp;amp;= \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1}e^{-\lambda \beta}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The choice of a Gamma distribution allows for fast updates using conjugacy between the prior beliefs about &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; based on data observed up to time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and the exponential likelihood for the interval observed at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda | s_t &amp;amp;\sim Gamma(\alpha_t,\beta_t) \\ 
p(s_{t+1}|\lambda) &amp;amp;= \lambda e^{-s_{t+1} \lambda} \\
p(\lambda|\alpha_t,\beta_t,s_{t+1}) &amp;amp;\propto p(s_{t+1}|\lambda) p(\lambda|\alpha_t,\beta_t) \\
&amp;amp;= \lambda e^{-s_{t+1} \lambda} \frac{\beta_t^{\alpha_t}}{\Gamma(\alpha_t)} \lambda^{\alpha_t-1}e^{-\lambda \beta_t} \\
&amp;amp;= \lambda^{\alpha_t} e^{-\lambda(s_{t+1}+\beta_t)} \\ 
\Rightarrow \lambda | s_{t+1} &amp;amp;\sim Gamma(\alpha_t+1,\beta_t+s_{t+1})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This implies the following update rules for computing the parameters of the posterior over &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\alpha_{t+1} &amp;amp;\leftarrow \alpha_t +1 \\
\beta_{t+1}  &amp;amp;\leftarrow \beta_t + s_{t+1} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To answer the question of how many tweets might be observed in a period of time, I’ll assume that the count of tweets &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is Poisson distributed with rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Then the number of tweets expected in an interval of length &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; would be;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\theta &amp;amp;= s\lambda \\ 
c|\theta &amp;amp;\sim Poisson(\theta) \\
\Rightarrow p(c|\theta) &amp;amp;= \frac{\theta^c e^{-\theta}}{c!}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This means that, rather than &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, we are actually interested in the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta=s\lambda\)&lt;/span&gt;. I derive this below using a change of vairables;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\theta = s\lambda &amp;amp;\Rightarrow \lambda = \frac{\theta}{s} = \theta s^{-1} \\
p_\theta(\theta | \lambda,s) &amp;amp;= p_\lambda\left(\theta s^{-1} | s,\alpha,\beta\right) \left| \frac{d \lambda}{d \theta}\right|\\
&amp;amp;= \frac{\beta^\alpha}{\Gamma(\alpha)} \left(\frac{\theta}{s}\right)^{\alpha-1}e^{-\frac{\theta}{s}\beta} \left|s^{-1}\right| \\
&amp;amp;= \frac{\beta^\alpha}{\Gamma(\alpha)} \theta^{(\alpha-1)}s^{-({\alpha-1})-1}e^{-\frac{\theta}{s}\beta} \\
\Rightarrow p(\theta|s,\alpha,\beta)&amp;amp;= \frac{\left(\frac{\beta}{s}\right)^\alpha}{\Gamma(\alpha)} \theta^{(\alpha-1)}e^{-\theta\frac{\beta}{s}}\\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can find the distribution of &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; that accounts for uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; through the prior on &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(c|s,\alpha,\beta) &amp;amp;= \int_\theta p(c|\theta)p(\theta|s,\alpha,\beta)d\theta \\
&amp;amp;=\int_\theta \frac{\theta^c e^{-\theta}}{c!} \frac{\left(\frac{\beta}{s}\right)^\alpha}{\Gamma(\alpha)} \theta^{(\alpha-1)}e^{-\theta\frac{\beta}{s}} d\theta \\
&amp;amp;= \frac{\left(\frac{\beta}{s}\right)^\alpha}{c!\Gamma(\alpha)}
\int_\theta \theta^c e^{-\theta} \theta^{(\alpha-1)}e^{-\theta\frac{\beta}{s}} d\theta \\
&amp;amp;=\frac{\left(\frac{\beta}{s}\right)^\alpha}{c!\Gamma(\alpha)}
\int_\theta \theta^{c+\alpha-1} e^{-\theta\left(\frac{\beta+s}{s}\right)} d\theta \\
&amp;amp;=\frac{\left(\frac{\beta}{s}\right)^\alpha}{c!\Gamma(\alpha)}
\frac{\Gamma(c+\alpha)}{\left(\frac{\beta+s}{s}\right)^{c+\alpha}} \\
&amp;amp;=\frac{\Gamma(c+\alpha)}{c!\Gamma(\alpha)} \beta^\alpha s^{-\alpha} s^{c+\alpha}
(\beta+s)^{-(c+\alpha)} \\
&amp;amp;=\frac{\Gamma(c+\alpha)}{\Gamma(c+1)\Gamma(\alpha)}  \left(\frac{s}{\beta+s}\right)^c \left(\frac{\beta}{\beta+s}\right)^\alpha \\ 
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since a Poisson Process assumes that no more than one event can occur in an interval, intervals can be treated as discrete Bernoulli trials in which an event either occurs or does not occur. In this discrete settng, the distribution of the count of intervals with an event &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; for a given number of intervals without the event &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and a probability of the event in each interval &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; will follow a negative binomial distribution;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k &amp;amp;\sim NegBino(r,p) \\ 
\Rightarrow p(k|r,p) &amp;amp;= \frac{(k+r-1)!}{r! (k-1)!}p^{k}(1-p)^{r} \\
&amp;amp;= \frac{\Gamma(k+r)}{\Gamma(r+1) \Gamma(k)}p^{k}(1-p)^{r} \\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combining the two results above, we see that the count &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; in an interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; will follow a negative binomial distribution such that &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; fixes the number of intervals in which no event occurs, and &lt;span class=&#34;math inline&#34;&gt;\(\frac{s}{\beta+s}\)&lt;/span&gt; captures the probability of an event in any unit length interval;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
c &amp;amp;\sim NegBino\left(\alpha,\frac{s}{\beta+s}\right) \\ 
\Rightarrow p(c|s,\alpha,\beta) &amp;amp;= \frac{\Gamma(c+\alpha)}{\Gamma(c+1)\Gamma(\alpha)} \left(\frac{s}{\beta+s}\right)^c \left(\frac{\beta}{\beta+s}\right)^\alpha \\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In summary, the key components of this model are the exponential likelihood and gamma priors which allow for the fast and simple updating rules to compute the posterior over &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, and the negative binomial predictive distribution which accounts for the uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. So the applicable information from above is;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda &amp;amp;\sim Gamma(\alpha_t,\beta_t) \\
s_{t+1} &amp;amp;\sim Expo(\lambda) \\
\alpha_{t+1} &amp;amp;\leftarrow \alpha_t + 1      \\ 
\beta_{t+1}  &amp;amp;\leftarrow \beta_t + s_{t+1} \\ 
c|s,\alpha,\beta &amp;amp;\sim NegBino\left(\alpha,\frac{s}{\beta+s}\right) \\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here I investigate posterior predictive intervals for tweets over a period of time &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; while using different components of this model. In the code below, I compute all values of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; based on the update rules derived above from conjugacy.&lt;/p&gt;
&lt;!---
This formulation can be thought of as discretizing the Poisson Process into $c+\alpha$ trials defined as intervals of length $\frac{1}{\beta+s}$, where $\beta$ is the sum of the interval lengths for which no event occured, and $s$ is the sum of interval lengths for those intervals that included an event. 
---&gt;
&lt;!---
$\lambda$ has units! 

Gamma prior on the rate;
$$ \lambda \sim Gamma(\alpha,\beta)$$
$$p(\lambda)=\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda}$$

Observe an exponential interval;
$$p(s|\lambda) = \lambda e^{-\lambda s}$$

Then the posterior becomes; 

$$p(\lambda|s)\propto p(s|\lambda)p(\lambda) 
= \lambda^{\alpha-1}e^{-\beta \lambda} \lambda e^{-\lambda s} 
= \lambda^{(\alpha+1)-1}e^{-\lambda (\beta+s)}$$
$$\Rightarrow \lambda|s \sim Gamma(\alpha+1,\beta+s)$$

To incorporate more prior information, the prior $\alpha$ would reflect the number of observations, and $\beta$ can reflect the sum of intervals associated with each observation.

Given an estimate of $\lambda$, a poisson can be used to estimate the number of events in an interval (a week). 

A Poisson and a gamma make a negative binomial, which is used for predictive densities that account for uncertainty in the rate.
---&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# initialize alpha,beta as 0
alpha,beta = 0,0
alphas,betas = list(),list()
# for each observed interval in ss,
for si in ss:
    si = si/(60*60*24) # si s/1 * 1/60 m/s * 1/60 h/m * 1/24 d/h convert to days
    alpha+=1 # increment alpha by 1
    beta+=si # increment beta by the interval length
    # save parameters for analysis
    alphas.append(alpha)
    betas.append(beta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above converts intervals from seconds to days and comptutes &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; for all intervals in my dataset. This gives the parameters for posterior beliefs over &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; as each tweet is observed. Below I check the final parameters and some statistics from the last posterior.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a,b,mode,median,mean=(alpha,beta,(alpha-1)/beta,gamma.ppf(.5,a=alpha,scale=1/beta),alpha/beta)
print(  &amp;#39;alpha : &amp;#39;+str(a)+
      &amp;#39;\nbeta  : &amp;#39;+str(b)+
      &amp;#39;\nmode  : &amp;#39;+str(mode)+
      &amp;#39;\nmedian: &amp;#39;+str(median)+
      &amp;#39;\nmean  : &amp;#39;+str(mean))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; alpha : 199
&amp;gt;&amp;gt;&amp;gt; beta  : 154.49241898148153
&amp;gt;&amp;gt;&amp;gt; mode  : 1.2816162845099446
&amp;gt;&amp;gt;&amp;gt; median: 1.2859321345366828
&amp;gt;&amp;gt;&amp;gt; mean  : 1.2880890940276715&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; correctly indicates 199 observed intervals, and the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; of 154.59 also correctly reflects the time difference in days that was computed in the first section. Lastly the ordinal relationship of the mode, median, and mean is consistent with that of a gamma distribution.&lt;/p&gt;
&lt;div id=&#34;inference-on-tweet-rate-lambda-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inference On Tweet Rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; Over Time&lt;/h3&gt;
&lt;p&gt;In the code below, I compute the maximum a posteriori (MAP) estimate, or posterior mode, across time. I plot this across time with the 97.5&lt;span class=&#34;math inline&#34;&gt;\(^{th}\)&lt;/span&gt; and 2.5&lt;span class=&#34;math inline&#34;&gt;\(^{th}\)&lt;/span&gt; percentiles as a shaded region representing the 95% credible interval.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get MAP lambda
lambda_map   = [(alpha-1)/beta for alpha,beta in zip(alphas,betas)]
# get CI upper and lower bounds
lambda_upper = [gamma.ppf(.975,a=alpha,scale=1/beta) for alpha,beta in zip(alphas,betas)]
lambda_lower = [gamma.ppf(.025,a=alpha,scale=1/beta) for alpha,beta in zip(alphas,betas)]
# plot over time
plt.figure(figsize=(10,4))
sns.distplot(times[1:], hist = False, kde = False, rug = True,
             color = &amp;#39;darkblue&amp;#39;, 
             rug_kws={&amp;#39;color&amp;#39;: &amp;#39;black&amp;#39;});
plt.plot(times[1:],lambda_map,color=[0,0,1],label=&amp;#39;MAP&amp;#39;);
plt.fill_between(times[1:], lambda_lower, lambda_upper,color=[0,0,1,.1],label=&amp;#39;95% CI&amp;#39;)
plt.ylabel(&amp;#39;Posterior on Lambda (tweets/day)&amp;#39;)
plt.xlabel(&amp;#39;Time&amp;#39;)
plt.title(&amp;#39;Summary of the posterior over $\lambda$ across time&amp;#39;)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The posterior seems to tighten dramatically around the mode during the first month, and the mode seems to stabalize after about that much time. However, this method seems slow to adjust to the slower rate of tweets in 2019. One way to address this might be to add weights to the parameter updates such that &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; are more sensitive to recent data than past data. These weights (or stepsize or learning rates) can be based on the surprisal, or likelihood, of a new tweet under the posterior. If the new interval was well anticipated, then little updating is needed, but if the new interval was very surprising or unlikely, then a sharpe change in &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; might be warrented.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-number-of-tweets-in-interval-s&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predicting Number Of Tweets In Interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Now I’ll visually compare predictions for the count of tweets in a week based on estimates of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; after the first 20 tweets and using all 200 tweets, using two models;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A Poisson using &lt;span class=&#34;math inline&#34;&gt;\(\theta=s\lambda_{MAP}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The negative binomial derived in the model section above&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First, an issue with the parameterization of the negative binomial has to be addressed. In my derivation, I ended with a negative binomial parameterized as;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k&amp;amp;:\text{Number of successes} \\
r&amp;amp;:\text{Number of failures} \\
p&amp;amp;:\text{Probability of failure} \\
p(k|r,p) &amp;amp;= \frac{(k+r-1)!}{r! (k-1)!}p^{k}(1-p)^{r} \\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Scipy.stats.nbinom&lt;/code&gt; function defines a negative binomial with;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k&amp;amp;:\text{Number of failures} \\
n&amp;amp;:\text{Number of successes} \\
p&amp;amp;:\text{Probability of success} \\
p(k|n,p) &amp;amp;= \frac{(k+n-1)!}{(n-1)!k!} p^{n}(1-p)^{k}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recall in my derivation that the parameter corresponding to probability of failure was &lt;span class=&#34;math inline&#34;&gt;\(p=\frac{s}{\beta+s}\)&lt;/span&gt;. The differences in parameterization can be accounted for by supplying the &lt;code&gt;nbinom&lt;/code&gt; function with &lt;span class=&#34;math inline&#34;&gt;\(p=1-\frac{s}{\beta+s}=\frac{\beta}{\beta+s}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The code below plots predictive densities based on the two approaches above from estimates of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; based only on the first 20 observations.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# for the first 20 tweets
# parameters
cs = range(20)
s = 7
index = 19
theta = s*lambda_map[index]
plt.figure(figsize=(10,4))
# plot the Poisson density
plt.scatter(cs,poisson.pmf(cs,theta))
plt.plot(   cs,poisson.pmf(cs,theta),label=&amp;#39;Poisson&amp;#39;)
# plot the negative binomial density
plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])))
plt.plot(   cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=&amp;#39;Negative Binomial&amp;#39;);
# labels
plt.ylabel(&amp;#39;Mass&amp;#39;)
plt.xlabel(&amp;#39;Count&amp;#39;)
plt.title(&amp;#39;Predictions for Tweet Counts in a Week, Using 20 Observations&amp;#39;)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With a small amount of data, the uncertainty in the estimate of tweet rates carries through the negative binomial model, which gives more mass to a wider range of counts relative to the Poisson model that discards uncertainty when by &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{MAP}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Using all data
# parameters
index = 198
theta = s*lambda_map[index]
plt.figure(figsize=(10,4))
# plot the Poisson density
plt.scatter(cs,poisson.pmf(cs,theta))
plt.plot(   cs,poisson.pmf(cs,theta),label=&amp;#39;Poisson&amp;#39;)
# plot the negative binomial density
plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])))
plt.plot(   cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=&amp;#39;Negative Binomial&amp;#39;);
# labels
plt.ylabel(&amp;#39;Mass&amp;#39;)
plt.xlabel(&amp;#39;Count&amp;#39;)
plt.title(&amp;#39;Predictions for Tweet Counts in a Week, Using All Data&amp;#39;)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With a larger amount of data and predicting the tweet count over a short interval (7 days), the predictive distribution from the negative binomial and Poisson models are nearly indistinguishable. Below I compare the predictions of these model for an interval of 1 year (365 days), again using all data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Using all data
# parameters
cs=range(350,600)
s = 365
index = 198
theta = s*lambda_map[index]
plt.figure(figsize=(10,4))
# plot the Poisson density
plt.scatter(cs,poisson.pmf(cs,theta))
plt.plot(   cs,poisson.pmf(cs,theta),label=&amp;#39;Poisson&amp;#39;)
# plot the negative binomial density
plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])))
plt.plot(   cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=&amp;#39;Negative Binomial&amp;#39;);
# labels
plt.ylabel(&amp;#39;Mass&amp;#39;)
plt.xlabel(&amp;#39;Count&amp;#39;)
plt.title(&amp;#39;Predictions for Tweet Counts in a Year, Using All Data&amp;#39;)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When considering a longer period of time, The uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; again carries through the negative binomial model, but is discarded in the Poisson model that uses &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{MAP}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;This was a beefy notebook!&lt;/p&gt;
&lt;p&gt;First I showed how to pull tweets as Status objects from Twitter’s API. Given Status objects, I then showed how to embed them in a notebook, view them as a dictionary, and otherwise access their attributes to construct a list of tweet timestamps. I then described Poisson Processes as they may apply to modeling user tweet rates and tweet counts over periods of time. I used kernel densities to check assumptions of a Poisson Process and found possible violations of homogeneity, and of the result that intervals between tweets should follow an exponential distribution.&lt;/p&gt;
&lt;p&gt;Punting these violations, I developed a model of tweet frequency using conjugacy between Gamma priors and Exponential likelihoods. To predict tweet counts over a period of time, I derived a negative binomial predictive distribution that accounted for uncertainty in a user’s tweet rate. I compared this distribution to a Poisson distribution that ignored uncertainty by taking only the maximum a posteriori estimate of a user’s tweet rate.&lt;/p&gt;
&lt;p&gt;Overall the model that discards posterior uncertainty attributes less mass to fringe counts, especially when there is little data or when the interval over which counts are being predicted is long. Incorporating posterior uncertainty broadens the predictive distribution to reflect uncertainty in the underlying tweet rate. That uncertainty exists regardless of the modeling approach – excluding it from predictive distributions only leads to narrow, overconfident predictions. This general principle of propagating uncertainty through a statistical process is one strong advantage of the Bayeian modeling approach that I developed and applied here.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>LinkedIn Notes</title>
      <link>/MacStrelioff/insightstudying/linkedin-deep-dive/</link>
      <pubDate>Sat, 17 Aug 2019 18:56:23 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/linkedin-deep-dive/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#company-visit&#34;&gt;Company Visit&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#chris-info&#34;&gt;Chris info –&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recruiter-call&#34;&gt;Recruiter Call&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recruiter-emails-ii-iii-interviews&#34;&gt;Recruiter emails II &amp;amp; III – Interviews&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#schedule-and-interviewers&#34;&gt;Schedule and interviewers:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#host-leader-joonhyung-lim&#34;&gt;10:00-10:45 | Host Leader | &lt;a href=&#34;https://www.linkedin.com/in/joonlim/&#34;&gt;Joonhyung Lim&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statistical-modeling-dongping-sunny-jing-and-caitlin-crump&#34;&gt;10:45-11:30 | Statistical Modeling | &lt;a href=&#34;https://www.linkedin.com/in/sunny-jing-ab530848/&#34;&gt;Dongping (sunny) Jing&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/caitlincrump/&#34;&gt;Caitlin Crump&lt;/a&gt;:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lunch-nuo-wang&#34;&gt;11:30-12:30 | Lunch | &lt;a href=&#34;https://www.linkedin.com/in/nuo-w-486b3027/&#34;&gt;Nuo Wang&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-manipulation-xianyun-mao&#34;&gt;12:30-1:15 | Data Manipulation | &lt;a href=&#34;https://www.linkedin.com/in/xianyun-mao-232ba916/&#34;&gt;Xianyun Mao&lt;/a&gt;:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statistics-ab-testing-qiannan-yin-and-yuan-chai&#34;&gt;1:15-2:00 | Statistics &amp;amp; A/B Testing | | &lt;a href=&#34;https://www.linkedin.com/in/qiannan-y-10987ba4/&#34;&gt;Qiannan Yin&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/yuan-chai-11127047/&#34;&gt;Yuan Chai&lt;/a&gt;:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analytics-case-study-gigi-zhang&#34;&gt;2:00-2:45 | Analytics Case Study | &lt;a href=&#34;https://www.linkedin.com/in/gigizhezhang/&#34;&gt;Gigi Zhang&lt;/a&gt;:&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#users&#34;&gt;Users&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#current-issues&#34;&gt;Current issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ideas-and-excitement&#34;&gt;Ideas and excitement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#excitement-from-recruiter&#34;&gt;Excitement from Recruiter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#onsite&#34;&gt;Onsite&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#glassdoor-interview-questions&#34;&gt;Glassdoor interview questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#knowledge-gaps&#34;&gt;Knowledge gaps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#next-steps&#34;&gt;Next steps&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;company-visit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Company Visit&lt;/h1&gt;
&lt;div id=&#34;chris-info&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chris info –&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;DS people generally get interviewed on computer engineering topics – eg implementing data structures from scratch, …&lt;/li&gt;
&lt;li&gt;Interview centrally for AI organization, then get matched with a team.&lt;/li&gt;
&lt;li&gt;Module on CS which is like data structures,&lt;/li&gt;
&lt;li&gt;LeetCode medium, but also more of a conversation&lt;/li&gt;
&lt;li&gt;Binary converter from scratch&lt;/li&gt;
&lt;li&gt;Being able to describe process&lt;/li&gt;
&lt;li&gt;ML on fundamentals of ML, basics and some applications&lt;/li&gt;
&lt;li&gt;Optional product design module&lt;/li&gt;
&lt;li&gt;lots of coding things on a whiteboard&lt;/li&gt;
&lt;li&gt;coding style matters / code should be interpratable&lt;/li&gt;
&lt;li&gt;making sure code would work, eg walk through an example or test cases, think of edge cases&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recruiter-call&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recruiter Call&lt;/h1&gt;
&lt;p&gt;Graydon, sr recruiter at linkedin&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;talks about roles, … .&lt;/li&gt;
&lt;li&gt;&lt;p&gt;product team&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Role?&lt;/li&gt;
&lt;li&gt;mission: create economic opportunity for members of workforce. Help people, as many as possiblt.&lt;/li&gt;
&lt;li&gt;inclusivity – economic opportunity, and recommending users to recruiters.&lt;/li&gt;
&lt;li&gt;How do we deploy products that highlight and empower users to&lt;/li&gt;
&lt;li&gt;Smart Products: recommend learning products based on career progression&lt;/li&gt;
&lt;li&gt;Business side: LinkedIn really white collar dominant, need to include blue-collar workers on LinkedIn..&lt;/li&gt;
&lt;li&gt;Or.. partner with governemnts to address skills gaps..&lt;/li&gt;
&lt;li&gt;for background and how it relates;;&lt;/li&gt;
&lt;li&gt;“Take intelligent risk”: intelligent means is there data to support actions..? My job.&lt;/li&gt;
&lt;li&gt;DS works closely with a product or user group, doing end to end functionality.. An opportunity to identify ways where things can be done better. –&lt;/li&gt;
&lt;li&gt;END TO END proactive DS work:&lt;/li&gt;
&lt;li&gt;interview towards a general profile – manipulation, problem solving, stats, … .&lt;/li&gt;
&lt;li&gt;Allocate after passing onsite.&lt;/li&gt;
&lt;li&gt;Some teams more ML, causal inference, experimentation.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;40 positions open now.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Expedite process? Onsite Friday..&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;directly to onsite.&lt;/li&gt;
&lt;li&gt;will send a application for a data scientist. Will probably get application by end of week / later tonight. Make sure core skillset looks good.&lt;/li&gt;
&lt;li&gt;apply, He’ll be notified, and send an introduction email to Shana for scheduling, onsite, offer.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then Shana will assist through onsite process.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;can maybe have the onsite on Monday. will be flagged as urgent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Next steps / interview timeline?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ping if I hear nothing by Friday.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Why linkedin – role sounded awesome when presented at Insight. After looking into LinkedIn more, I came across papers on inferring latent user types, and that kind of work is really interesting to me. Generally though, I’m interested in experimentation roles and sequential testing, efficient experimentation and optional stopping.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;recruiter-emails-ii-iii-interviews&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recruiter emails II &amp;amp; III – Interviews&lt;/h1&gt;
&lt;p&gt;Here is the outline of what to expect from your onsite in terms of the structure:&lt;/p&gt;
&lt;div id=&#34;schedule-and-interviewers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Schedule and interviewers:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;10:00 AM – 10:45 AM | Host Manager | &lt;a href=&#34;https://www.linkedin.com/in/joonlim/&#34;&gt;Joonhyung Lim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Growth, user acquisition, and user onboarding&lt;/li&gt;
&lt;li&gt;10:45 AM – 11:30 AM | Statistical Modeling | Dongping Jing maybe &lt;a href=&#34;https://www.linkedin.com/in/sunny-jing-ab530848/&#34;&gt;Dongping (sunny) Jing&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/caitlincrump/&#34;&gt;Caitlin Crump&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;11:30 AM – 12:30 PM | Lunch | &lt;a href=&#34;https://www.linkedin.com/in/nuo-w-486b3027/&#34;&gt;Nuo Wang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;12:30 PM – 1:15 PM | Data Manipulation | &lt;a href=&#34;https://www.linkedin.com/in/xianyun-mao-232ba916/&#34;&gt;Xianyun Mao&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1:15 PM – 2:00 PM | Stats A/B Testing | &lt;a href=&#34;https://www.linkedin.com/in/qiannan-y-10987ba4/&#34;&gt;Qiannan Yin&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/yuan-chai-11127047/&#34;&gt;Yuan Chai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2:00 PM – 2:45 PM | Product Case Study | &lt;a href=&#34;https://www.linkedin.com/in/gigizhezhang/&#34;&gt;Gigi Zhang&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;host-leader-joonhyung-lim&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;10:00-10:45 | Host Leader | &lt;a href=&#34;https://www.linkedin.com/in/joonlim/&#34;&gt;Joonhyung Lim&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;One of the interviews will be done by a host manager; this person may, or may not, end up being the final hiring manager, but will be able to provide you with information on the different projects, engineering team culture, challenges, etc. He/she will be asking you about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;your career history&lt;/li&gt;
&lt;li&gt;your job search (why you’re looking, why is LinkedIn interesting, what technologies are you interested in)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;and an overview of interesting projects you worked on, and your involvement in these projects.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/joonlim/&#34;&gt;Joonhyung Lim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Growth, user acquisition, and user onboarding&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Overall: Excited about their products, their work (experimentation research, psychology in terms of identifying who wants to change jobs and what opportunities would be attractive), and culture (InDay, creating economic opportunity for all).&lt;/li&gt;
&lt;li&gt;My family is mainly construction workers and mechanics, which is a demographic that LinkedIn could expand to..&lt;/li&gt;
&lt;li&gt;Construction – work with unions to find employment for qualified workers, and with contractors to find jobs.&lt;/li&gt;
&lt;li&gt;More generally, LinkedIn could create a marketplace for union workers and contractors.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Amazon has mturk for very low-skill tasks, LinkedIn could create a marketplace for higher skill gigs, freelance work, temp work, and contract work.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Areas for growth:&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Services: resume drafting, job description drafting, HR system software for industry (could link it with corporate Office products) and get more data about the probability of an applicant attaining an offer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;History: PredictIt project, strategy inference, decision making&lt;/li&gt;
&lt;li&gt;Job Search: Generally like experimentation roles. I was first interested in LinkedIn because the anti-abuse team sounded awesome. I also liked the demo experience, there was a big turn out, and people seemed engaged. But overall the most exciting thing to me has been how company culture manifests in things like InDay, where individual across the company take time to give back to the community. I also really like the value that LinkedIn adds, and the direction LinkedIn seems to be going in, in terms of identifying the skills a person can learn to advance their career and advertising helpful resources to them. Thinking about my own journey through academia and now into data science, I spent a lot of time browsing Coursera, YouTube, and various online resources – I can imagine LinkedIn learning becoming a streamlined product for independent lerarning that basically syncs with your professional LinkedIn profile.&lt;/li&gt;
&lt;li&gt;I like the idea to &lt;a href=&#34;https://engineering.linkedin.com/blog/2015/11/video--higher-ed-team-seeks-to-predict-collegiate-futures&#34;&gt;engage high school students&lt;/a&gt;, and opening access to knowledge of hiring practices, career trajectories, and salary ranges. I come from a family of construction workers, so I was very much a black swan while figuring out how to navigate through community college, undergraduate school, and graduate school, and now into a technical career. I grew up in ‘blue collar’ world, and broke into the academic world, and now I’m jumping into ‘white collar’ technical industry world. This was really hard, and required a lot of self-learning and building professional relationships – so I very much understand the value that LinkedIn can add for people like me.&lt;/li&gt;
&lt;li&gt;So yeah I can see LinkedIn as something that moderates the education market, and the labor market – and based on my experiences navigating these things mostly on my own, I’d be really interested in contributing to products that democratize education and professional connections.&lt;/li&gt;
&lt;li&gt;Also really interested in the almost academic work that has come out of LinkedIn – I found papers on inferring user attributes, which is a very similar problem to one of my favorite projects in grad school. For that project I (kid gambling, strategy inference project).&lt;/li&gt;
&lt;li&gt;Also interested in the economics work (see others in ‘excitement’ section below)&lt;/li&gt;
&lt;li&gt;Overall, exciting work and a culture / mission / potential that resonates with me.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Online Bayesian Logistic Regression:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-modeling-dongping-sunny-jing-and-caitlin-crump&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;10:45-11:30 | Statistical Modeling | &lt;a href=&#34;https://www.linkedin.com/in/sunny-jing-ab530848/&#34;&gt;Dongping (sunny) Jing&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/caitlincrump/&#34;&gt;Caitlin Crump&lt;/a&gt;:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Evaluate and asses analytical thinking and familiarity with the LinkedIn Platform.&lt;/li&gt;
&lt;li&gt;Assess your ability to take business context and translate to an analytical problem&lt;/li&gt;
&lt;li&gt;Evaluate experience working with statistical modeling techniques&lt;/li&gt;
&lt;li&gt;Testing/Training/Evaluating models&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Asses knowledge of model methodologies&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/caitlincrump/&#34;&gt;Caitlin Crump&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;9yrs&lt;/li&gt;
&lt;li&gt;Working on Publishing - our news writing and news reading experience. Trying to answer questions like;&lt;/li&gt;
&lt;li&gt;Can we tell that something is relevant or good before anyone has even seen it?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Are our top writers/ influencers happy when we make changes on the site?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/sunny-jing-ab530848/&#34;&gt;Dongping (sunny) Jing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Market research and product analytics&lt;/li&gt;
&lt;li&gt;Connection Requests Deepdive for Network PVOM; MoT Drivers Analysis; ABI Funnel Deepdive; Unwanted Invites Analysis&lt;/li&gt;
&lt;li&gt;proof of concept (POC), another term, not mentioned; proof of value (PoV)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Market Research Analytics: Developing product journey data for all product areas (wvmp, abi, jymbii, etc…) and finding important drivers of NPS and MoT Surveys through Machine Learning and Statistical analysis;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mention:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wanting to write a blog, considered Wordpress, Medium, LinkedIn – Liked that LinkedIn was a platform where what I write would get high quality exposure, but didn’t like the interface – particulatly that I couldn’t figure out how to write a post in syntax like R markdown.&lt;/li&gt;
&lt;li&gt;Ended up making my own website with blogdown.&lt;/li&gt;
&lt;li&gt;Ask Dongping about her work – can draw on this later in product analytics interview.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Review variables / metrics in analytics section.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TODO:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LOOK UP OVERFITTING, and how to detect it&lt;/li&gt;
&lt;li&gt;Look up confusion martix&lt;/li&gt;
&lt;li&gt;Look into POC, POV&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;lunch-nuo-wang&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;11:30-12:30 | Lunch | &lt;a href=&#34;https://www.linkedin.com/in/nuo-w-486b3027/&#34;&gt;Nuo Wang&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/nuo-w-486b3027/&#34;&gt;Nuo Wang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4yrs, 3 years in consumer behavior at other companies&lt;/li&gt;
&lt;li&gt;LinkedIn Pages, Ads - I am hiring experienced DS who is passionate about shaping product roadmap and strategy through data!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mention:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-manipulation-xianyun-mao&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;12:30-1:15 | Data Manipulation | &lt;a href=&#34;https://www.linkedin.com/in/xianyun-mao-232ba916/&#34;&gt;Xianyun Mao&lt;/a&gt;:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Goal: assess ability to manipulate and analyze data for extracting insights&lt;/li&gt;
&lt;li&gt;Usually at least 1 SQL question + 1 R or Python question&lt;/li&gt;
&lt;li&gt;Familiarity with SQL AND R or Python is required for Senior Data Scientist.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Determine familiarity of advanced analytical functions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/xianyun-mao-232ba916/&#34;&gt;Xianyun Mao&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1yr, 2.5 yrs previous companies&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Behavior modeling and predictive analytics&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;statistics-ab-testing-qiannan-yin-and-yuan-chai&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1:15-2:00 | Statistics &amp;amp; A/B Testing | | &lt;a href=&#34;https://www.linkedin.com/in/qiannan-y-10987ba4/&#34;&gt;Qiannan Yin&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/yuan-chai-11127047/&#34;&gt;Yuan Chai&lt;/a&gt;:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Design and analyze experimentations, understands basic stats concepts (e.g. hypothesis testing, mean, variance, probability distributions, sample size calculation, power calculation…) and how to apply them in a business setting.&lt;/li&gt;
&lt;li&gt;For Sr DS, must show an in-depth understanding of A/B testing is needed (e.g. multi-variant testing, sample size calculation, power…)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Asses knowledge of probability&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/qiannan-y-10987ba4/&#34;&gt;Qiannan Yin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2yrs&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistics PhD, worked on fMRI and agent based modeling&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/yuan-chai-11127047/&#34;&gt;Yuan Chai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2yrs, 2 yrs previous analytics&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data analyst with extensive experiences on increasing user engagement, improving product &amp;amp; campaign performance, and optimizing ROI with experimental (A/B test, MVT) &amp;amp; exploratory research (classification/clustering/regression).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;analytics-case-study-gigi-zhang&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2:00-2:45 | Analytics Case Study | &lt;a href=&#34;https://www.linkedin.com/in/gigizhezhang/&#34;&gt;Gigi Zhang&lt;/a&gt;:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To assess ability to solve a business case with the right analytical approach and reasonable data intuition; and the ability to make relevant product recommendation based on data insights. _ Asses your familiarity with the LinkedIn Platform&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ability to correctly communicate and present solutions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/gigizhezhang/&#34;&gt;Gigi Zhang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;7months, around 3 years prior consulting experience&lt;/li&gt;
&lt;li&gt;Insight Fellow 2017&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Strong in ML&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LinkedIn Metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VPI (views per impression) for a job recommendation&lt;/li&gt;
&lt;li&gt;API (applications per impression) for a job application&lt;/li&gt;
&lt;li&gt;Job fits: Person-Environment (P-E) – comprised of Person-Organization (P-O), Person-Vocation (P-V), Person-Group (P-G), and Person-Job (P-J) fits&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;users&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Users&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Businesses, recruiters&lt;/li&gt;
&lt;li&gt;Content creators&lt;/li&gt;
&lt;li&gt;Job seekers&lt;/li&gt;
&lt;li&gt;Students picking a college&lt;/li&gt;
&lt;li&gt;College admin seeking alumni networks&lt;/li&gt;
&lt;li&gt;Networkers, wanting to keep in touch with classmates and coleagues&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;current-issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Current issues&lt;/h3&gt;
&lt;p&gt;GROWTH: LinkedIn is now backed by microsoft, wants to develop new products, and acquire more users – particularly those in underserved segments like blue collar labor.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ideas-and-excitement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ideas and excitement&lt;/h3&gt;
&lt;p&gt;Ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For blue collar workers, again become a marketplace, somewhere where contractors (blue collar) can meet with union workers and bid on contracts, similar to Yelp’s segment for contractors.&lt;/li&gt;
&lt;li&gt;The AirBnb or Uber of consulting / freelance work. I’ve been interested in consulting work. LinkedIn seems to be in an excellent position to become a platform or marketplace for consultants or freelance work.&lt;/li&gt;
&lt;li&gt;market to lower level education, which LinkedIn is already working on with their student future products (students can see what colleges are associated with careers and companies they like).&lt;/li&gt;
&lt;li&gt;Insight, lots of other interview prep that LinkedIn might be able to assist with.. Eg resume review verticals, behavioral interview questions, mock interviewing services, Coding practice like LeetCode (see that this is being worked in with skill quizes).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Excitement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I like their general work on &lt;a href=&#34;https://engineering.linkedin.com/teams/data/projects/jobs&#34;&gt;inferring user attributes&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economicgraph.linkedin.com/research#all&#34;&gt;economic research&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.media.mit.edu/~msaveski/assets/publications/2017_detecting_network_effects/paper.pdf&#34;&gt;experimentation research&lt;/a&gt; (also &lt;a href=&#34;https://dl.acm.org/citation.cfm?id=2488509&amp;amp;dl=ACM&amp;amp;coll=DL&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://engineering.linkedin.com/blog/2019/06/detecting-interference--an-a-b-test-of-a-b-tests&#34;&gt;here&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/citation.cfm?id=2741126&#34;&gt;latent user research&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;excitement-from-recruiter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Excitement from Recruiter&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2015/11/video--higher-ed-team-seeks-to-predict-collegiate-futures&#34;&gt;Student Future&lt;/a&gt;, a product to help high school students choose universities, based on entering a company and a role and seeing statistics about the schools that people with those jobs attended.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.linkedin.com/2015/07/29/inday-investing-in-our-employees-so-they-can-invest-in-themselves&#34;&gt;InDay&lt;/a&gt; – LinkedIn officially takes one day a month to allow employees to give back to the community, company, or themselves in some (themed) way.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economicgraph.linkedin.com/resources/linkedin-workforce-report-may-2018&#34;&gt;Economics graph&lt;/a&gt; project that gives reports on skills gaps and migrations.&lt;/li&gt;
&lt;li&gt;I noticed the job ad emails that I get seem based on my job search, possibly more-so than my skills.. Do you think you’d get more retention / engagement by recommending jobs based on skills or degrees and work history, rather than past searches? Or at least exploring this – randomizing emails to suggest jobs based on search vs based on skills and experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;onsite&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Onsite&lt;/h2&gt;
&lt;div id=&#34;glassdoor-interview-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Glassdoor interview questions&lt;/h3&gt;
&lt;p&gt;Given a random generator that produces a number 1 to 5 uniformly, write a function that produces a number from 1 to 7 uniformly or something like that.&lt;/p&gt;
&lt;p&gt;Solution 1: &lt;span class=&#34;math inline&#34;&gt;\(x^{log(7)/log(5)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Solution 2:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from random import randint

def rand5():
    return randint(1,5)

# generate random integers from 1 to num
def rand7(num):
    r=0
    for i in range(num):
        r += rand5()
    return r % num +1

b= np.zeros(100000)
for count in range(100000):
    b[count] = rand7(7)
_= plt.hist(b) 
Counter(b)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;How many active members in LinkedIn right now? What is the business model at LinkedIn? Basic SQL as long as you finished most of questions in Hackerrank.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;design recommendation engine for jobs&lt;/li&gt;
&lt;li&gt;How can you help to improve sales given a LinkedIn database&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Talk about a product that you wanna build at linkedin&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Product analytics case study around Linkedin profile&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;There is a table of page view data. Two columns: Member ID and time stamp. You can assume this is for a 24 hour period. Members vs. guests/visitors are differentiable via ID’s. Have to come up with a method for determining which all activities are crawler related (and not my members) and write a query for the same as well.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Using a set of tables, how would we find out whether users have a certain feature on or off?&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;What is the optimization problem for a SVM?&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;How many lines do you think a users’ daily login table has? And a general SQL question about joining two table with some conditions.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;What product metrics do you construct? How to tell if your experiment is successful?&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;I was asked to come up with some of the factors that could be used to produce certain algorithms (‘people you may know,’ and an algorithm to discover when a person is starting to search for new job).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The Interviewer ask the running up staircase with n steps quesion which need to get how many possible ways to run up the stairs. Each time you can hop either 1 step, 2 steps or 3 steps at a time. In addition, he asks some question about my research such as graph algorithms.&lt;/li&gt;
&lt;li&gt;I had 3 phone interviews with Linkedin data scientists. Everyone I spoke to was very bright, thoughtful and polite and made a great impression. On the first two calls I was asked how I would solve some of the problems Linkedin worked on (ex: people you may know) and on the third call I had a more technical coding interview. However I didn’t have a machine learning background at the time, so they did not give me an offer but encouraged me to e-apply after I had taken some of the machine learning classes I was planning to take the next semester. Considering how little machine learning I knew at the time it’s a wonder they even gave me a first interview, let alone 3.&lt;/li&gt;
&lt;li&gt;There is a significant increase in linkedin signups in this month. How much of this will you attribute to the changes you made in signup process. What data-points would you look into to confirm/deny this?&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Implement pow function.&lt;/li&gt;
&lt;li&gt;Segment a long string into a set of valid words using a dictionary. Return false if the string cannot be segmented. What is the complexity of your solution?&lt;/li&gt;
&lt;li&gt;find out k most frequent numbers from incoming stream of numbers one the fly.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;generating a sorted vector from two sorted vectors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Seemed every interviewer asked a variant of an A/B test for the homepage.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;design A/B test&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;SQL, Coding (R/Python), Product Case, Machine Learning, Stats&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;A few simple SQL queries with joins.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Two questions were asked, both about searching in sorted arrays of numbers. The second interview was almost identical, except that the question was more about algorithm design, which required general problem solving skill.&lt;/li&gt;
&lt;li&gt;I don’t recall exactly, but they asked about how I would approach several scenarios. This would be very difficult to answer well if you have not worked at a similar company and know what sorts of data are available to answer the question.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;basic data mining questions, including the concepts of classification and clustering; and a simple dp question which is quite similar to “Climbing Stairs”, and failed the second one right after I came back from another state (basic nlp questions, like named entity extraction, and basic data mining questions, like SVM, naive bayes; and a sampling question which is quite similar to Reservoir sampling).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The first interview was to test my SQL/R coding skills. The second interview was around experimenation - A/B testing, metrics etc., The third interview was on a bit of probability, machine learning techniques . The final one was with director of data science on a case study related to some linkedin scenarios and overall career interests. Even though all the interviews were set-up without any delay, what surprised me is that I didn’t receive any response from the HR after all the interviews. I followed up for a month or so multiple times, but I received absolutely no response, which I have no clue why.&lt;/li&gt;
&lt;li&gt;Questions around current / previous experience and education.&lt;/li&gt;
&lt;li&gt;Important metrics to think of in relation to LinkedIn data, to be included in dashboards available to executive level.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Questions around SQL, R, and Python.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;knowledge-gaps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Knowledge gaps&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Lots of questions about the platform&lt;/li&gt;
&lt;li&gt;Causal inference: how to construct a synthetic control, e.g. for estimating from historical data the impact of a email nudge on different user behaviors&lt;/li&gt;
&lt;li&gt;Advantages / disadvantages of different ML models (why logistic regression? How would you interpret the coefficients in a logistic regression model?)&lt;/li&gt;
&lt;li&gt;Causal inference: How to identify who would be successfully nudged by an email, ie a decision system for sending an email nudge that doesn’t feel like spam.&lt;/li&gt;
&lt;li&gt;SQL, Python: counting ids with more than 1 null&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Next steps&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;LinkedIn messages – her work sounded exciting. Mention finding some research papers from LinkedIn on user modeling, and ask for possibly more papers to look into!&lt;/li&gt;
&lt;li&gt;C:&lt;/li&gt;
&lt;li&gt;J:&lt;/li&gt;
&lt;li&gt;Recruiters:&lt;/li&gt;
&lt;li&gt;Send thank you email to recruiters&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Yelp Notes</title>
      <link>/MacStrelioff/insightstudying/yelp-deep-dive/</link>
      <pubDate>Fri, 09 Aug 2019 10:16:30 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/yelp-deep-dive/</guid>
      <description>


&lt;div id=&#34;todo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TODO:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;compute common business metrics from Yelp dataset.&lt;/li&gt;
&lt;li&gt;Product deep dive&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;company-visit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Company Visit&lt;/h1&gt;
&lt;div id=&#34;business-model-culture-and-vision&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Business Model, Culture, and Vision&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Mission: Connect people with great local places&lt;/li&gt;
&lt;li&gt;Main source of revenue is business ads, &lt;a href=&#34;https://www.thedailybeast.com/yelps-new-ad-products-can-increase-potential-customer-engagement-by-38&#34;&gt;overview of offerings here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biz.yelp.com/guide_to_success&#34;&gt;More services for businesses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Want: openness, curiosity, approachable&lt;/li&gt;
&lt;li&gt;DS team has “Office hours” to facilitate communication&lt;/li&gt;
&lt;li&gt;Journal clubs and learning groups&lt;/li&gt;
&lt;li&gt;Can move to PM or ML teams&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;projects-tasks-and-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Projects, Tasks, and Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Working on personalization&lt;/li&gt;
&lt;li&gt;e.g. project: decrease experiment duration&lt;/li&gt;
&lt;li&gt;not much collaboration with UX other than maybe getting data from them&lt;/li&gt;
&lt;li&gt;data: crowd-sourced data on businesses&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ds-role&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;DS Role&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Looking for 2 roles&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;DS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Job consists of reining in management, helping with experimentation. Common deliverables are a Jupyter notebook and reports.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deliverables are reports and Jupyter notebooks&lt;/li&gt;
&lt;li&gt;More confident decisions based on data&lt;/li&gt;
&lt;li&gt;Investigate and define metrics&lt;/li&gt;
&lt;li&gt;Design and analyze experiments on centralized platform.&lt;/li&gt;
&lt;li&gt;Helps with scaling up experiments and interpreting results.&lt;/li&gt;
&lt;li&gt;Predictive modeling&lt;/li&gt;
&lt;li&gt;Qualities: Generalist, communication, statistical inference, experimental design, empathy of understanding of other roles in the business&lt;/li&gt;
&lt;li&gt;Skills: SQL, Python (possibly R for analysis), clean code, reproducible results.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;ML&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Back end of production models&lt;/li&gt;
&lt;li&gt;Assists DE team with feature engineering&lt;/li&gt;
&lt;li&gt;Want: ML, DE, Python, Java, CS and mathematics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recruiter-call&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recruiter Call&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Q: why did you apply to Yelp? Show enthusiasm for yelp during interview.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interview topics and evaluation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Technical skills (analysis, stats, programming)&lt;/li&gt;
&lt;li&gt;product/bus intuition – impact for business opportunities&lt;/li&gt;
&lt;li&gt;communication – clarity and insight&lt;/li&gt;
&lt;li&gt;Can discuss past work with large data or complicated work&lt;/li&gt;
&lt;li&gt;Might get metrics or experimentation based on questions Yelp is facing at the moment&lt;/li&gt;
&lt;li&gt;A/B testing, Python&lt;/li&gt;
&lt;li&gt;Enthusiasm for company – why yelp versus another company&lt;/li&gt;
&lt;li&gt;Communicate while going along while solving the problems.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Other info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Team: ~ 600 engs, 70 PMs, 20 DS.&lt;/li&gt;
&lt;li&gt;DS mostly work with feature team, to create, calculate, and validate metrics for feature work. Modeling user engagement, predicting delivery time,&lt;/li&gt;
&lt;li&gt;DS works on sub team – Bus, consumer, core&lt;/li&gt;
&lt;li&gt;DS works with product groups, growth, contributions, ads.&lt;/li&gt;
&lt;li&gt;PM prioritizes projects based on impact.&lt;/li&gt;
&lt;li&gt;DS Helps with A/B testing, experimentation.. Looking to solve&lt;/li&gt;
&lt;li&gt;Can work with 1-3PMs within group. 50-70% of time working on projects for product groups. Most time is getting and giving feedback on approach to specific problems.&lt;/li&gt;
&lt;li&gt;Expert on knowing the data we can work with to address PM problems.&lt;/li&gt;
&lt;li&gt;Measure “goodness” of a feature in scientific and principled way.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What role am I being considered for?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Don’t have anything posted on careers page. This role just opened. Role just opened up. Data scientist rec.&lt;/p&gt;
&lt;p&gt;can send panel and links to their linkedin – meeting with four folks. &lt;a href=&#34;https://www.linkedin.com/in/sebastiencouvidat/&#34;&gt;Sebastian&lt;/a&gt;, Inhan (maybe &lt;a href=&#34;https://www.linkedin.com/in/yinghanfu/&#34;&gt;Yinghan Fu&lt;/a&gt;?), Nick (maybe &lt;a href=&#34;https://www.linkedin.com/in/nicholasmalecek/&#34;&gt;Nick&lt;/a&gt;), &lt;a href=&#34;https://www.linkedin.com/in/peter-weir/&#34;&gt;Peter&lt;/a&gt;.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What topics might be covered?&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Should I bring anything? Laptop, … ? Could bring laptop, but there are laptops that can be borrowed as well. Whiteboards will be in the room.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;likely-interviewers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likely Interviewers:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/sebastiencouvidat/&#34;&gt;Sebastian&lt;/a&gt;,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lead DS, 4y 4m at Yelp&lt;/li&gt;
&lt;li&gt;10 years in research at Stanford&lt;/li&gt;
&lt;li&gt;Astrophysics postdoc&lt;/li&gt;
&lt;li&gt;Coursera courses on finance and computational investing&lt;/li&gt;
&lt;li&gt;Yelp Job responsibilities:&lt;/li&gt;
&lt;li&gt;Data scientist in the consumer and mobile products division. Examples of activities include:&lt;/li&gt;
&lt;li&gt;helping product managers and decision makers to reach fact-based decision&lt;/li&gt;
&lt;li&gt;providing support to engineering teams for A/B test design and analysis&lt;/li&gt;
&lt;li&gt;writing Python scripts to perform frequentist and Bayesian analysis of A/B tests&lt;/li&gt;
&lt;li&gt;using machine-learning algorithms to predict revenue metrics and identify leads for sales force&lt;/li&gt;
&lt;li&gt;providing support to PR team for data-driven newspaper articles, blogs, news releases, etc.&lt;/li&gt;
&lt;li&gt;testing the integrity and accuracy of some curated data in the company databases&lt;/li&gt;
&lt;li&gt;implementing dashboard solutions for product managers for easy access to, and visualization of, relevant metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nick (maybe &lt;a href=&#34;https://www.linkedin.com/in/nicholasmalecek/&#34;&gt;Nick&lt;/a&gt;),&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DS Manager, 3y8m at Yelp&lt;/li&gt;
&lt;li&gt;Insight&lt;/li&gt;
&lt;li&gt;Neuroscience UT Austin&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/peter-weir/&#34;&gt;Peter&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neuroscience at Caltech&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inhan (maybe &lt;a href=&#34;https://www.linkedin.com/in/yinghanfu/&#34;&gt;Yinghan Fu&lt;/a&gt;?),&lt;/p&gt;
&lt;p&gt;Another DS person: &lt;a href=&#34;https://www.linkedin.com/in/samuelhansen/&#34;&gt;Samuel Hansen&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Member of Core Data Science team, supporting:&lt;/li&gt;
&lt;li&gt;Yelp’s Local Economic Outlook (LEO) initiative to derive measures of economic health from Yelp data;&lt;/li&gt;
&lt;li&gt;Data infrastructure improvements to optimize analysis workflows.&lt;/li&gt;
&lt;li&gt;Developed pipeline to predict Yelp Transactions Platform user retention by scripting SQL queries, engineering longitudinal features, and implementing machine learning classifiers, lifting AUC 10% over the baseline model.&lt;/li&gt;
&lt;li&gt;Constructed behavioral segmentation of Yelp Transactions Platform users to understand food-ordering patterns with K-means and factor analysis, primarily identifying user clusters using mealtime, delivery type, and food category.&lt;/li&gt;
&lt;li&gt;Built RShiny application with interactive geographical map of spatial-temporal changes in Yelp activity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another DS person: &lt;a href=&#34;https://www.linkedin.com/in/wolfe-styke-98108920/&#34;&gt;Wolfe Styke&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;switched from SWE to DS&lt;/li&gt;
&lt;li&gt;Continuing involvement with new experiment framework at Yelp from a data science perspective.&lt;/li&gt;
&lt;li&gt;Defining, implementing, and spreading awareness among product managers and engineers of user retention analysis for experiments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another DS person: &lt;a href=&#34;https://www.linkedin.com/in/yang-song-516350b2/&#34;&gt;Yang Song&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I work at the marketplace team, products include Yelp Waitlist, Yelp Reservations, Yelp Request a Quote and Yelp Local Services&lt;/p&gt;
&lt;p&gt;Projects I have done: - Built ML models on waitlist waiting time estimation - Various of observational causal inferences projects (User Behavior, Biz page features, Quotes qualities) - Geo experiment(For ads fulfillment, CPC, CTR) - Randomized A/B Testing - Sequential A/B Testing (Which allow PMs to peek A/B testing results) - Local services categories recommendation&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recruiter-prep-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recruiter prep info&lt;/h1&gt;
&lt;div id=&#34;schedule-and-interviewers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Schedule and Interviewers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;12:45PM- Greet (Fahren)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;pm--sebastien-couvidat&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1:00PM- &lt;a href=&#34;https://www.linkedin.com/in/sebastiencouvidat/&#34;&gt;Sebastien Couvidat&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lead DS, 4y 4m at Yelp&lt;/li&gt;
&lt;li&gt;10 years in research at Stanford&lt;/li&gt;
&lt;li&gt;Astrophysics postdoc&lt;/li&gt;
&lt;li&gt;*Coursera courses on finance and computational investing&lt;/li&gt;
&lt;li&gt;Yelp Job responsibilities:&lt;/li&gt;
&lt;li&gt;Data scientist in the consumer and mobile products division. Examples of activities include:&lt;/li&gt;
&lt;li&gt;helping product managers and decision makers to reach fact-based decision&lt;/li&gt;
&lt;li&gt;providing support to engineering teams for A/B test design and analysis&lt;/li&gt;
&lt;li&gt;** writing Python scripts to perform frequentist and Bayesian analysis of A/B tests&lt;/li&gt;
&lt;li&gt;using machine-learning algorithms to predict revenue metrics and identify leads for sales force&lt;/li&gt;
&lt;li&gt;providing support to PR team for data-driven newspaper articles, blogs, news releases, etc.&lt;/li&gt;
&lt;li&gt;testing the integrity and accuracy of some curated data in the company databases&lt;/li&gt;
&lt;li&gt;implementing dashboard solutions for product managers for easy access to, and visualization of, relevant metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm--yinghan-fu&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1:45PM- &lt;a href=&#34;https://www.linkedin.com/in/yinghanfu/&#34;&gt;Yinghan Fu&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Physics and biophysics&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm--nicholas-malecek&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2:30PM- &lt;a href=&#34;https://www.linkedin.com/in/nicholasmalecek/&#34;&gt;Nicholas Malecek&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cog sci and Neurosci background!&lt;/li&gt;
&lt;li&gt;google scholar citations are neuroeconomics papers!!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm--peter-weir&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3:15PM- &lt;a href=&#34;https://www.linkedin.com/in/peter-weir/&#34;&gt;Peter Weir&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;“Sell in May” Mode analysis&lt;/li&gt;
&lt;li&gt;Neuroscience at Caltech&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pm--goodbye-fahren&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4:00PM- Goodbye (Fahren)&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prep-tips&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prep Tips&lt;/h2&gt;
&lt;p&gt;To prepare for your interview, you may want to review some probability, data analysis, SQL, coding, and experimental design. Be prepared to discuss your past work involving analyzing large and complicated datasets, defending your approaches and communicating what you learned during your project. You have a good chance of getting a product metrics or experimentation question based on some actual questions Yelp is tackling at the moment.&lt;/p&gt;
&lt;p&gt;If you have time before the interview, check out Yelp’s &lt;a href=&#34;https://engineeringblog.yelp.com/&#34;&gt;engineering&lt;/a&gt; and &lt;a href=&#34;https://blog.yelp.com/section/data&#34;&gt;data&lt;/a&gt; blogs and see if anything’s relevant. Be familiar with A/B testing and brush up on your Python (Yelpers love Python) and/or R abilities to be ready for a live data analysis problem.&lt;/p&gt;
&lt;p&gt;And finally, of course, follow the general interview advice: elaborate on related projects from your resume, be enthusiastic, share your thoughts with your interviewer as you’re going through a problem or doing a piece of analysis, and be sure to ask questions for clarification!&lt;/p&gt;
&lt;p&gt;The team will also focus on why you want to be a part of the Yelp team! – look into &lt;a href=&#34;https://www.yelp.com/factsheet&#34;&gt;Metrics Fact sheet&lt;/a&gt; and &lt;a href=&#34;https://www.yelp.com/careers/who-we-are&#34;&gt;Culture&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;discussion-of-past-work&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Discussion of past work&lt;/h3&gt;
&lt;p&gt;“Be prepared to discuss your past work involving analyzing large and complicated datasets, defending your approaches and communicating what you learned during your project.”&lt;/p&gt;
&lt;p&gt;“elaborate on related projects from your resume”&lt;/p&gt;
&lt;p&gt;PredictIt Project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Situation:&lt;/li&gt;
&lt;li&gt;Project:&lt;/li&gt;
&lt;li&gt;Maybe sketch the fig of price for Trump win versus Republican win&lt;/li&gt;
&lt;li&gt;I also aggregated data based on days until expiration, and applied a linear in log odds model at each day.&lt;/li&gt;
&lt;li&gt;Found the common S shaped calibration curve that comes up in lab experiments, particularly at 1-2 days before market expiration.&lt;/li&gt;
&lt;li&gt;Result:&lt;/li&gt;
&lt;li&gt;Value to Yelp: Relates to the economic patterns maybe?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Strategy Inference Project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Situation:&lt;/li&gt;
&lt;li&gt;Project:&lt;/li&gt;
&lt;li&gt;Children and adults might use different strategies when making decisions.&lt;/li&gt;
&lt;li&gt;I worked with a friend to formalize the possible set of strategies, and then I implemented a model to infer the probability of a kid following each strategy on each trial, and applied Bayes Theorem to get a probability that the kid was following any strategy over time.&lt;/li&gt;
&lt;li&gt;Result: A lot of experiments focus on which strategy best fits all data across trials – the approach that I took allows for changes in strategies, and the exciting result was an ability to track latent decision making strategies over time.&lt;/li&gt;
&lt;li&gt;Value to Yelp: At yelp, for example, a user might be choosing restaurants based primarily on value, then shift to preferring restaurants based on rating. An analogous modeling approach might be able to detect, in near real time, the features that are driving an individual user’s decisions. This informaiton could then be used to modify search rankings.&lt;/li&gt;
&lt;li&gt;More specifically, model click probability as a function of value and stars, estimate the coefficients online, and use the coefficients to prioritize these features in rankings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;General interest in decision making:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Situation: Models of multi-alternative decision making are often applied to food preferences to see how people integrate information about price, healthyness, flavor, … .&lt;/li&gt;
&lt;li&gt;Excitement: Yelp is basically a giant experiment in multiattribute decision making.&lt;/li&gt;
&lt;li&gt;Value to Yelp: Theoretical insights might help in guiding how restaurant informaiton is displayed to consumers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Insight Project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User funnel and user flow, maybe should have started at these simpler analyses&lt;/li&gt;
&lt;li&gt;Learned lots about decision trees, random forests versus gradient boosting and gini impurity versus mean decreas in accuracy&lt;/li&gt;
&lt;li&gt;Chose gini impurity because mean decrease in accuracy labels collinear features as unimportant&lt;/li&gt;
&lt;li&gt;Chose random forests because it does a better job of pulling out important collinear features&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;product-metrics-or-experimentation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Product metrics or experimentation&lt;/h3&gt;
&lt;p&gt;“You have a good chance of getting a product metrics or experimentation question based on some actual questions Yelp is tackling at the moment.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;General questions:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One of Yelp’s values is Tenaciousness – can you give some examples of how that manefists on a data science team?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-dive&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Deep Dive&lt;/h1&gt;
&lt;div id=&#34;stakeholders-and-use-cases&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stakeholders and Use Cases&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Consumers: Use Yelp as a recommendation engine based on crown-sourced reviews, and as a platform for expression through reviews&lt;/li&gt;
&lt;li&gt;Businesses: Use Yelp to gain clients&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;user-flows-and-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;User Flows and Metrics&lt;/h2&gt;
&lt;p&gt;Thinking of the broader user flow can help with identifying measurable events within that flow (metrics). Overall, consider units of individuals (accounts, unique visitors, cookies, IPs…), units of time (daily, weekly), and referents (week over week, …)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;good source for some key &lt;a href=&#34;https://www.yelp.com/knowledge&#34;&gt;metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;on-web&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On Web&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Users (IPs) arrive on Yelp, can be assigned a cookie, or associated with an account if the user creates one.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Growth: DAU, MAU&lt;/li&gt;
&lt;li&gt;Growth: monthly new users, monthly unique users&lt;/li&gt;
&lt;li&gt;Engagement: Reviews per user per month&lt;/li&gt;
&lt;li&gt;Engagement:&lt;/li&gt;
&lt;li&gt;Engagement: Traffic to business pages&lt;/li&gt;
&lt;li&gt;Monetization: Reservations&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Monetization: Waitlist&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Performance: Wait times at restaurants&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://biz.yelp.com/&#34;&gt;Some metrics for business partners&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;on-mobile&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On Mobile&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;current-issues&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Current issues&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Trust, businesses can game Yelp by &lt;a href=&#34;https://thehustle.co/botto-bistro-1-star-yelp/&#34;&gt;asking for low ratings&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Trust, Businesses accuse Yelp of &lt;a href=&#34;https://www.businessinsider.com/court-rules-yelp-can-manipulate-reviews-2014-9&#34;&gt;maliciously manipulating reviews&lt;/a&gt;, to such an extent that, in response to a documentary about this, Yelp redirected BillionDollerBully.com to &lt;a href=&#34;https://www.yelp.com/extortion&#34;&gt;this response&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Influence: “Such ubiquity has consequences. A study from last year found that a measly half-star difference made it 19% more likely that a San Francisco restaurant would be busy at peak times.” - &lt;a href=&#34;https://www.buzzfeed.com/sandraeallen/is-yelp-evil-or-just-misunderstood&#34;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lack of expected growth: “He cited a lack of growth from several of the site’s advertising efforts for his downgrade, as well as a lack of “consequential” new products “to reduce churn or drive revenue.” &lt;a href=&#34;https://www.barrons.com/articles/what-the-democratic-debates-mean-for-health-care-investors-51564665029&#34;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Balancing review usefulness, reliability, and legitimacy -&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;competitors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Competitors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;“On one extreme, Angie’s List features reviews of local products and services provided by and visible to its fee-paying members. On the other, Urbanspoon and CitySearch, Tripadvisor, and Amazon don’t really scrutinize reviewers or reviews.” - &lt;a href=&#34;https://www.buzzfeed.com/sandraeallen/is-yelp-evil-or-just-misunderstood&#34;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Full paragraph: “For better and for worse, strangers’ opinions affect our purchasing behavior to an unprecedented degree. Such feedback, of course, is easily corrupted. Websites like Yelp face the problem of whether and how to scrutinize the data they receive to ensure reliability. On one extreme, Angie’s List features reviews of local products and services provided by and visible to its fee-paying members. On the other, Urbanspoon and CitySearch, Tripadvisor, and Amazon don’t really scrutinize reviewers or reviews. While more democratic, they’re also much more susceptible to manipulation and therefore theoretically less useful to consumers. If a business owner or her friend or someone she pays can pose as a happy customer — or sabotage her competition — a review is no more useful than an ad.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ideas-excitement-questions-to-ask&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ideas, excitement, questions to ask&lt;/h2&gt;
&lt;p&gt;Ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I like that they have &lt;a href=&#34;https://blog.yelp.com/2015/06/data-science-challenge-predict-restaurant-health-scores-with-yelp-data&#34;&gt;data challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;I like their openness with data, particularly the &lt;a href=&#34;https://www.kaggle.com/datasets?search=yelp&#34;&gt;datasets on Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Excitement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chek relationship between restaurant response time to critical reviews, and other restaurant demand metrics like reservations or ratings.&lt;/li&gt;
&lt;li&gt;check for predictive relationship between reservations or delivery orders made in a quarter, and a conglomerate’s or restaurant’s revenue for that quarter?&lt;/li&gt;
&lt;li&gt;Since a restaurant’s revenue is basically customers * customer value, if Yelp reservations is a good proxy for number of customers, then it might also be a good proxy for quarterly revenue.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are product decisions made with p-values? .. What thresholds do you use in practice?&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;onsite&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Onsite&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Expect from company visit: SQL, Python, statistics, experiment design&lt;/li&gt;
&lt;li&gt;Expect from recruiter call: A/B testing, product sense&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;glassdoor-interview-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Glassdoor interview questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How to evaluate the features classifying spams and emails.&lt;/li&gt;
&lt;li&gt;How to swap two elements of a list&lt;/li&gt;
&lt;li&gt;Whats the difference between decision trees and extreme boosted trees mathematically?&lt;/li&gt;
&lt;li&gt;List the strings that are anagrams from a set of strings&lt;/li&gt;
&lt;li&gt;Mid-level coding/algorithm questions; ask you to explain the projects you’ve worked on.&lt;/li&gt;
&lt;li&gt;about my project on resume.&lt;/li&gt;
&lt;li&gt;3 Sum&lt;/li&gt;
&lt;li&gt;What programming language do you use? What are some pros and cons of it?&lt;/li&gt;
&lt;li&gt;The difference between Bayesian vs frequentist statistics&lt;/li&gt;
&lt;li&gt;Setting up A/B tests to improve engagement&lt;/li&gt;
&lt;li&gt;What features would you use to predict the number of likes on a review?&lt;/li&gt;
&lt;li&gt;questions around product/metrics, A/B Testing, stats, ML&lt;/li&gt;
&lt;li&gt;If you had to propose a new Yelp feature, what would it be?&lt;/li&gt;
&lt;li&gt;What is an n-gram?&lt;/li&gt;
&lt;li&gt;Simple coding and high level ML questions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/MacStrelioff/data-science/working-environment/</link>
      <pubDate>Sun, 04 Aug 2019 17:33:43 +0000</pubDate>
      
      <guid>/MacStrelioff/data-science/working-environment/</guid>
      <description>


&lt;!---
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/LgYl1ffS_6Y&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
---&gt;
&lt;div id=&#34;working-environment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Working Environment&lt;/h1&gt;
&lt;p&gt;My working environment relies on bash and git. Here I provide some background on these tools and a walkthrough for automatic, scheduled pushes to git.&lt;/p&gt;
&lt;div id=&#34;working-with-bash-and-git&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working With Bash and Git&lt;/h2&gt;
&lt;div id=&#34;useful-bash-commands&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Useful Bash Commands&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;login     # prompts for username and then password to log in
man       # man (command) returns documentation a command&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;directory-commands&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Directory Commands&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;pwd       # print working directory
cd        # changes directory to home
cd \      # changes directory to (/path)
cd ..     # moves up one folder in the directory
ls -al    # lists contencts of directory, -a includes hidden items, -l includes details&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;file-operations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;File Operations&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;mkdir    # make a new directory
touch    # create a file
open     # open a file
mv       # move (file) (to), can also be used to rename a file
cp       # copy (file) (to) 
rm       # delete a file&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-utilities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other Utilities&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;echo    # print commands screen
date    # print date
cal     # print calendar (year)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-custom-commands&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating Custom Commands&lt;/h2&gt;
&lt;p&gt;This section is partially based on &lt;a href=&#34;https://medium.com/devnetwork/how-to-create-your-own-custom-terminal-commands-c5008782a78e&#34;&gt;this Medium blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use the alias command to map an execution of bash commands to a single command. For example, this will create a command ‘hi’ where the Luca voice slowly says ‘hello world’;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;alias hi=&amp;quot;say -v Luca  -r 100 &amp;#39;hello world&amp;#39;&amp;quot;
hi # test this alias&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Functions can be created with the following syntax;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;function function_name() {
 &amp;lt;args&amp;gt; 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inputs are denoted with $1, $2, … . For example, when called from the terminal, this function will print ‘Your Input Was:’ followed by the first argument it receives;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;function print_my_input() {
 echo &amp;#39;Your Input Was:&amp;#39; $1 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a file where custom commands are defined, navigate to the home directory, create, and open the file that will hold function definitions;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd 
touch .custom_commands
open .custom_commands&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define commands you wish to use in future Terminal sessions in the .custom_commands file. The next section describes how to source this file automatically when a Terminal session is initialized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sourcing-files-during-terminal-initialization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sourcing Files During Terminal Initialization&lt;/h2&gt;
&lt;p&gt;This section is partially based on &lt;a href=&#34;https://stackoverflow.com/questions/19662713/where-do-i-find-the-bashrc-file-on-mac&#34;&gt;this StackOverflow post&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bash sources a run control file (~/.bashrc) at initialization. Check if this file exists in the home directory with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd
ls -a&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If there is no .bashrc file listed, then first open the .bash_profile file in a text editor and add the following;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then create and open a .bashrc file in the home directory with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd
touch .bashrc
open .bashrc&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a text editor add the following lines to the .bashrc file;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Source global definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
fi

# source file with function definitions
source ~/.custom_commands&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This last line will source the .custom_commands file described at the end of the previous section, thereby defining any aliases or functions in the .custom_commands file whenever a Terminal session is initialized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scheduling-bash-functions-with-crontab&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling Bash Functions With Crontab&lt;/h2&gt;
&lt;p&gt;Some basic crontab commands;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;crontab -l # lists crontab jobs
crontab -r # removes all crontab jobs
crontab -e # edits list of crontab jobs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The crontab -e command is somewhat hard to navigate, the following code is an easier way to create crontab jobs from ther terminal;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;crontab -l | { cat; echo &amp;quot;* * * * * some entry&amp;quot;; } | crontab -

# format for string argument for crontab;
* * * * * &amp;quot;some entry&amp;quot;
- - - - -
| | | | |
| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)
| | | ------- Month (1 - 12)
| | --------- Day of month (1 - 31)
| ----------- Hour (0 - 23)
------------- Minute (0 - 59)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I simplify this farther in a custom function called ‘schedule’ that I define in .&lt;/p&gt;
&lt;p&gt;To execute functions from a user, the user must be specified in /etc/cron.allow – however, this file can only be edited by a superuser. To create and edit this file, use;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd /etc
sudo touch cron.allow
sudo nano cron.allow&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nano opens a text editor in ther terminal, and that can be used to edit and save changes to cron.allow.&lt;/p&gt;
&lt;p&gt;More information on crontab &lt;a href=&#34;https://www.computerhope.com/unix/ucrontab.htm&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;git-and-github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Git and GitHub&lt;/h2&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create user name and email for tagging contributions later
git config --global user.name &amp;quot;USERNAME&amp;quot;   
git config --global user.email &amp;quot;EMAIL&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Connecting to Git can be done through a username/password or through SSH verification.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;usernamepassword&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Username/Password&lt;/h3&gt;
&lt;p&gt;This didn’t work for ubdates scheduled through crontab.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ssh&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SSH&lt;/h3&gt;
&lt;p&gt;First, &lt;a href=&#34;https://help.github.com/articles/checking-for-existing-ssh-keys/&#34;&gt;check for SSH keys with&lt;/a&gt;;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd ~/.ssh  # navigate to user/.ssh folder
ls -a      # list all files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A public SSH key is in id_rsa.pub, which can be opened in a text editor.&lt;/p&gt;
&lt;p&gt;If no id_rsa file exists, or if you wish to create a new SSH key, you can use the following command to generate a new SSH key;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ssh-keygen -t rsa -b 4096 -C &amp;quot;your_email@example.com&amp;quot; # run this if id_rsa file does not exist&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will prompt you for the file location where the SSH key should be saved – id_rsa is the default file.&lt;/p&gt;
&lt;p&gt;From here, follow the steps &lt;a href=&#34;https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/&#34;&gt;to add an SSH key&lt;/a&gt;, summarized below.&lt;/p&gt;
&lt;p&gt;First, create a config file for the SSH agent with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;touch ~/.ssh/config&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Open the config file in a text editor and add the the following commands;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Host *
 AddKeysToAgent yes
 UseKeychain yes
 IdentityFile ~/.ssh/id_rsa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, from Terminal, add the key from the file (e.g. id_rsa) to the ssh agent with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ssh-add -K ~/.ssh/id_rsa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, &lt;a href=&#34;https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/&#34;&gt;add the ssh key to the GitHub account&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And &lt;a href=&#34;https://help.github.com/articles/testing-your-ssh-connection/&#34;&gt;test the connection&lt;/a&gt; with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ssh -T git@github.com&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;working-from-a-repository&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Working From a Repository&lt;/h3&gt;
&lt;p&gt;Create a repository through GitHub’s user interface (log in and select ‘new repository’).&lt;/p&gt;
&lt;p&gt;Then create a directory on a local computer, possibly using bash commands; mkdir and cd.&lt;/p&gt;
&lt;p&gt;The simplest way is to clone the repo from GitHub with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git clone (url)     # this will clone from the url to the current directory&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way, from inside the directory that is to be the local repo, issue these commands;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git init   # initialize a repo
git remote add origin (url or SSH address) # point local repo to remote repo&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;switching-between-ssh-and-https-access&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Switching between SSH and HTTPS Access&lt;/h3&gt;
&lt;p&gt;In both examples below, replace ‘USERNAME/REPOSITORY’ with the appropriate values. From within the local repo folder,&lt;/p&gt;
&lt;p&gt;Change from HTTPS to SSH with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git remote set-url origin git@github.com:USERNAME/REPOSITORY.git
git remote -v # list fetch and push destinations, should start with git@github.com&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Change from SSH to HTTPS with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git remote set-url origin https://github.com/USERNAME/REPOSITORY.git
git remote -v # list fetch and push destinations, should start with https://github.com/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://help.github.com/articles/changing-a-remote-s-url/&#34;&gt;More detailed guide here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other methods for addressing ‘Device not configured’ errors &lt;a href=&#34;https://stackoverflow.com/questions/40274484/fatal-could-not-read-username-for-https-github-com-device-not-configured&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;repository-maintanence-and-collaboration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Repository Maintanence And Collaboration&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/MacStrelioff/y.jpg&#34; alt=&#34;Git workflow&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Git workflow&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Useful commands;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git pull    # adds files from the configured remote repo to your local repo and workspace
git add .   # adds all new files to be tracked
git add -u  # updates all files
git add -A  # does both &amp;#39;.&amp;#39; and &amp;#39;-u&amp;#39;
git commit -m &amp;quot;message&amp;quot; # saves changes local repo
git push    # updates repo on GitHub&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Collaborators can all access the GitHub repo through push/pull commands. In collaborative settings you might need to pull, so that your repository is current, before pushing changes from the local repository to the remote repository on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;useful-custom-aliases-and-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Useful Custom Aliases and Functions&lt;/h2&gt;
&lt;div id=&#34;gitup-combines-add-commit-and-push-into-one-command&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;gitup: combines add, commit, and push into one command&lt;/h4&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;alias gitup=&amp;quot;git add -A; git commit -m &amp;#39;auto&amp;#39;; git push&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gitupall-updates-all-git-repos-in-my-git-folder&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;gitupall: updates all git repos in my git folder&lt;/h4&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;function gitupall{
for filename in /Users/mac/git/*; do
echo &amp;quot;**************************updating $filename&amp;quot;
cd &amp;quot;$filename&amp;quot;; 
gitup
cd ..
done
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;schedule-schedules-a-command-to-be-executed-periodically&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;schedule: schedules a command to be executed periodically&lt;/h4&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;function schedule(){
crontab -l | { cat; echo &amp;quot;$1&amp;quot;; } | crontab -
}

# format for string argument for crontab;
* * * * * &amp;quot;command to be executed&amp;quot;
- - - - -
| | | | |
| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)
| | | ------- Month (1 - 12)
| | --------- Day of month (1 - 31)
| ----------- Hour (0 - 23)
------------- Minute (0 - 59)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example use;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# e.g. use: schedule updating of all repos each hour
schedule &amp;quot;0 0-23 * * * gitupall&amp;quot;
crontab -l # lists crontab jobs after schedule call&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;my-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My Setup&lt;/h2&gt;
&lt;div id=&#34;bash-aliases-functions-scheduled-commands&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bash Aliases, Functions, Scheduled Commands&lt;/h3&gt;
&lt;p&gt;In my root directory, I have a file called .custom_commands with contents;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# aliases

# goes to my folder of git projects
alias gitgo=&amp;quot;cd /Users/mac/git; ls&amp;quot;
echo &amp;#39;sourced alias gitgo&amp;#39;

# pushes updates on a git project
alias gitup=&amp;quot;git add -A; git commit -m &amp;#39;auto&amp;#39;; git push&amp;quot;
echo &amp;#39;sourced alias gitup&amp;#39;

# functions

# schedule &amp;quot;(m h DoM M DoW command)&amp;quot;
# schedules command to be executed at 
# minute m of hour h on DoM days of the month in months M or DoW days of the week.
function schedule(){
crontab -l | { cat; echo &amp;quot;$1&amp;quot;; } | crontab -
}
echo &amp;#39;sourced function schedule&amp;#39;

# for updating all git projects, schedule this to be done periodically
function gitupall(){
for filename in /Users/mac/git/*; do
echo &amp;quot;************************** updating $filename&amp;quot;
cd &amp;quot;$filename&amp;quot;;
git remote -v    # prints the fetch and push destinations
gitup
done
cd # return to home directory
}
echo &amp;#39;sourced function gitupall&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also have a .bashrc file in my root directory, which sources the custom commands. Particularly the contents of the .bashrc file are;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Source system-wide definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
fi

# source file with function definitions
source ~/.custom_commands&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also have a file in my root directory for scheduling, called .scheduled with contents;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# put all scheduled functions here

# source custom commands
source ~/.custom_commands

# activate key agent
eval &amp;quot;$(ssh-agent -s)&amp;quot;

# test SSH key / print username
ssh -T git@github.com

# update all git repos
gitupall&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then scheduled this file to be read hourly by opening a terminal session, which sources .bashrc at initialization thereby sourcing my custom commands, and running;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;schedule &amp;quot;0 0-23 * * * source ~/.scheduled&amp;quot;
crontab -l # the scheduled job should be listed as a crontab job&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Web Agent</title>
      <link>/MacStrelioff/data-science/cookie-clicker-bot/</link>
      <pubDate>Fri, 02 Aug 2019 15:59:29 +0000</pubDate>
      
      <guid>/MacStrelioff/data-science/cookie-clicker-bot/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background-and-setup&#34;&gt;Background and Setup&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cookie-clicker-game&#34;&gt;Cookie Clicker Game&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agents&#34;&gt;Agents&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#naive-buy-all-affordable-investments&#34;&gt;Naive: Buy all affordable investments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maxroi-buy-best-return-on-investment&#34;&gt;MaxROI: Buy best return on investment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minwait-buy-what-minimizes-the-time-to-the-highest-revenue-purchase&#34;&gt;MinWait: Buy what minimizes the time to the highest revenue purchase&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance&#34;&gt;Performance&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#revenue&#34;&gt;Revenue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-count&#34;&gt;Building Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-prices&#34;&gt;Building Prices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#return-on-investment-revenue-per-cost&#34;&gt;Return on Investment (Revenue Per Cost)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix:&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#minwait-decision-is-independent-of-balance&#34;&gt;MinWait Decision is Independent of Balance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#helper-functions&#34;&gt;Helper Functions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#browser-and-game-initialization&#34;&gt;Browser and Game Initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clicking-cookies&#34;&gt;Clicking Cookies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#purchasing-upgrades&#34;&gt;Purchasing Upgrades&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logging-balance-and-revenue&#34;&gt;Logging balance and revenue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#get-building-information&#34;&gt;Get building information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;!---
- Write post and tag https://twitter.com/Orteil42 and the cookie clicker twitter in final post
---&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Lately I’ve been interested in writing algorithms (agents) that interact with websites. The game &lt;a href=&#34;https://orteil.dashnet.org/cookieclicker/&#34;&gt;Cookie Clicker&lt;/a&gt; is a great testing ground for such algorithms. The game is played by clicking a big cooke to earn money (cookies) that can be used to invest in instruments (buy buildings), which in turn generate revenue (more cookies). Here I’ll describe three agents that I made for this game and assess their performance. I also describe many of the helper functions involved in implementing these agents. The code that implements these agents and reproduces all figures in this blog post can be found on my GitHub, here: &lt;a href=&#34;https://github.com/MacStrelioff/CookieClickerAgent&#34; class=&#34;uri&#34;&gt;https://github.com/MacStrelioff/CookieClickerAgent&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background-and-setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background and Setup&lt;/h1&gt;
&lt;div id=&#34;cookie-clicker-game&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cookie Clicker Game&lt;/h2&gt;
&lt;p&gt;The interface for &lt;a href=&#34;https://orteil.dashnet.org/cookieclicker/&#34;&gt;Cookie Clicker&lt;/a&gt; is shown below.&lt;/p&gt;
&lt;!--- UPDATE IMAGE POINTER TO ONE THAT IS CORRECT IN THE FILE STRUCTURE OF THE HOSTED FILES ---&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/CookieClickerGameEdited.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The goal of the game is to amass wealth in the form of cookies. Your balance and revenue are shown above the large cookie. Cookies are earned each time you click the large cookie and investments, shown at the bottom right, are unlocked as you acquire wealth. Purchasing an investment (Cursor, Grandma, …) provides recurring revenue in cookies per second. For any specific investment, the price of the next investment increases each time the investment is purchased. In developing the algorithms below I focused on maximizing revenue, which would maximize wealth over time.&lt;/p&gt;
&lt;p&gt;The game has other mechanics (upgrades, ascension, golden cookies, …). I programed functions to purchase upgrades and click golden cookies, but these functions were disabled in the analyses below to allow for a controlled comparison of the agents.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;agents&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Agents&lt;/h1&gt;
&lt;p&gt;While the goal was to earn as many cookies as possible, the game also allows revenue to accrue while the player is away, and the amount of cookies earned per click can scale with revenue. For these reasons, I focused on maximizing revenue as the overall goal for any strategy.&lt;/p&gt;
&lt;div id=&#34;naive-buy-all-affordable-investments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naive: Buy all affordable investments&lt;/h2&gt;
&lt;p&gt;The simplest investment strategy was to purchase any affordable building. This is implemented in the code below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class agent_class_naive:

...

def buy_products(self):
    products = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;)
    while products: # if there are affordable products, buy them
        products[-1].click()
        products = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;)
        
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;products&lt;/code&gt; is a list of the web elements that represent the affordable buildings, and the while loop cycles over them, buying the most expensive ones first with &lt;code&gt;products[-1].click()&lt;/code&gt;, until no more buildings are affordable. This purchasing logic is implemented through a &lt;code&gt;buy_products&lt;/code&gt; method of the naive agent, &lt;code&gt;agent_class_naive&lt;/code&gt;. In &lt;a href=&#34;https://github.com/MacStrelioff/CookieClickerAgent&#34;&gt;my code&lt;/a&gt;, the naive agent class also contained the necessary helper functions, described in the section above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maxroi-buy-best-return-on-investment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MaxROI: Buy best return on investment&lt;/h2&gt;
&lt;p&gt;The second strategy is to buy the option that will have the best return on investment (revenue to price ratio). The code below extends the naive agent, &lt;code&gt;agent_class_naive&lt;/code&gt;, by overriding the &lt;code&gt;buy_products&lt;/code&gt; method that implements the investment strategy.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class agent_class_max_rps_price_ratio(agent_class_naive):
    # overwrite the buy_products method
    def buy_products(self):
        ## update building info
        self.get_building_info()
        # while best is affordable, buy the best rps/price building
        best_building_affordable = True
        while best_building_affordable:
            ## get unlocked products
            products = (driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;) + 
                        driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked disabled&amp;quot;]&amp;#39;))
            # find max rps/price building
            max_rps_pp,building_to_buy,product_to_buy = 0,[],[]
            for i,building in enumerate(self.building_info):
                # get rps/price for building
                cur_rps_pp = self.building_info[building][&amp;#39;cps/price&amp;#39;] 
                # if it&amp;#39;s the best so far, update max and building id
                if cur_rps_pp &amp;gt; max_rps_pp:
                    max_rps_pp,building_to_buy = cur_rps_pp,building
                    product_to_buy = products[i] # store element to click
            # update balance
            self.log_balance_and_revenue()
            # check if best building is affordable.
            if self.building_info[building_to_buy][&amp;#39;price&amp;#39;]&amp;lt;=self.balance:
                # buy building_to_buy (click on this product)
                product_to_buy.click()
                # update building info (including rps per price rps_pp)
                self.get_building_info()
            else: best_building_affordable=False # if not affordable, break the loop  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First the MaxROI agent updates building information, including; cookies per second, price, and the ratio cps/price, using the helper method &lt;code&gt;get_building_info()&lt;/code&gt;. Then it purchases the buildings that provide the max ROI, until the best building by this metric is unaffordable. The search for the best building is implemented in the section:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# find max rps/price building
max_rps_pp,building_to_buy,product_to_buy = 0,[],[]
for i,building in enumerate(self.building_info):
    # get rps/price for building
    cur_rps_pp = self.building_info[building][&amp;#39;cps/price&amp;#39;] 
    # if it&amp;#39;s the best so far, update max and building id
    if cur_rps_pp &amp;gt; max_rps_pp:
        max_rps_pp,building_to_buy = cur_rps_pp,building
        product_to_buy = products[i] # store element to click&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an &lt;span class=&#34;math inline&#34;&gt;\(O(N)\)&lt;/span&gt; search through the buildings that have information, where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the number of buildings. This code logs the maximum ROI in &lt;code&gt;max_rps_pp&lt;/code&gt; and the building associated with this ROI in &lt;code&gt;building_to_buy&lt;/code&gt;, as well as the web element to click in order to purchase this building, &lt;code&gt;product_to_buy&lt;/code&gt;. After the best ROI investment is found, the next few lines of code update the agent’s balance and check if the investment is affordable. If it is, the agent buys it and this process repeats, if it is not then &lt;code&gt;best_building_affordable&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; which ends the while loop.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minwait-buy-what-minimizes-the-time-to-the-highest-revenue-purchase&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MinWait: Buy what minimizes the time to the highest revenue purchase&lt;/h2&gt;
&lt;p&gt;The intuition of this strategy is to buy the investment that maximizes revenue, however, those investments can be expensive. Other investments may increase revenue enough to decrease the amount of time before the highest revenue investment can be purchased. This intuition is sketched in the figure below, where the naive waiter (red) waits until they can purchase a hypothetical maximum revenue investment for 200, and the MinWait algorithm makes an investment for 100 that increases revenue enough to purchase the maximum revenue investment faster than the naive waiter:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Cookie-Clicker-Bot_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Formally, this algorithm first computes the wait until the maximum revenue investment can be purchased (&lt;span class=&#34;math inline&#34;&gt;\(w_{max}\)&lt;/span&gt;), then searches for investments that can reduce the wait and purchases any that it finds. For a given current revenue &lt;span class=&#34;math inline&#34;&gt;\(r_{t}\)&lt;/span&gt; and cost of the maximum revenue investment, &lt;span class=&#34;math inline&#34;&gt;\(c_{max}\)&lt;/span&gt;, the wait until this investment can be purchased is &lt;span class=&#34;math inline&#34;&gt;\(w_{max}=\frac{c_{max}}{r_t}\)&lt;/span&gt;, which corresponds to the time when the red line reaches 200. This formula ignores current balance, but in the appendix I show that the current balance is irrelevant for the purchasing decision. An alternative investment, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, could improve the wait time if it’s addition to revenue is large enough to make up for its cost before the time &lt;span class=&#34;math inline&#34;&gt;\(w_{max}\)&lt;/span&gt;. This is shown with the blue dashed line in the example above. If investment &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; adds &lt;span class=&#34;math inline&#34;&gt;\(r_b\)&lt;/span&gt; to the current revenue &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt;, and costs &lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt;, then the wait until the maximum revenue investment if &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is purchased is: &lt;span class=&#34;math inline&#34;&gt;\(w_b = \frac{c_b}{r_t}+\frac{c_a}{r_t+r_b}\)&lt;/span&gt;. This algorithm buys &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(w_b&amp;lt;w_{max}\)&lt;/span&gt;; that is, if purchasing &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; reduces the wait until the maximum revenue investment can be purchased.&lt;/p&gt;
&lt;p&gt;The code I used to implement this is provided below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Max RPS/price agent
class agent_class_min_wait(agent_class_naive):
    # overwrite the buy_products method for this agent&amp;#39;s purchase logic
    def buy_products(self):
        ## update building info
        self.get_building_info()
        # while best building affordable, buy it and look for next best building
        best_building_affordable = True
        while best_building_affordable:
            ## get unlocked products
            products = (driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;) + 
                        driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked disabled&amp;quot;]&amp;#39;))
            # find building with max revenue per second and it&amp;#39;s cost
            max_rps,cost_max_rps = 0, float(&amp;#39;inf&amp;#39;)
            building_to_buy,product_to_buy = [], []
            for i,building in enumerate(self.building_info):
                # get rps for building
                cur_rps = self.building_info[building][&amp;#39;cps&amp;#39;] 
                # if it&amp;#39;s the best rps far, update max, cost, and building id
                if cur_rps &amp;gt; max_rps:
                    max_rps,building_to_buy = cur_rps,building
                    cost_max_rps = self.building_info[building][&amp;#39;price&amp;#39;] 
                    product_to_buy = products[i] # queue this building to buy

            # update revenue for computations below
            self.log_balance_and_revenue()
            # check if any other purchase would reduce wait time to buying max_rps product
            wait_max = float(cost_max_rps) / self.revenue if self.revenue else 0 # stops division by 0
            for i,building in enumerate(self.building_info):
                cost_cur = self.building_info[building][&amp;#39;price&amp;#39;]
                rps_cur  = self.revenue + self.building_info[building][&amp;#39;cps&amp;#39;]
                # conditional to stop division by 0
                wait_till_cur = float(cost_cur) / self.revenue if self.revenue else 0
                wait_cur = (wait_till_cur + 
                             cost_max_rps / rps_cur)
                if wait_cur &amp;lt;= wait_max: 
                    wait_max = wait_cur # update minimum wait
                    building_to_buy = building
                    product_to_buy = products[i] # queue this building to buy instead
            # update balance for checking if building affordable
            self.log_balance_and_revenue()
            # buy either max_rps product, or the building that would reduce wait time
            # check if best building is affordable
            if self.building_info[building_to_buy][&amp;#39;price&amp;#39;]&amp;lt;=self.balance:
                # buy building_to_buy (click on this product)
                product_to_buy.click()
                # update building info (including rps per price rps_pp)
                self.get_building_info()
            else: best_building_affordable=False # if not affordable, break purchase loop &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This implementation again extends the &lt;code&gt;agent_class_naive&lt;/code&gt; class by replacing the &lt;code&gt;buy_products&lt;/code&gt; method. The algorithm makes two passes through the list of unlocked investments. The fist pass is copied below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# find building with max revenue per second and it&amp;#39;s cost
max_rps,cost_max_rps = 0, float(&amp;#39;inf&amp;#39;)
building_to_buy,product_to_buy = [], []
for i,building in enumerate(self.building_info):
    # get rps for building
    cur_rps = self.building_info[building][&amp;#39;cps&amp;#39;] 
    # if it&amp;#39;s the best rps far, update max, cost, and building id
    if cur_rps &amp;gt; max_rps:
        max_rps,building_to_buy = cur_rps,building
        cost_max_rps = self.building_info[building][&amp;#39;price&amp;#39;] 
        product_to_buy = products[i] # queue this building to buy&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similar to that used in the MaxROI agent, this code is an &lt;span class=&#34;math inline&#34;&gt;\(O(N)\)&lt;/span&gt; search through the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; unlocked buildings. It logs the maximum revenue in &lt;code&gt;max_rps&lt;/code&gt; and the building associated with this revenue in &lt;code&gt;building_to_buy&lt;/code&gt;, as well as the web element to click in order to purchase this building, &lt;code&gt;product_to_buy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Then the agent computes &lt;span class=&#34;math inline&#34;&gt;\(w_{max}\)&lt;/span&gt; with:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;wait_max = float(cost_max_rps) / self.revenue if self.revenue else 0 # stops division by 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;(value) if (condition) else 0&lt;/code&gt; is used to stop division by 0 – if &lt;code&gt;self.revenue&lt;/code&gt; is 0, then wait_max will take the value 0 rather than the computed value which is undefined when &lt;code&gt;self.revenue&lt;/code&gt; is 0.&lt;/p&gt;
&lt;p&gt;Next, another pass through the list of buildings, this time searching for a building that will reduce the wait time:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i,building in enumerate(self.building_info):
    cost_cur = self.building_info[building][&amp;#39;price&amp;#39;]
    rps_cur  = self.revenue + self.building_info[building][&amp;#39;cps&amp;#39;]
    # conditional to stop division by 0
    wait_till_cur = float(cost_cur) / self.revenue if self.revenue else 0
    wait_cur = (wait_till_cur + 
                 cost_max_rps / rps_cur)
    if wait_cur &amp;lt;= wait_max: 
        wait_max = wait_cur # update minimum wait
        building_to_buy = building
        product_to_buy = products[i] # queue this building to buy instead&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the MinWait agent gets the cost of a candidate investment, &lt;code&gt;cost_cur&lt;/code&gt; and the revenue that would be attained if this investment is bought &lt;code&gt;rps_cur&lt;/code&gt;. Then computes the time before this candidate investment could be bought at the current revenue, &lt;code&gt;wait_till_cur&lt;/code&gt; and finally computes &lt;span class=&#34;math inline&#34;&gt;\(w_b\)&lt;/span&gt;, the wait until the maximum revenue option could be bought if the candidate investment is bought first as &lt;code&gt;wait_cur&lt;/code&gt;. Lastly, if this is lower than &lt;code&gt;wait_max&lt;/code&gt; (&lt;span class=&#34;math inline&#34;&gt;\(w_{max}\)&lt;/span&gt;), then the agent stores the best wait in &lt;code&gt;wait_max&lt;/code&gt; and updates the building to buy and the web element to click.&lt;/p&gt;
&lt;p&gt;Like the MaxROI agent, the last few lines of code check if the investment is affordable, and if not, terminates the ends the purchasing loop.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Performance&lt;/h1&gt;
&lt;p&gt;Each agent class was instantiated as &lt;code&gt;agent&lt;/code&gt; and run for 100,000 big cookie clicks, with the purchase logic run after every 200 clicks. The Naive, MaxROI, and MinWait agents ran for approximately 3650 seconds, 3785 seconds, and 3800 seconds, respectively. The differences in runtime were small enough that I wasn’t worried about differences in the performance metrics below being attributable to extra income earned from a longer runtime.&lt;/p&gt;
&lt;div id=&#34;revenue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Revenue&lt;/h2&gt;
&lt;p&gt;First I looked at the main metric, revenue per second, shown in the figure below:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/Revenue.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The MaxROI and MinWait strategies clearly performed better than the Naive strategy in terms of maximizing revenue per second. It also appeared that the MaxROI algorithm generally had higher revenue when in a purchasing cycle, but the MinWait algorithm surpassed the MaxROI algorithm when waiting for a large purchase.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building Count&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/BuildingCount.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Naive agent purchases each investment at about the same rate. At the other extreme, MinWait agent generally saves for the highest revenue option, then occasionally buys cheaper options to reduce the wait until the next purchase of the highest revenue option. This is most clearly seen in the spikes across buildings soon after a new, expensive, building is purchased. Like the Naive agent, the MaxROI agent also buys each investment frequently, however this agent prioritizes the investments that give the best return on investment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-prices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building Prices&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/Price.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Naieve strategy results in equalizing prices, while both the MaxROI and MinWait strategies can save to purchase the most expensive investment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;return-on-investment-revenue-per-cost&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Return on Investment (Revenue Per Cost)&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/RevenuePerPrice.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Naive strategy rarely chooses the best ROI option. The MaxROI strategy equalizes the ratio between revenue and price across the investments, while the MinWait strategy similarly picks options with a good ratio here but also frequently chooses worse deals. Overall the Naive and MinWait strategies often end up paying more than they should for revenue.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;Both the MaxROI and MinWait agents perform far better than the Naive agent in terms of maximizing revenue. The MaxROI agent performed better when in a buying cycle, but the MinWait agent surpassed the MaxROI agent during long periods of saving for an expensive purchase. Perhaps a better algorithm would be a hybrid that follows the MaxROI strategy when all revenues are known, and uses the MinWait strategy when saving up for an expensive option with an unknown revenue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix:&lt;/h1&gt;
&lt;div id=&#34;minwait-decision-is-independent-of-balance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MinWait Decision is Independent of Balance&lt;/h2&gt;
&lt;p&gt;Another consideration for the MinWait strategy was that the balance could reduce wait overall, and this might mathematically result in a preference reversal if the wait for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; becomes negligably small. Accounting for balance and using &lt;span class=&#34;math inline&#34;&gt;\(c_{max}\)&lt;/span&gt; to represent the cost of the maximum revenue option, &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt; to represent the current revenue, &lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt; to represent the cost of another investment, and &lt;span class=&#34;math inline&#34;&gt;\(r_b\)&lt;/span&gt; to be the additional revenue provided by that investment, the wait time calculations become;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{aligned} 
w_{max} &amp;amp;= \frac{c_{max}-balance}{r_t}  \\
w_{b} &amp;amp;= \frac{c_b-balance}{r_t} + \frac{c_{max}}{r_t+r_b}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The the decision rule can be cast as: buy &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(w_{max} - w_{b}&amp;lt;0\)&lt;/span&gt;. This results in the decision function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{aligned} 
w_{max} -  w_{b} &amp;amp; = \frac{c_{max}-balance}{r_t} - \left(\frac{c_b-balance}{r_t} + \frac{c_{max}}{r_t+r_b}\right) \\
&amp;amp;= \frac{c_{max}-balance-c_b+balance}{r_t} + \frac{c_{max}}{r_t+r_b} \\
&amp;amp;= \frac{c_{max}-c_b}{r_t} + \frac{c_{max}}{r_t+r_b} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The same function results from &lt;span class=&#34;math inline&#34;&gt;\(w_{max}=\frac{c_{max}}{r_t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_{b} = \frac{c_b}{r_t} + \frac{c_{max}}{r_t+r_b}\)&lt;/span&gt;, hence the decision does not depend on balance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;helper-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Helper Functions&lt;/h2&gt;
&lt;p&gt;The base agent class, &lt;code&gt;agent_class_naive&lt;/code&gt;, contained many helper functions as well as the naive investment purchasing logic. This section explains the code used for the helper functions.&lt;/p&gt;
&lt;div id=&#34;browser-and-game-initialization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Browser and Game Initialization&lt;/h3&gt;
&lt;p&gt;The agent was initialized with a web driver object stored in &lt;code&gt;driver&lt;/code&gt;. The first part of the agent class &lt;code&gt;__init__&lt;/code&gt; method is copied below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class agent_class_naive:
    def __init__(self,driver):
        # navigate to site
        driver.get(&amp;#39;https://orteil.dashnet.org/cookieclicker/&amp;#39;)
        time.sleep(10) # time for page to load
        self.big_cookie = driver.find_element_by_id(&amp;#39;bigCookie&amp;#39;)
        self.golden_cookie_clicks = 0
        # initialize balance and revenue variables
        self.balance=0
        self.revenue=0 
        # initialize balance and revenue logs
        self.log_balance=[0]
        self.log_revenue=[0]
        self.log_bal_rev_epoch=[0]
        # initialize building info, and logs of building info
        self.get_building_info()
        self.log_build_info = dict()
        self.building_info_logger(epoch=0)
        
        ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First the agent navigates the webdriver to the Cookie Clicker URL and pauses for 10 seconds to let the page load. After that, the agent finds the big cookie web element and assigns it to attribute &lt;code&gt;big_cookie&lt;/code&gt;. The remaining lines above initialize various attributes that were either used in purchasing logic, or used to log data on agent performance.&lt;/p&gt;
&lt;p&gt;The second part of the &lt;code&gt;__init__&lt;/code&gt; method navigated to the ‘options’ tab and changed the game settings. One important setting to change was the &lt;code&gt;numbersButton&lt;/code&gt; option – this toggled between displaying numbers numerically, ‘1,000,000’, versus with words, ‘1 million’. Since the balance is scraped from this text, the agents require that numbers be displayed numerically. The other changed settings related to graphics and game performance. Code for this second part of &lt;code&gt;__init__&lt;/code&gt; is shown below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;        # CHANGE GRAPHICS AND OTHER SETTINGS
        # click &amp;#39;options&amp;#39; tab
        driver.find_element_by_id(&amp;quot;prefsButton&amp;quot;).click()
        time.sleep(1) # let page load
        # disable text instead of numbers, e.g. &amp;#39;million&amp;#39; -&amp;gt; &amp;#39;000,000&amp;#39;
        driver.find_element_by_id(&amp;quot;numbersButton&amp;quot;).click()
        # slide volume to around 25%
        time.sleep(.5) # so actions don&amp;#39;t happen too fast
        volume = driver.find_element_by_class_name(&amp;quot;slider&amp;quot;)
        move = ActionChains(driver)
        move.click_and_hold(volume).move_by_offset(-50, 0).release().perform()
        # change graphics for optimal performance
        buttons = (&amp;quot;fancyButton&amp;quot;,&amp;quot;particlesButton&amp;quot;,&amp;quot;cursorsButton&amp;quot;,
                  &amp;quot;milkButton&amp;quot;,&amp;quot;wobblyButton&amp;quot;,&amp;quot;cookiesoundButton&amp;quot;,
                  &amp;quot;formatButton&amp;quot;,&amp;quot;extraButtonsButton&amp;quot;,&amp;quot;customGrandmasButton&amp;quot;)
        for button in buttons:
            time.sleep(.5) # so actions don&amp;#39;t happen too fast to execute
            driver.find_element_by_id(button).click()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;clicking-cookies&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Clicking Cookies&lt;/h3&gt;
&lt;p&gt;A simple, important function, &lt;code&gt;click_cookie()&lt;/code&gt; effects the cookie click. The game also has golden cookies that provide a large lump sum of cookies, or transient increases in revenue. Golden cookies appear at random times, and at random locations on the screen. The second function below identifies and clicks these golden cookies until there are no more golden cookies on the screen, and logs the number of golden cookie clicks.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def click_cookie(self):
        self.big_cookie.click()
        
    def click_golden_cookie(self):
        golden_cookies = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;shimmer&amp;quot;]&amp;#39;)
        while len(golden_cookies)&amp;gt;0: # if there are products, and while we can afford products,
            for golden_cookie in golden_cookies:
                golden_cookie.click() # buy each one
                self.golden_cookie_clicks+=1
            golden_cookies = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;shimmer&amp;quot;]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;purchasing-upgrades&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Purchasing Upgrades&lt;/h3&gt;
&lt;p&gt;Upgrades are another way to spend cookies. The function below finds affordable upgrades and purchases (click) them, starting with the most expensive avilable upgrade.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def buy_upgrades(self):
        upgrades = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;crate upgrade enabled&amp;quot;]&amp;#39;)
        while len(upgrades)&amp;gt;0: # if there are products, and while we can afford products,
            try: upgrades[-1].click() # buy each one, most expensive first
            except: None
            upgrades = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;crate upgrade enabled&amp;quot;]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logging-balance-and-revenue&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logging balance and revenue&lt;/h3&gt;
&lt;p&gt;Balance and revenue were important for some strategies and were logged as a metric to compare the agents. To facilitate both use cases, I created a method that took an argument &lt;code&gt;epoch&lt;/code&gt;. This method parses the text above the big cookie to convert a string like ‘177 cookies \n per second: 1.1’ to extract the balance of 177 and revenue of 1.1. If the method is called without an &lt;code&gt;epoch&lt;/code&gt;, then it only updates the balance and revenue. Alternatively, if an &lt;code&gt;epoch&lt;/code&gt; is passed, then this method also logs the balance, revenue, and eopch or click number. The code for this is shown below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def log_balance_and_revenue(self,epoch=None):
        # parse string with balance and revenue
        tmp = driver.find_elements_by_xpath(&amp;#39;//div[@id=&amp;quot;cookies&amp;quot;]&amp;#39;)
        tmp = tmp[0].text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;) # remove camas 1,000 -&amp;gt; 1000
        tmp = re.findall(&amp;quot;\d+&amp;quot;,tmp) # extract balance and revenue
        tmp = [int(i) for i in tmp] # convert str -&amp;gt; int
        # update balance and revenue
        self.balance = tmp[0] # update current balance
        self.revenue = tmp[1] # update revenue
        # if epoch passed, log balance and revenue at this epoch
        if epoch:
            self.log_bal_rev_epoch.append(epoch) # index for balance and revenue
            self.log_balance.append(tmp[0]) # log balance
            self.log_revenue.append(tmp[1]) # log revenue&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;get-building-information&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Get building information&lt;/h3&gt;
&lt;p&gt;Building information like the cost and revenue was used in purchasing logic. The code used to extract this is shown below, and each component is elaborated on afterwards:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def get_building_info(self):
        # get unlocked products
        products_unlocked = (driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;) + 
                             driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked disabled&amp;quot;]&amp;#39;))
        # info for unlocked buildings
        building_info = dict()
        for i,building in enumerate(products_unlocked):
            # get info from building button
            tmp=building.text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;).split(sep=&amp;quot;\n&amp;quot;)
            building_name,building_price,building_count = tmp if len(tmp)==3 else tmp+[0]
            # initialize dict for building
            building_info[building_name] = dict()
            # fill in count and price
            building_info[building_name][&amp;#39;count&amp;#39;]=int(building_count)
            building_info[building_name][&amp;#39;price&amp;#39;]=int(building_price)
            # get info from building tooltip
            hover = ActionChains(driver).move_to_element(building)
            hover.perform()
            tooltip = driver.find_elements_by_xpath(&amp;#39;//div[@id=&amp;quot;tooltip&amp;quot;]&amp;#39;)
            tmp=tooltip[0].text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;);
            tmp_cps  = re.findall(r&amp;quot;produces [-+]?\d*\.\d+|produces \d+&amp;quot;,tmp);
            # &amp;gt; &amp;#39;produces X&amp;#39;
            if tmp_cps: # if building has been purchased
                tmp_cps_2=float(re.findall(r&amp;quot;[-+]?\d*\.\d+|\d+&amp;quot;,tmp_cps[0])[0])
                building_cps = tmp_cps_2
            else: # if building hasn&amp;#39;t been purchased, store cps as infinity
                building_cps = float(&amp;#39;inf&amp;#39;)
            # &amp;gt; X
            #### default of inf below encourages purchasing unlocked, unowned buildings
            #building_cps = float(tmp_cps[0][:tmp_cps[0].find(&amp;#39; &amp;#39;)]) if tmp_cps else float(&amp;#39;inf&amp;#39;)
            building_info[building_name][&amp;#39;cps&amp;#39;]=building_cps
            building_info[building_name][&amp;#39;cps/price&amp;#39;]=building_cps/int(building_price)
        self.building_info = building_info&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code first obtains a list of the buildings that are unlocked, &lt;code&gt;products_unlocked&lt;/code&gt;, then iterates over these to extract the relevant information: count or number owned, price of the next building, revenue, and the revenue to price ratio. Count and price could be obtained by parsing the text on the building button. This was done in the the following lines:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get info from building button
tmp=building.text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;).split(sep=&amp;quot;\n&amp;quot;)
building_name,building_price,building_count = tmp if len(tmp)==3 else tmp+[0]
# initialize dict for building
building_info[building_name] = dict()
# fill in count and price
building_info[building_name][&amp;#39;count&amp;#39;]=int(building_count)
building_info[building_name][&amp;#39;price&amp;#39;]=int(building_price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Revenue could only be obtained from a tooltip that appeared when the mouse hovered over the building button. The code below effects a mouse hover over the building, then extracts and parses the text from the building tooltip;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get info from building tooltip
hover = ActionChains(driver).move_to_element(building)
hover.perform()
tooltip = driver.find_elements_by_xpath(&amp;#39;//div[@id=&amp;quot;tooltip&amp;quot;]&amp;#39;)
tmp=tooltip[0].text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;);
tmp_cps  = re.findall(r&amp;quot;produces [-+]?\d*\.\d+|produces \d+&amp;quot;,tmp);
if tmp_cps: # if building has been purchased
    tmp_cps_2=float(re.findall(r&amp;quot;[-+]?\d*\.\d+|\d+&amp;quot;,tmp_cps[0])[0])
    building_cps = tmp_cps_2
else: # if building hasn&amp;#39;t been purchased, store cps as infinity
    building_cps = float(&amp;#39;inf&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One challenge was that the revenue was only viewable for owned buildings. To address this, I assumed a revenue of infinity for unowned buildings. This would incentivise agents that bought based on revenue to save and purchase the new building – which was reasonable because these buildings had higher revenue than the owned buildings. The last few lines in the code chunk above extract the revenue for owned buildings, and impute a revenue of infinity for unlocked but unowned buildings.&lt;/p&gt;
&lt;p&gt;Finally, the last few lines of code log the revenue and the return on investment (revenue divided by price) for each building and store the updated building information.&lt;/p&gt;
&lt;p&gt;Since this method was long already, I used a seperate method when logging this information. This logging function is shown below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def building_info_logger(self,epoch):
        if self.building_info: # if self.building_info exists
            # modified from get_building_info to append.
            log_keys = [&amp;#39;count&amp;#39;,&amp;#39;price&amp;#39;,&amp;#39;cps&amp;#39;,&amp;#39;cps/price&amp;#39;]
            # info for unlocked buildings
            for building in self.building_info:
                # initialize new buildings
                if building not in self.log_build_info:
                    self.log_build_info[building] = dict()
                    for k in log_keys+[&amp;#39;epoch&amp;#39;]:
                        self.log_build_info[building][k]=[]
                # fill in count,price,cps, cps/price
                for k in log_keys:
                    self.log_build_info[building][k].append(self.building_info[building][k])
                # log epoch, within each building since buildings become available at different times
                self.log_build_info[building][&amp;#39;epoch&amp;#39;].append(epoch)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cookie Clicker Agent</title>
      <link>/MacStrelioff/data-science/cookie_clicker_images/cookie-clicker-bot/</link>
      <pubDate>Fri, 02 Aug 2019 15:56:33 +0000</pubDate>
      
      <guid>/MacStrelioff/data-science/cookie_clicker_images/cookie-clicker-bot/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background-and-setup&#34;&gt;Background and Setup&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cookie-clicker-game&#34;&gt;Cookie Clicker Game&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agents&#34;&gt;Agents&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#naive-buy-all-affordable-investments&#34;&gt;Naive: Buy all affordable investments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maxroi-buy-best-return-on-investment&#34;&gt;MaxROI: Buy best return on investment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minwait-buy-what-minimizes-the-time-to-the-highest-revenue-purchase&#34;&gt;MinWait: Buy what minimizes the time to the highest revenue purchase&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance&#34;&gt;Performance&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#revenue&#34;&gt;Revenue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-count&#34;&gt;Building Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-prices&#34;&gt;Building Prices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#return-on-investment-revenue-per-cost&#34;&gt;Return on Investment (Revenue Per Cost)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix:&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#minwait-decision-is-independent-of-balance&#34;&gt;MinWait Decision is Independent of Balance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#helper-functions&#34;&gt;Helper Functions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#browser-and-game-initialization&#34;&gt;Browser and Game Initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clicking-cookies&#34;&gt;Clicking Cookies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#purchasing-upgrades&#34;&gt;Purchasing Upgrades&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logging-balance-and-revenue&#34;&gt;Logging balance and revenue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#get-building-information&#34;&gt;Get building information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Lately I’ve been interested in writing algorithms (agents) that interact with websites. The game &lt;a href=&#34;https://orteil.dashnet.org/cookieclicker/&#34;&gt;Cookie Clicker&lt;/a&gt; is a great testing ground for such algorithms. The game is played by clicking a big cooke to earn money (cookies) that can be used to invest in instruments (buy buildings), which in turn generate revenue (more cookies). Here I’ll describe three agents that I made for this game and assess their performance. I also describe many of the helper functions involved in implementing these agents. The code that implements these agents and reproduces all figures in this blog post can be found on my GitHub, here: &lt;a href=&#34;https://github.com/MacStrelioff/CookieClickerAgent&#34; class=&#34;uri&#34;&gt;https://github.com/MacStrelioff/CookieClickerAgent&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background-and-setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background and Setup&lt;/h1&gt;
&lt;div id=&#34;cookie-clicker-game&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cookie Clicker Game&lt;/h2&gt;
&lt;p&gt;The interface for &lt;a href=&#34;https://orteil.dashnet.org/cookieclicker/&#34;&gt;Cookie Clicker&lt;/a&gt; is shown below.&lt;/p&gt;
&lt;!--- UPDATE IMAGE POINTER TO ONE THAT IS CORRECT IN THE FILE STRUCTURE OF THE HOSTED FILES ---&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/CookieClickerGameEdited.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The goal of the game is to amass wealth in the form of cookies. Your balance and revenue are shown above the large cookie. Cookies are earned each time you click the large cookie and investments, shown at the bottom right, are unlocked as you acquire wealth. Purchasing an investment (Cursor, Grandma, …) provides recurring revenue in cookies per second. For any specific investment, the price of the next investment increases each time the investment is purchased. In developing the algorithms below I focused on maximizing revenue, which would maximize wealth over time.&lt;/p&gt;
&lt;p&gt;The game has other mechanics (upgrades, ascension, golden cookies, …). I programed functions to purchase upgrades and click golden cookies, but these functions were disabled in the analyses below to allow for a controlled comparison of the agents.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;agents&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Agents&lt;/h1&gt;
&lt;p&gt;While the goal was to earn as many cookies as possible, the game also allows revenue to accrue while the player is away, and the amount of cookies earned per click can scale with revenue. For these reasons, I focused on maximizing revenue as the overall goal for any strategy.&lt;/p&gt;
&lt;div id=&#34;naive-buy-all-affordable-investments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naive: Buy all affordable investments&lt;/h2&gt;
&lt;p&gt;The simplest investment strategy was to purchase any affordable building. This is implemented in the code below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class agent_class_naive:

...

def buy_products(self):
    products = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;)
    while products: # if there are affordable products, buy them
        products[-1].click()
        products = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;)
        
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;products&lt;/code&gt; is a list of the web elements that represent the affordable buildings, and the while loop cycles over them, buying the most expensive ones first with &lt;code&gt;products[-1].click()&lt;/code&gt;, until no more buildings are affordable. This purchasing logic is implemented through a &lt;code&gt;buy_products&lt;/code&gt; method of the naive agent, &lt;code&gt;agent_class_naive&lt;/code&gt;. In &lt;a href=&#34;https://github.com/MacStrelioff/CookieClickerAgent&#34;&gt;my code&lt;/a&gt;, the naive agent class also contained the necessary helper functions, described in the section above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maxroi-buy-best-return-on-investment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MaxROI: Buy best return on investment&lt;/h2&gt;
&lt;p&gt;The second strategy is to buy the option that will have the best return on investment (revenue to price ratio). The code below extends the naive agent, &lt;code&gt;agent_class_naive&lt;/code&gt;, by overriding the &lt;code&gt;buy_products&lt;/code&gt; method that implements the investment strategy.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class agent_class_max_rps_price_ratio(agent_class_naive):
    # overwrite the buy_products method
    def buy_products(self):
        ## update building info
        self.get_building_info()
        # while best is affordable, buy the best rps/price building
        best_building_affordable = True
        while best_building_affordable:
            ## get unlocked products
            products = (driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;) + 
                        driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked disabled&amp;quot;]&amp;#39;))
            # find max rps/price building
            max_rps_pp,building_to_buy,product_to_buy = 0,[],[]
            for i,building in enumerate(self.building_info):
                # get rps/price for building
                cur_rps_pp = self.building_info[building][&amp;#39;cps/price&amp;#39;] 
                # if it&amp;#39;s the best so far, update max and building id
                if cur_rps_pp &amp;gt; max_rps_pp:
                    max_rps_pp,building_to_buy = cur_rps_pp,building
                    product_to_buy = products[i] # store element to click
            # update balance
            self.log_balance_and_revenue()
            # check if best building is affordable.
            if self.building_info[building_to_buy][&amp;#39;price&amp;#39;]&amp;lt;=self.balance:
                # buy building_to_buy (click on this product)
                product_to_buy.click()
                # update building info (including rps per price rps_pp)
                self.get_building_info()
            else: best_building_affordable=False # if not affordable, break the loop  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First the MaxROI agent updates building information, including; cookies per second, price, and the ratio cps/price, using the helper method &lt;code&gt;get_building_info()&lt;/code&gt;. Then it purchases the buildings that provide the max ROI, until the best building by this metric is unaffordable. The search for the best building is implemented in the section:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# find max rps/price building
max_rps_pp,building_to_buy,product_to_buy = 0,[],[]
for i,building in enumerate(self.building_info):
    # get rps/price for building
    cur_rps_pp = self.building_info[building][&amp;#39;cps/price&amp;#39;] 
    # if it&amp;#39;s the best so far, update max and building id
    if cur_rps_pp &amp;gt; max_rps_pp:
        max_rps_pp,building_to_buy = cur_rps_pp,building
        product_to_buy = products[i] # store element to click&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an &lt;span class=&#34;math inline&#34;&gt;\(O(N)\)&lt;/span&gt; search through the buildings that have information, where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the number of buildings. This code logs the maximum ROI in &lt;code&gt;max_rps_pp&lt;/code&gt; and the building associated with this ROI in &lt;code&gt;building_to_buy&lt;/code&gt;, as well as the web element to click in order to purchase this building, &lt;code&gt;product_to_buy&lt;/code&gt;. After the best ROI investment is found, the next few lines of code update the agent’s balance and check if the investment is affordable. If it is, the agent buys it and this process repeats, if it is not then &lt;code&gt;best_building_affordable&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; which ends the while loop.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minwait-buy-what-minimizes-the-time-to-the-highest-revenue-purchase&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MinWait: Buy what minimizes the time to the highest revenue purchase&lt;/h2&gt;
&lt;p&gt;The intuition of this strategy is to buy the investment that maximizes revenue, however, those investments can be expensive. Other investments may increase revenue enough to decrease the amount of time before the highest revenue investment can be purchased. This intuition is sketched in the figure below, where the naive waiter (red) waits until they can purchase a hypothetical maximum revenue investment for 200, and the MinWait algorithm makes an investment for 100 that increases revenue enough to purchase the maximum revenue investment faster than the naive waiter:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Cookie_Clicker_Images/Cookie-Clicker-Bot_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Formally, this algorithm first computes the wait until the maximum revenue investment can be purchased (&lt;span class=&#34;math inline&#34;&gt;\(w_{max}\)&lt;/span&gt;), then searches for investments that can reduce the wait and purchases any that it finds. For a given current revenue &lt;span class=&#34;math inline&#34;&gt;\(r_{t}\)&lt;/span&gt; and cost of the maximum revenue investment, &lt;span class=&#34;math inline&#34;&gt;\(c_{max}\)&lt;/span&gt;, the wait until this investment can be purchased is &lt;span class=&#34;math inline&#34;&gt;\(w_{max}=\frac{c_{max}}{r_t}\)&lt;/span&gt;, which corresponds to the time when the red line reaches 200. This formula ignores current balance, but in the appendix I show that the current balance is irrelevant for the purchasing decision. An alternative investment, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, could improve the wait time if it’s addition to revenue is large enough to make up for its cost before the time &lt;span class=&#34;math inline&#34;&gt;\(w_{max}\)&lt;/span&gt;. This is shown with the blue dashed line in the example above. If investment &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; adds &lt;span class=&#34;math inline&#34;&gt;\(r_b\)&lt;/span&gt; to the current revenue &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt;, and costs &lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt;, then the wait until the maximum revenue investment if &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is purchased is: &lt;span class=&#34;math inline&#34;&gt;\(w_b = \frac{c_b}{r_t}+\frac{c_a}{r_t+r_b}\)&lt;/span&gt;. This algorithm buys &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(w_b&amp;lt;w_{max}\)&lt;/span&gt;; that is, if purchasing &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; reduces the wait until the maximum revenue investment can be purchased.&lt;/p&gt;
&lt;p&gt;The code I used to implement this is provided below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Max RPS/price agent
class agent_class_min_wait(agent_class_naive):
    # overwrite the buy_products method for this agent&amp;#39;s purchase logic
    def buy_products(self):
        ## update building info
        self.get_building_info()
        # while best building affordable, buy it and look for next best building
        best_building_affordable = True
        while best_building_affordable:
            ## get unlocked products
            products = (driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;) + 
                        driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked disabled&amp;quot;]&amp;#39;))
            # find building with max revenue per second and it&amp;#39;s cost
            max_rps,cost_max_rps = 0, float(&amp;#39;inf&amp;#39;)
            building_to_buy,product_to_buy = [], []
            for i,building in enumerate(self.building_info):
                # get rps for building
                cur_rps = self.building_info[building][&amp;#39;cps&amp;#39;] 
                # if it&amp;#39;s the best rps far, update max, cost, and building id
                if cur_rps &amp;gt; max_rps:
                    max_rps,building_to_buy = cur_rps,building
                    cost_max_rps = self.building_info[building][&amp;#39;price&amp;#39;] 
                    product_to_buy = products[i] # queue this building to buy

            # update revenue for computations below
            self.log_balance_and_revenue()
            # check if any other purchase would reduce wait time to buying max_rps product
            wait_max = float(cost_max_rps) / self.revenue if self.revenue else 0 # stops division by 0
            for i,building in enumerate(self.building_info):
                cost_cur = self.building_info[building][&amp;#39;price&amp;#39;]
                rps_cur  = self.revenue + self.building_info[building][&amp;#39;cps&amp;#39;]
                # conditional to stop division by 0
                wait_till_cur = float(cost_cur) / self.revenue if self.revenue else 0
                wait_cur = (wait_till_cur + 
                             cost_max_rps / rps_cur)
                if wait_cur &amp;lt;= wait_max: 
                    wait_max = wait_cur # update minimum wait
                    building_to_buy = building
                    product_to_buy = products[i] # queue this building to buy instead
            # update balance for checking if building affordable
            self.log_balance_and_revenue()
            # buy either max_rps product, or the building that would reduce wait time
            # check if best building is affordable
            if self.building_info[building_to_buy][&amp;#39;price&amp;#39;]&amp;lt;=self.balance:
                # buy building_to_buy (click on this product)
                product_to_buy.click()
                # update building info (including rps per price rps_pp)
                self.get_building_info()
            else: best_building_affordable=False # if not affordable, break purchase loop &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This implementation again extends the &lt;code&gt;agent_class_naive&lt;/code&gt; class by replacing the &lt;code&gt;buy_products&lt;/code&gt; method. The algorithm makes two passes through the list of unlocked investments. The fist pass is copied below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# find building with max revenue per second and it&amp;#39;s cost
max_rps,cost_max_rps = 0, float(&amp;#39;inf&amp;#39;)
building_to_buy,product_to_buy = [], []
for i,building in enumerate(self.building_info):
    # get rps for building
    cur_rps = self.building_info[building][&amp;#39;cps&amp;#39;] 
    # if it&amp;#39;s the best rps far, update max, cost, and building id
    if cur_rps &amp;gt; max_rps:
        max_rps,building_to_buy = cur_rps,building
        cost_max_rps = self.building_info[building][&amp;#39;price&amp;#39;] 
        product_to_buy = products[i] # queue this building to buy&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similar to that used in the MaxROI agent, this code is an &lt;span class=&#34;math inline&#34;&gt;\(O(N)\)&lt;/span&gt; search through the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; unlocked buildings. It logs the maximum revenue in &lt;code&gt;max_rps&lt;/code&gt; and the building associated with this revenue in &lt;code&gt;building_to_buy&lt;/code&gt;, as well as the web element to click in order to purchase this building, &lt;code&gt;product_to_buy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Then the agent computes &lt;span class=&#34;math inline&#34;&gt;\(w_{max}\)&lt;/span&gt; with:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;wait_max = float(cost_max_rps) / self.revenue if self.revenue else 0 # stops division by 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;(value) if (condition) else 0&lt;/code&gt; is used to stop division by 0 – if &lt;code&gt;self.revenue&lt;/code&gt; is 0, then wait_max will take the value 0 rather than the computed value which is undefined when &lt;code&gt;self.revenue&lt;/code&gt; is 0.&lt;/p&gt;
&lt;p&gt;Next, another pass through the list of buildings, this time searching for a building that will reduce the wait time:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i,building in enumerate(self.building_info):
    cost_cur = self.building_info[building][&amp;#39;price&amp;#39;]
    rps_cur  = self.revenue + self.building_info[building][&amp;#39;cps&amp;#39;]
    # conditional to stop division by 0
    wait_till_cur = float(cost_cur) / self.revenue if self.revenue else 0
    wait_cur = (wait_till_cur + 
                 cost_max_rps / rps_cur)
    if wait_cur &amp;lt;= wait_max: 
        wait_max = wait_cur # update minimum wait
        building_to_buy = building
        product_to_buy = products[i] # queue this building to buy instead&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the MinWait agent gets the cost of a candidate investment, &lt;code&gt;cost_cur&lt;/code&gt; and the revenue that would be attained if this investment is bought &lt;code&gt;rps_cur&lt;/code&gt;. Then computes the time before this candidate investment could be bought at the current revenue, &lt;code&gt;wait_till_cur&lt;/code&gt; and finally computes &lt;span class=&#34;math inline&#34;&gt;\(w_b\)&lt;/span&gt;, the wait until the maximum revenue option could be bought if the candidate investment is bought first as &lt;code&gt;wait_cur&lt;/code&gt;. Lastly, if this is lower than &lt;code&gt;wait_max&lt;/code&gt; (&lt;span class=&#34;math inline&#34;&gt;\(w_{max}\)&lt;/span&gt;), then the agent stores the best wait in &lt;code&gt;wait_max&lt;/code&gt; and updates the building to buy and the web element to click.&lt;/p&gt;
&lt;p&gt;Like the MaxROI agent, the last few lines of code check if the investment is affordable, and if not, terminates the ends the purchasing loop.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Performance&lt;/h1&gt;
&lt;p&gt;Each agent class was instantiated as &lt;code&gt;agent&lt;/code&gt; and run for 100,000 big cookie clicks, with the purchase logic run after every 200 clicks. The Naive, MaxROI, and MinWait agents ran for approximately 3650 seconds, 3785 seconds, and 3800 seconds, respectively. The differences in runtime were small enough that I wasn’t worried about differences in the performance metrics below being attributable to extra income earned from a longer runtime.&lt;/p&gt;
&lt;div id=&#34;revenue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Revenue&lt;/h2&gt;
&lt;p&gt;First I looked at the main metric, revenue per second, shown in the figure below:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/Revenue.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The MaxROI and MinWait strategies clearly performed better than the Naive strategy in terms of maximizing revenue per second. It also appeared that the MaxROI algorithm generally had higher revenue when in a purchasing cycle, but the MinWait algorithm surpassed the MaxROI algorithm when waiting for a large purchase.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building Count&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/BuildingCount.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Naive agent purchases each investment at about the same rate. At the other extreme, MinWait agent generally saves for the highest revenue option, then occasionally buys cheaper options to reduce the wait until the next purchase of the highest revenue option. This is most clearly seen in the spikes across buildings soon after a new, expensive, building is purchased. Like the Naive agent, the MaxROI agent also buys each investment frequently, however this agent prioritizes the investments that give the best return on investment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-prices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building Prices&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/Price.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Naieve strategy results in equalizing prices, while both the MaxROI and MinWait strategies can save to purchase the most expensive investment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;return-on-investment-revenue-per-cost&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Return on Investment (Revenue Per Cost)&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../Cookie_Clicker_Images/RevenuePerPrice.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Naive strategy rarely chooses the best ROI option. The MaxROI strategy equalizes the ratio between revenue and price across the investments, while the MinWait strategy similarly picks options with a good ratio here but also frequently chooses worse deals. Overall the Naive and MinWait strategies often end up paying more than they should for revenue.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;Both the MaxROI and MinWait agents perform far better than the Naive agent in terms of maximizing revenue. The MaxROI agent performed better when in a buying cycle, but the MinWait agent surpassed the MaxROI agent during long periods of saving for an expensive purchase. Perhaps a better algorithm would be a hybrid that follows the MaxROI strategy when all revenues are known, and uses the MinWait strategy when saving up for an expensive option with an unknown revenue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix:&lt;/h1&gt;
&lt;div id=&#34;minwait-decision-is-independent-of-balance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MinWait Decision is Independent of Balance&lt;/h2&gt;
&lt;p&gt;Another consideration for the MinWait strategy was that the balance could reduce wait overall, and this might mathematically result in a preference reversal if the wait for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; becomes negligably small. Accounting for balance and using &lt;span class=&#34;math inline&#34;&gt;\(c_{max}\)&lt;/span&gt; to represent the cost of the maximum revenue option, &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt; to represent the current revenue, &lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt; to represent the cost of another investment, and &lt;span class=&#34;math inline&#34;&gt;\(r_b\)&lt;/span&gt; to be the additional revenue provided by that investment, the wait time calculations become;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{aligned} 
w_{max} &amp;amp;= \frac{c_{max}-balance}{r_t}  \\
w_{b} &amp;amp;= \frac{c_b-balance}{r_t} + \frac{c_{max}}{r_t+r_b}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The the decision rule can be cast as: buy &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(w_{max} - w_{b}&amp;lt;0\)&lt;/span&gt;. This results in the decision function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{aligned} 
w_{max} -  w_{b} &amp;amp; = \frac{c_{max}-balance}{r_t} - \left(\frac{c_b-balance}{r_t} + \frac{c_{max}}{r_t+r_b}\right) \\
&amp;amp;= \frac{c_{max}-balance-c_b+balance}{r_t} + \frac{c_{max}}{r_t+r_b} \\
&amp;amp;= \frac{c_{max}-c_b}{r_t} + \frac{c_{max}}{r_t+r_b} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The same function results from &lt;span class=&#34;math inline&#34;&gt;\(w_{max}=\frac{c_{max}}{r_t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_{b} = \frac{c_b}{r_t} + \frac{c_{max}}{r_t+r_b}\)&lt;/span&gt;, hence the decision does not depend on balance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;helper-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Helper Functions&lt;/h2&gt;
&lt;p&gt;The base agent class, &lt;code&gt;agent_class_naive&lt;/code&gt;, contained many helper functions as well as the naive investment purchasing logic. This section explains the code used for the helper functions.&lt;/p&gt;
&lt;div id=&#34;browser-and-game-initialization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Browser and Game Initialization&lt;/h3&gt;
&lt;p&gt;The agent was initialized with a web driver object stored in &lt;code&gt;driver&lt;/code&gt;. The first part of the agent class &lt;code&gt;__init__&lt;/code&gt; method is copied below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class agent_class_naive:
    def __init__(self,driver):
        # navigate to site
        driver.get(&amp;#39;https://orteil.dashnet.org/cookieclicker/&amp;#39;)
        time.sleep(10) # time for page to load
        self.big_cookie = driver.find_element_by_id(&amp;#39;bigCookie&amp;#39;)
        self.golden_cookie_clicks = 0
        # initialize balance and revenue variables
        self.balance=0
        self.revenue=0 
        # initialize balance and revenue logs
        self.log_balance=[0]
        self.log_revenue=[0]
        self.log_bal_rev_epoch=[0]
        # initialize building info, and logs of building info
        self.get_building_info()
        self.log_build_info = dict()
        self.building_info_logger(epoch=0)
        
        ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First the agent navigates the webdriver to the Cookie Clicker URL and pauses for 10 seconds to let the page load. After that, the agent finds the big cookie web element and assigns it to attribute &lt;code&gt;big_cookie&lt;/code&gt;. The remaining lines above initialize various attributes that were either used in purchasing logic, or used to log data on agent performance.&lt;/p&gt;
&lt;p&gt;The second part of the &lt;code&gt;__init__&lt;/code&gt; method navigated to the ‘options’ tab and changed the game settings. One important setting to change was the &lt;code&gt;numbersButton&lt;/code&gt; option – this toggled between displaying numbers numerically, ‘1,000,000’, versus with words, ‘1 million’. Since the balance is scraped from this text, the agents require that numbers be displayed numerically. The other changed settings related to graphics and game performance. Code for this second part of &lt;code&gt;__init__&lt;/code&gt; is shown below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;        # CHANGE GRAPHICS AND OTHER SETTINGS
        # click &amp;#39;options&amp;#39; tab
        driver.find_element_by_id(&amp;quot;prefsButton&amp;quot;).click()
        time.sleep(1) # let page load
        # disable text instead of numbers, e.g. &amp;#39;million&amp;#39; -&amp;gt; &amp;#39;000,000&amp;#39;
        driver.find_element_by_id(&amp;quot;numbersButton&amp;quot;).click()
        # slide volume to around 25%
        time.sleep(.5) # so actions don&amp;#39;t happen too fast
        volume = driver.find_element_by_class_name(&amp;quot;slider&amp;quot;)
        move = ActionChains(driver)
        move.click_and_hold(volume).move_by_offset(-50, 0).release().perform()
        # change graphics for optimal performance
        buttons = (&amp;quot;fancyButton&amp;quot;,&amp;quot;particlesButton&amp;quot;,&amp;quot;cursorsButton&amp;quot;,
                  &amp;quot;milkButton&amp;quot;,&amp;quot;wobblyButton&amp;quot;,&amp;quot;cookiesoundButton&amp;quot;,
                  &amp;quot;formatButton&amp;quot;,&amp;quot;extraButtonsButton&amp;quot;,&amp;quot;customGrandmasButton&amp;quot;)
        for button in buttons:
            time.sleep(.5) # so actions don&amp;#39;t happen too fast to execute
            driver.find_element_by_id(button).click()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;clicking-cookies&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Clicking Cookies&lt;/h3&gt;
&lt;p&gt;A simple, important function, &lt;code&gt;click_cookie()&lt;/code&gt; effects the cookie click. The game also has golden cookies that provide a large lump sum of cookies, or transient increases in revenue. Golden cookies appear at random times, and at random locations on the screen. The second function below identifies and clicks these golden cookies until there are no more golden cookies on the screen, and logs the number of golden cookie clicks.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def click_cookie(self):
        self.big_cookie.click()
        
    def click_golden_cookie(self):
        golden_cookies = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;shimmer&amp;quot;]&amp;#39;)
        while len(golden_cookies)&amp;gt;0: # if there are products, and while we can afford products,
            for golden_cookie in golden_cookies:
                golden_cookie.click() # buy each one
                self.golden_cookie_clicks+=1
            golden_cookies = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;shimmer&amp;quot;]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;purchasing-upgrades&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Purchasing Upgrades&lt;/h3&gt;
&lt;p&gt;Upgrades are another way to spend cookies. The function below finds affordable upgrades and purchases (click) them, starting with the most expensive avilable upgrade.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def buy_upgrades(self):
        upgrades = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;crate upgrade enabled&amp;quot;]&amp;#39;)
        while len(upgrades)&amp;gt;0: # if there are products, and while we can afford products,
            try: upgrades[-1].click() # buy each one, most expensive first
            except: None
            upgrades = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;crate upgrade enabled&amp;quot;]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logging-balance-and-revenue&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logging balance and revenue&lt;/h3&gt;
&lt;p&gt;Balance and revenue were important for some strategies and were logged as a metric to compare the agents. To facilitate both use cases, I created a method that took an argument &lt;code&gt;epoch&lt;/code&gt;. This method parses the text above the big cookie to convert a string like ‘177 cookies \n per second: 1.1’ to extract the balance of 177 and revenue of 1.1. If the method is called without an &lt;code&gt;epoch&lt;/code&gt;, then it only updates the balance and revenue. Alternatively, if an &lt;code&gt;epoch&lt;/code&gt; is passed, then this method also logs the balance, revenue, and eopch or click number. The code for this is shown below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def log_balance_and_revenue(self,epoch=None):
        # parse string with balance and revenue
        tmp = driver.find_elements_by_xpath(&amp;#39;//div[@id=&amp;quot;cookies&amp;quot;]&amp;#39;)
        tmp = tmp[0].text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;) # remove camas 1,000 -&amp;gt; 1000
        tmp = re.findall(&amp;quot;\d+&amp;quot;,tmp) # extract balance and revenue
        tmp = [int(i) for i in tmp] # convert str -&amp;gt; int
        # update balance and revenue
        self.balance = tmp[0] # update current balance
        self.revenue = tmp[1] # update revenue
        # if epoch passed, log balance and revenue at this epoch
        if epoch:
            self.log_bal_rev_epoch.append(epoch) # index for balance and revenue
            self.log_balance.append(tmp[0]) # log balance
            self.log_revenue.append(tmp[1]) # log revenue&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;get-building-information&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Get building information&lt;/h3&gt;
&lt;p&gt;Building information like the cost and revenue was used in purchasing logic. The code used to extract this is shown below, and each component is elaborated on afterwards:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def get_building_info(self):
        # get unlocked products
        products_unlocked = (driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;) + 
                             driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked disabled&amp;quot;]&amp;#39;))
        # info for unlocked buildings
        building_info = dict()
        for i,building in enumerate(products_unlocked):
            # get info from building button
            tmp=building.text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;).split(sep=&amp;quot;\n&amp;quot;)
            building_name,building_price,building_count = tmp if len(tmp)==3 else tmp+[0]
            # initialize dict for building
            building_info[building_name] = dict()
            # fill in count and price
            building_info[building_name][&amp;#39;count&amp;#39;]=int(building_count)
            building_info[building_name][&amp;#39;price&amp;#39;]=int(building_price)
            # get info from building tooltip
            hover = ActionChains(driver).move_to_element(building)
            hover.perform()
            tooltip = driver.find_elements_by_xpath(&amp;#39;//div[@id=&amp;quot;tooltip&amp;quot;]&amp;#39;)
            tmp=tooltip[0].text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;);
            tmp_cps  = re.findall(r&amp;quot;produces [-+]?\d*\.\d+|produces \d+&amp;quot;,tmp);
            # &amp;gt; &amp;#39;produces X&amp;#39;
            if tmp_cps: # if building has been purchased
                tmp_cps_2=float(re.findall(r&amp;quot;[-+]?\d*\.\d+|\d+&amp;quot;,tmp_cps[0])[0])
                building_cps = tmp_cps_2
            else: # if building hasn&amp;#39;t been purchased, store cps as infinity
                building_cps = float(&amp;#39;inf&amp;#39;)
            # &amp;gt; X
            #### default of inf below encourages purchasing unlocked, unowned buildings
            #building_cps = float(tmp_cps[0][:tmp_cps[0].find(&amp;#39; &amp;#39;)]) if tmp_cps else float(&amp;#39;inf&amp;#39;)
            building_info[building_name][&amp;#39;cps&amp;#39;]=building_cps
            building_info[building_name][&amp;#39;cps/price&amp;#39;]=building_cps/int(building_price)
        self.building_info = building_info&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code first obtains a list of the buildings that are unlocked, &lt;code&gt;products_unlocked&lt;/code&gt;, then iterates over these to extract the relevant information: count or number owned, price of the next building, revenue, and the revenue to price ratio. Count and price could be obtained by parsing the text on the building button. This was done in the the following lines:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get info from building button
tmp=building.text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;).split(sep=&amp;quot;\n&amp;quot;)
building_name,building_price,building_count = tmp if len(tmp)==3 else tmp+[0]
# initialize dict for building
building_info[building_name] = dict()
# fill in count and price
building_info[building_name][&amp;#39;count&amp;#39;]=int(building_count)
building_info[building_name][&amp;#39;price&amp;#39;]=int(building_price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Revenue could only be obtained from a tooltip that appeared when the mouse hovered over the building button. The code below effects a mouse hover over the building, then extracts and parses the text from the building tooltip;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get info from building tooltip
hover = ActionChains(driver).move_to_element(building)
hover.perform()
tooltip = driver.find_elements_by_xpath(&amp;#39;//div[@id=&amp;quot;tooltip&amp;quot;]&amp;#39;)
tmp=tooltip[0].text.replace(&amp;#39;,&amp;#39;,&amp;#39;&amp;#39;);
tmp_cps  = re.findall(r&amp;quot;produces [-+]?\d*\.\d+|produces \d+&amp;quot;,tmp);
if tmp_cps: # if building has been purchased
    tmp_cps_2=float(re.findall(r&amp;quot;[-+]?\d*\.\d+|\d+&amp;quot;,tmp_cps[0])[0])
    building_cps = tmp_cps_2
else: # if building hasn&amp;#39;t been purchased, store cps as infinity
    building_cps = float(&amp;#39;inf&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One challenge was that the revenue was only viewable for owned buildings. To address this, I assumed a revenue of infinity for unowned buildings. This would incentivise agents that bought based on revenue to save and purchase the new building – which was reasonable because these buildings had higher revenue than the owned buildings. The last few lines in the code chunk above extract the revenue for owned buildings, and impute a revenue of infinity for unlocked but unowned buildings.&lt;/p&gt;
&lt;p&gt;Finally, the last few lines of code log the revenue and the return on investment (revenue divided by price) for each building and store the updated building information.&lt;/p&gt;
&lt;p&gt;Since this method was long already, I used a seperate method when logging this information. This logging function is shown below:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;    def building_info_logger(self,epoch):
        if self.building_info: # if self.building_info exists
            # modified from get_building_info to append.
            log_keys = [&amp;#39;count&amp;#39;,&amp;#39;price&amp;#39;,&amp;#39;cps&amp;#39;,&amp;#39;cps/price&amp;#39;]
            # info for unlocked buildings
            for building in self.building_info:
                # initialize new buildings
                if building not in self.log_build_info:
                    self.log_build_info[building] = dict()
                    for k in log_keys+[&amp;#39;epoch&amp;#39;]:
                        self.log_build_info[building][k]=[]
                # fill in count,price,cps, cps/price
                for k in log_keys:
                    self.log_build_info[building][k].append(self.building_info[building][k])
                # log epoch, within each building since buildings become available at different times
                self.log_build_info[building][&amp;#39;epoch&amp;#39;].append(epoch)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/MacStrelioff/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>/MacStrelioff/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/post/jupyter/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./academic_0_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;install-python-and-jupyter&#34;&gt;Install Python and Jupyter&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and Jupyter notebook.&lt;/p&gt;

&lt;p&gt;Otherwise, for advanced users, install Jupyter notebook with &lt;code&gt;pip3 install jupyter&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;create-a-new-blog-post-as-usual-https-sourcethemes-com-academic-docs-managing-content-create-a-blog-post&#34;&gt;Create a new blog post &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-a-blog-post&#34; target=&#34;_blank&#34;&gt;as usual&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY_WEBSITE_FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;my-post&lt;/code&gt; with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd &amp;lt;MY_WEBSITE_FOLDER&amp;gt;
hugo new  --kind post post/my-post
cd &amp;lt;MY_WEBSITE_FOLDER&amp;gt;/content/post/my-post/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;

&lt;p&gt;Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (&lt;em&gt;New &amp;gt; Python Notebook&lt;/em&gt;) or upload a notebook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=.

# Copy the contents of Untitled.md and append it to index.md:
cat Untitled.md | tee -a index.md

# Remove the temporary file:
rm Untitled.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;

&lt;p&gt;Open &lt;code&gt;index.md&lt;/code&gt; in your text editor and edit the title etc. in the &lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34;&gt;front matter&lt;/a&gt; according to your preference.&lt;/p&gt;

&lt;p&gt;To set a &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;

&lt;p&gt;For other tips, such as using math, see the guide on &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/MacStrelioff/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/MacStrelioff/video-lectures/basic-statistics/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/video-lectures/basic-statistics/</guid>
      <description>

&lt;h1 id=&#34;basic-applied&#34;&gt;Basic Applied&lt;/h1&gt;

&lt;h2 id=&#34;model-comparison-f-test-for-nested-models&#34;&gt;Model Comparison: F-test for nested models&lt;/h2&gt;

&lt;iframe width=&#34;680&#34; height=&#34;400&#34; src=&#34;https://www.youtube.com/embed/hceIXHjQPdk&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2 id=&#34;model-comparison-f-test-all-parameters-in-a-model&#34;&gt;Model Comparison: F-test, all parameters in a model&lt;/h2&gt;

&lt;iframe width=&#34;680&#34; height=&#34;400&#34; src=&#34;https://www.youtube.com/embed/NVf3PGoQpKA&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2 id=&#34;correlation-r-square-and-correlation&#34;&gt;Correlation: R-square and correlation&lt;/h2&gt;

&lt;iframe width=&#34;680&#34; height=&#34;400&#34; src=&#34;https://www.youtube.com/embed/EIUdQ7v5mtM&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2 id=&#34;correlation-partial-correlation&#34;&gt;Correlation: Partial correlation&lt;/h2&gt;

&lt;iframe width=&#34;680&#34; height=&#34;400&#34; src=&#34;https://www.youtube.com/embed/TvyZpw3rUSc&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h2 id=&#34;correlation-part-correlations&#34;&gt;Correlation: Part correlations&lt;/h2&gt;

&lt;iframe width=&#34;680&#34; height=&#34;400&#34; src=&#34;https://www.youtube.com/embed/EAnjg7jqEAw&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;!---
## My First Handstand

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/CgltP_bmfm8&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
---&gt;

&lt;!---
https://macstrelioff.github.io/MacStrelioff/files/CV.pdf
---&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/MacStrelioff/video-lectures/fun/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/video-lectures/fun/</guid>
      <description>&lt;p&gt;Check back later&amp;hellip;&lt;/p&gt;

&lt;!---
## My First Handstand

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/CgltP_bmfm8&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
---&gt;

&lt;!---
https://macstrelioff.github.io/MacStrelioff/files/CV.pdf
---&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/MacStrelioff/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/tutorial/example/</guid>
      <description>

&lt;h1 id=&#34;h1&#34;&gt;H1&lt;/h1&gt;

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;h3&#34;&gt;H3&lt;/h3&gt;

&lt;p&gt;123&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/MacStrelioff/tutorial2/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/tutorial2/example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
true
&lt;/div&gt;

&lt;div id=&#34;h1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;H1&lt;/h1&gt;
&lt;p&gt;In this tutorial, I’ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;div id=&#34;tip-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tip 1&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(rnorm(20),runif(20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStreliofftutorial2/example_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;div id=&#34;h3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;H3&lt;/h3&gt;
&lt;p&gt;123&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tip-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/MacStrelioff/files/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/files/privacy/</guid>
      <description>&lt;p&gt;Anything you post here can be seen by others.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/MacStrelioff/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>/MacStrelioff/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic: the website builder for Hugo</title>
      <link>/MacStrelioff/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/post/getting-started/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 &lt;em&gt;widgets&lt;/em&gt;, &lt;em&gt;themes&lt;/em&gt;, and &lt;em&gt;language packs&lt;/em&gt; included!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://sourcethemes.com/academic/#expo&#34; target=&#34;_blank&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#install&#34;&gt;&lt;strong&gt;Setup Academic&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;View the documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://discuss.gohugo.io/&#34; target=&#34;_blank&#34;&gt;Ask a question&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gcushen/hugo-academic/issues&#34; target=&#34;_blank&#34;&gt;Request a feature or report a bug&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Updating? View the &lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://sourcethemes.com/academic/updates/&#34; target=&#34;_blank&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support development of Academic:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://paypal.me/cushen&#34; target=&#34;_blank&#34;&gt;Donate a coffee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.patreon.com/cushen&#34; target=&#34;_blank&#34;&gt;Become a backer on Patreon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redbubble.com/people/neutreno/works/34387919-academic&#34; target=&#34;_blank&#34;&gt;Decorate your laptop or journal with an Academic sticker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://academic.threadless.com/&#34; target=&#34;_blank&#34;&gt;Wear the T-shirt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; alt=&#34;Screenshot&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://sourcethemes.com/academic/docs/jupyter/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 15+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;color-themes&#34;&gt;Color Themes&lt;/h2&gt;

&lt;p&gt;Academic comes with &lt;strong&gt;day (light) and night (dark) mode&lt;/strong&gt; built-in. Click the sun/moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34;&gt;Demo&lt;/a&gt; to see it in action!&lt;/p&gt;

&lt;p&gt;Choose a stunning color and font theme for your site. Themes are fully customizable and include:&lt;/p&gt;









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  

  
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Default&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-default.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-default.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Ocean&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-ocean.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-ocean.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Forest&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-forest.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-forest.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Dark&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-dark.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-dark.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Apogee&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-apogee.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-apogee.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;1950s&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-1950s.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-1950s.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Coffee theme with Playfair font&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-coffee-playfair.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-coffee-playfair.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Cupcake&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-cupcake.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-cupcake.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
&lt;/div&gt;

&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/sourcethemes/academic-admin&#34; target=&#34;_blank&#34;&gt;Academic Admin&lt;/a&gt;:&lt;/strong&gt; An admin tool to import publications from BibTeX or import assets for an offline site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/sourcethemes/academic-scripts&#34; target=&#34;_blank&#34;&gt;Academic Scripts&lt;/a&gt;:&lt;/strong&gt; Scripts to help migrate content to new versions of Academic&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;

&lt;p&gt;You can choose from one of the following four methods to install:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-web-browser&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;one-click install using your web browser (recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-git&#34; target=&#34;_blank&#34;&gt;install on your computer using &lt;strong&gt;Git&lt;/strong&gt; with the Command Prompt/Terminal app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-zip&#34; target=&#34;_blank&#34;&gt;install on your computer by downloading the &lt;strong&gt;ZIP files&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34;&gt;install on your computer with &lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then &lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34;&gt;personalize and deploy your new site&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;updating&#34;&gt;Updating&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34;&gt;View the Update Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Feel free to &lt;em&gt;star&lt;/em&gt; the project on &lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt; to help keep track of &lt;a href=&#34;https://sourcethemes.com/academic/updates&#34; target=&#34;_blank&#34;&gt;updates&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;

&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/MacStrelioff/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/MacStrelioff/post/2015-07-23/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/MacStrelioff/post/2015-07-23/2015-07-23-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/MacStrelioff/MacStrelioff/post/2015-07-23/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/MacStrelioff/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bandit Algos for Estimation, Hypothesis Testing, and Decision Making</title>
      <link>/MacStrelioff/unlisted/banditalgos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/banditalgos/</guid>
      <description>


&lt;div id=&#34;sources-alternatives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sources / Alternatives&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/netflix-techblog/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be&#34;&gt;Netflix Experimentation and Sequential Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Optamizely&lt;/li&gt;
&lt;li&gt;Google Analytics&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;todo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TODO:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;redo in Python, make an agent that uses each strategy as a method. Build the agent throughout the script.&lt;/li&gt;
&lt;/ol&gt;
&lt;!---
See: 
For Insight Bandit blogpost:
https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d

For Air BNB experimentation dashboard:
https://medium.com/airbnb-engineering/experiment-reporting-framework-4e3fcd29e6c0
---&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Scientists and business students are trained in decision making from an outdated perspective – classical decision making based on p-values.&lt;/p&gt;
&lt;p&gt;(make a case against p-values – inflated error rates, incoherence, difficulty integrating with expected value)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bandit-algorithms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bandit Algorithms&lt;/h1&gt;
&lt;p&gt;Much of this is from &lt;a href=&#34;https://sudeepraja.github.io/Bandits/&#34;&gt;this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here I evaluate different algorithms for bandit problems similar to those anticipated in industry or testing settings. Criteria of consideration are;&lt;/p&gt;
&lt;div id=&#34;problem-formalization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem Formalization&lt;/h2&gt;
&lt;p&gt;Bandit tasks can be cast as a Markov Decision Process.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
t &amp;amp;\in \{1,...,T\} \\
a_t &amp;amp;\in \mathcal{A} \\
s_t &amp;amp;\in \mathcal{S} \\
r_t &amp;amp;= R(s_{t+1}|a_t,s_t) = v(s_{t+1}|a_t,s_t) \\
T(s_{t+1}|a_t,s_t) &amp;amp;= p(s_{t+1}|a_t,s_t) \\
\pi(s_t)&amp;amp;=p(a_t|s_t)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is referred to as a policy or decision rule. Mathematically, it is a probability distribution over actions – a funciton that maps from states to actions.&lt;/p&gt;
&lt;p&gt;The decision maker’s goal here is to learn a policy (&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;) that is optimal for some criteria. A variety of possible criteria are discussed and evaluated below. This objective is considered when chooseing the value function &lt;span class=&#34;math inline&#34;&gt;\(v(s_{t+1}|a_t,s_t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Objective is to maximize &lt;span class=&#34;math display&#34;&gt;\[
E\left(\sum_t r_t\right)=\sum_tE(r_t)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a typical testing scenario, the policy replaces the assignment mechanism. Hence, I treat assignment mechanisms as policies here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-cases-and-evaluation-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use Cases and Evaluation Criteria&lt;/h2&gt;
&lt;p&gt;Bandit problems arise across many theoretical and applied fields. Everything from industry A/B testing, medical clinical trials, experimental lab studies, and toy problems for reinforcement learning algorithms can be cast as a bandit task. These different domains generally have different goals. A/B tests are conducted to find evidence for an advantage of one version of a product over another while emphasizing classical statistical objectives like minimizing type I error rates or false discovery rates. The goal of clinical experiments is to quickly discover the best treatment so that patient lives can be improved or saved. In simulation settings, bandit problems have been used to benchmark a variety of algorithms in terms of regret. Here I conduct similar benchmarks, while also evaluating standard ‘best proctices’ in terms of type I error rates and false discovery rates.&lt;/p&gt;

&lt;div id=&#34;type-i-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Type I Error&lt;/h3&gt;
&lt;p&gt;Among situations where there is no difference, how often is a difference detected?&lt;/p&gt;
&lt;p&gt;Type I error occurs when a null hypothesis is reongly rejected – so when a difference in outcomes of arms is detected when none actually exists.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;false-discovery-rate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;False Discovery Rate&lt;/h3&gt;
&lt;p&gt;Among detected differences, how many are real?&lt;/p&gt;
&lt;p&gt;A false discovery occurs when an arm is selected but is not the actual optimal arm.&lt;/p&gt;
&lt;p&gt;I’ll consider this on a trial-by-trial level as well as the result of the overall experiment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regret&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regret&lt;/h3&gt;
&lt;p&gt;Regret is the difference between the reward that would have been obtained had the optimal action been chosen, and the reward that was actually obtained from the chosen action.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(Regret) &amp;amp;= \sum_t E(r^*_{t}-r_t)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;uniform-policy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uniform Policy&lt;/h2&gt;
&lt;p&gt;This corresponds to a simple random sample type of assignment mechanism – the gold standard for causal inference from experimental data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\pi(s_t) &amp;amp;= \frac{1}{|\mathcal{A}|} \\
E(Regret) &amp;amp;= TE(r^*_t) - \sum_t E(r_t|\pi) \\
&amp;amp;= T(E(r^*_t) - E(r_t)) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The expected regret on each trial is the difference between the maximal reward and mean reward across actions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;greedy-policies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Greedy Policies&lt;/h2&gt;
&lt;div id=&#34;epsilon-greedy-policy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;-Greedy Policy&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;softmax-policy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Softmax Policy&lt;/h3&gt;
&lt;p&gt;Might be good for parameter estimation while also reducing regret.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;na=5      # Number of arms
as=1:na   # action IDs
beta = 5
R = runif(na) # arm probabilities
#R[1]=1; R[2]=.5; R[3]=0
T = 1500   # Number of trials
q=rep(0,na) 

pis = c()
qs = c()
ats = c()
rts = c()
regret = c()
regrett= 0;

for(ti in 1:T){
# select action
pi = exp(beta*q)/sum(exp(beta*q))
at = rmultinom(n=1,size=as,prob=pi)
at = which(at==1)
# observe outcome  
rt = rbinom(1,size=1,prob=R[at])
# save stats
pis = rbind(pis,pi)
ats = c(ats,at)
rts = c(rts,rt)
# update action values
q[at] = q[at] + .2 * (rt - q[at])
qs = rbind(qs,q)
# regret
regrett= regrett+max(R)-R[at]
regret = c(regret,regrett)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(regret,type=&amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cols = rainbow(na,s=1,v=.7)
for(ai in as){
if( ai&amp;gt;1){par(new=TRUE)}
plot(qs[,ai],type=&amp;quot;l&amp;quot;,ylim=c(0,1),col=cols[ai],
     main=&amp;quot;Estimation&amp;quot;,
     ylab=&amp;quot;Q-value&amp;quot;,
     xlab=&amp;quot;Trial&amp;quot;)
abline(h=R[ai],col=cols[ai],lty=2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(ai in as){
if( ai&amp;gt;1){par(new=TRUE)}
plot(pis[,ai],type=&amp;quot;l&amp;quot;,ylim=c(0,1),col=cols[ai],
     main=&amp;quot;Action Probabilities&amp;quot;,
     ylab=&amp;quot;Policy&amp;quot;,
     xlab=&amp;quot;Trial&amp;quot;)
#abline(h=R[ai],col=cols[ai],lty=2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ent = 0
ps=c()
for(ai in as){
p = sum(ats==ai)/length(ats)
ent = ent + p*(-log2(p))
ps=c(ps,p)
}
c(ent,ps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.41206826 0.01533333 0.68200000 0.03400000 0.14333333 0.12533333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(entropy.empirical(table(ats),unit=&amp;quot;log2&amp;quot;),freqs.empirical(table(ats)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     1          2          3          4          5 
## 1.41206826 0.01533333 0.68200000 0.03400000 0.14333333 0.12533333&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;upper-confidence-bound-ucb-policy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Upper Confidence Bound (UCB) Policy&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;thompson-sampling-policy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thompson Sampling Policy&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#na=5      # Number of arms
#as=1:na   # action IDs
#beta = 5
# keeps same as those above
# R = runif(na) # arm probabilities
#T = 5000   # Number of trials
#priors: rows: actions, col1:alpha, col2:beta
prior = matrix(1,nrow=na,ncol=2)

aposts=c()
bposts=c()
thompsonSamples = c()
ats = c()
rts = c()
regret = c()
regrett= 0;

for(ti in 1:T){
# select action
thompsonSample=c()
for(ai in as){
thompsonSample = c(thompsonSample,rbeta(1,prior[ai,1],prior[ai,2]))
}
at = which.max(thompsonSample)
# observe outcome
rt = rbinom(1,size=1,prob=R[at])
# save stats
thompsonSamples = rbind(thompsonSamples,thompsonSample)
ats = c(ats,at)
rts = c(rts,rt)
# update beta distributions
prior[at,1]=prior[at,1]+rt
prior[at,2]=prior[at,2]+(1-rt)
# save posterior parameters
aposts=rbind(aposts,prior[,1])
bposts=rbind(bposts,prior[,2])
# regret
regrett= regrett+max(R)-R[at]
regret = c(regret,regrett)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(regret,type=&amp;quot;l&amp;quot;,
     main=&amp;quot;Regret = max E(reward) -  E(reward|choice)&amp;quot;,
     ylab=&amp;quot;Cumulative Regret&amp;quot;,
     xlab=&amp;quot;Trial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# something might be wrong with rbeta(x,vec1,vec2)

cols = rainbow(na,s=1,v=.7)
for(ai in as){
if( ai&amp;gt;1){par(new=TRUE)}
plot(aposts[,ai]/(aposts[,ai]+bposts[,ai]),type=&amp;quot;l&amp;quot;,ylim=c(0,1),col=cols[ai],
     main=&amp;quot;Reward Probability Estimation&amp;quot;,
     ylab=&amp;quot;Posterior Means&amp;quot;,
     xlab=&amp;quot;Trial&amp;quot;)
abline(h=R[ai],col=cols[ai],lty=2)
}
legend(1100,.8,c(&amp;quot;Estimate&amp;quot;,&amp;quot;True&amp;quot;),lty=c(1,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(ai in as){
if( ai&amp;gt;1){par(new=TRUE)}
plot(thompsonSamples[,ai],type=&amp;quot;l&amp;quot;,ylim=c(0,1),col=cols[ai],
     main=&amp;quot;Posterior Samples&amp;quot;,
     #ylab=expression(&amp;#39;Sampled Value -- Policy=argmax&amp;#39;[&amp;#39;a&amp;#39;]*&amp;#39;(sample&amp;#39;[&amp;#39;a&amp;#39;]*&amp;#39;)&amp;#39;),
     ylab=expression(&amp;#39;Sampled Value&amp;#39;),
     xlab=&amp;quot;Trial&amp;quot;)
#abline(h=R[ai],col=cols[ai],lty=2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ent = 0
ps=c()
for(ai in as){
p = sum(ats==ai)/length(ats)
ent = ent + p*(-log2(p))
ps=c(ps,p)
}
c(ent,ps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.109426242 0.001333333 0.988666667 0.003333333 0.001333333 0.005333333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(entropy.empirical(table(ats),unit=&amp;quot;log2&amp;quot;),freqs.empirical(table(ats)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       1           2           3           4           5 
## 0.109426242 0.001333333 0.988666667 0.003333333 0.001333333 0.005333333&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;policy-comparisons&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Policy Comparisons&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Growing Smarter: Understanding User Acquisition</title>
      <link>/MacStrelioff/consultingproject/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/consultingproject/</guid>
      <description>


&lt;!---
NOTES: 
for online, add a backslish in image paths. 
For knitting an html, do not have this backslash.
---&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;As part of my fellowship at &lt;a href=&#34;https://blog.insightdatascience.com/&#34;&gt;Insight Data Science&lt;/a&gt;, I worked on a 2 week consulting project with an external company. Their product was a SaaS application enabling team collaboration on shared files, and has a 30-day free trial model. Multiple users can be associated with an account, and users can have different roles that enable different privileges within the service. The overall goal was to identify free trial accounts that would convert to subscription-based paying customers.&lt;/p&gt;
&lt;p&gt;At the request of the client, some of the information in this post has been masked so as to not reveal any confidential information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;business-need&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Business Need&lt;/h1&gt;
&lt;p&gt;The survival of any company hinges on it’s ability to acquire new users. However, the majority of free trial users for my client failed to convert to customers. This leaves much opportunity to increase their userbase.&lt;/p&gt;
&lt;div id=&#34;project-goals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Project Goals&lt;/h2&gt;
&lt;p&gt;As a consultant, I helped with two major goals focused on issuing nudges to users in order to increase user acquisition:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Identify, as early as possible, patterns in user behavior that indicate whether a free trial user is likely to become a customer after the trial.&lt;/li&gt;
&lt;li&gt;Leverage those patterns to identify outreach strategies for users that might otherwise be unlikely to continue using the service..&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-sources-and-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data sources and processing&lt;/h1&gt;
&lt;!---(exclude clearbit)---&gt;
&lt;p&gt;The original dataset consisted of account activity and product performance data for all accounts over a one year period. The dataset contained a large number of accounts, including some that were not germane to the project goals. Since the primary goal focused on behavior during the free trial, I excluded any accounts that were never on a free trial during the data collection period. In exploratory analysis, I found a large number of accounts that showed little to no activity. This prompted me to believe there are two types of trial accounts that do not convert to paying customers – 1) accounts that were created then never engaged with the product, and 2) accounts that engaged with the product, then failed to find the product valuable and decided to stop engagement. Since the strategies to address these accounts might differ, I decided to exclude any account without a minimal degree of engagement with the product and focus on the &lt;span class=&#34;math inline&#34;&gt;\(2^{nd}\)&lt;/span&gt; type of nonconversion. The criteria for a minimal level of engagement was chosen based on dependencies between product features, and decided during discussion with the client. Finally, some product features were not available during the full duration of data collection, as the product evolved over time. Since there was a large sample size, the easiest way to make the analyses pertinent for all features, and to keep accounts comparable to one another, was to exclude data collected before all product features were available.&lt;/p&gt;
&lt;div id=&#34;feature-engineering-and-exploration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Engineering and Exploration&lt;/h2&gt;
&lt;p&gt;The original account activity data was in terms of counts of each possible action (e.g. workspaces created) on each date during data collection. To make the accounts easily comparable regardless of observation dates, I created a variable for account age (days since the account was created). Also, rather than working with daily counts I computed cumulative sums, which represented an account’s total usage aggregated over all its users’ activity of a product feature up to a particular age of their account. Finally, to mitigate confounding of the counts of activity by the number of users, I divided the cumulative sums by the number of users. These were features that I focused on – total usage of each product activity per user up to a particular day since the creation of the account.&lt;/p&gt;
&lt;p&gt;The time of conversion could be many months after a trial had ended, as it may take time for a user associated with an organization to gain approval to purchase a subscription, or it may take time for a large organization to negotiate a price with my client. To focus on classifying free users as &lt;em&gt;potential&lt;/em&gt; customers based on activity, I created a variable that indicated whether an account &lt;em&gt;ever&lt;/em&gt; ended up as a customer. I then conceptualized the problem as a classification problem where, based on account activity over time since the account’s creation, I estimated the probability that the account would ever convert to a customer.&lt;/p&gt;
&lt;p&gt;I initially thought that the accounts that converted would differ in terms of the distribution of these features (cumulative activity per user), relative to those that did not convert. Hence, in exploration, I focused on probing this intuition by plotting the median and &lt;span class=&#34;math inline&#34;&gt;\(20^{th}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(80^{th}\)&lt;/span&gt; quantiles of cross sections of these features across account age, split by users who ended up converting versus those who did not. An example, focusing on the number of workspaces created per user, is shown in the figure below. Classification would be easiest and most interpretable if there were features that the converters clearly used more than those who never converted.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../csum_files_per_contributor.png&#34; alt=&#34;Median (solid line) and middle 60% (shading) of the distribution of cumulative workspaces created per user, split by those who ever paid (blue) and those that were on a free trial forever (red)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Median (solid line) and middle 60% (shading) of the distribution of cumulative workspaces created per user, split by those who ever paid (blue) and those that were on a free trial forever (red)&lt;/p&gt;
&lt;/div&gt;
&lt;!---
^ This one should be a plot of median and 20-60% quantiless
comments sometimes help the captions appear...
---&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling&lt;/h1&gt;
&lt;div id=&#34;considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Considerations&lt;/h2&gt;
&lt;p&gt;My intuition about differing distributions is natrually expressed in &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_discriminant_analysis&#34;&gt;linear discriminant analysis&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Quadratic_classifier&#34;&gt;quadratic discriminant analysis&lt;/a&gt;. However, these algorithms hinge on an assumption that the features are Gaussian distributed, which was clearly not the case – values were strictly non-negative, and distributions were skewed such that there were lots of values around 0 and some values far from the mean. Because of these violations, I also considered a &lt;a href=&#34;https://en.wikipedia.org/wiki/Support-vector_machine&#34;&gt;support vector classifier&lt;/a&gt; with radial basis functions, and tree based algorithms – &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;random forests&lt;/a&gt;, and &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;gradient boosting decision trees&lt;/a&gt; (GBT). An advantage of the tree-based approaches is their robustness to any distribution of the features, and any functional relationship between the features and the probability of an account continuing product use after the free trial.&lt;/p&gt;
&lt;p&gt;Another issue was class imbalance, since the majority of accounts did not continue. I used &lt;a href=&#34;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&#34;&gt;SMOTE&lt;/a&gt;, an algorithm that generates synthetic data from the underrepresented class (continuing customers), to address the class imbalance when fitting the model in training sets. I also considered metrics beyond accuracy, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&#34;&gt;AUC&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/F1_score&#34;&gt;F1 score&lt;/a&gt;, when selecting a final model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;initial-performance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initial Performance&lt;/h2&gt;
&lt;p&gt;To quickly hone in on a model, I assessed the algorithms mentioned above with default hyperparameter values using &lt;a href=&#34;https://scikit-learn.org/stable/index.html&#34;&gt;scikit learn&lt;/a&gt; in Python. I focused on classification accuracy in a test set, and the tree-based algorithms outperformed the others by approximately 10%. This outperformance is likely due to the non-standard distributions of the features.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hyperparameter-tuning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hyperparameter Tuning&lt;/h2&gt;
&lt;p&gt;To assess performance across hyperparameters for the tree-based algorithms, I created &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html&#34;&gt;validation curves&lt;/a&gt; with 5-fold cross validation and a set of hyperparameter values. The performance of both tree based algorithms was mostly stable in terms of &lt;a href=&#34;https://en.wikipedia.org/wiki/F1_score&#34;&gt;F1 scores&lt;/a&gt;, except for poor performance when there were very few (&lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;10\)&lt;/span&gt;) estimators. To mitigate potential overfitting, I increased the minimum number of observations in a leaf to 10, but left all other hyperparameters at their default values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting Procedure&lt;/h2&gt;
&lt;p&gt;I fit the random forests and GBT algorithms to cross-sections of the data at 7, 14, and 30 days since account creation. For each cross section, I split the data into 5 folds, and used &lt;a href=&#34;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&#34;&gt;SMOTE&lt;/a&gt; when training the algorithms within each fold to account for the class imbalance. In each fold, I logged the feature importances and performance metrics from both algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;actionable-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Actionable Results&lt;/h1&gt;
&lt;p&gt;I used the mean feature importances across folds from the GBT algorithm to identify the features that differentiated between successful accounts and accounts with users that may have needed more onboarding. Since these features seperate continuing accounts from those that did not continue after the free trial, these are the features to prioritize when considering interventions to add value to the user’s experience.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../feature_importance.png&#34; alt=&#34;Mean (bar length) and standard deviation (black error bars) of feature importance evaluated across the 5 folds for the gradient boosting trees algorithm. Feature names have been obscured at the client’s request.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Mean (bar length) and standard deviation (black error bars) of feature importance evaluated across the 5 folds for the gradient boosting trees algorithm. Feature names have been obscured at the client’s request.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To understand the form of the relationship between these features and continued engagement after the trial, I binned accounts based on product feature usage and plotted the proportion of accounts that continued using the product after the free trial across these bins. Figures like this, paired with data on an individual account activity, could help in personalizing user outreach to focus on aligning users’ feature usage with that of users in more successful accounts. For example, &lt;strong&gt;based on the figure below, if an account has fewer than 1 workspace per user, their experience might be improved by resources that make workspace creation easier to understand or engage with&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../ConvertByFeature.png&#34; alt=&#34;Proportion of accounts that convert as a function of workspaces per user. There seems to be a bump in conversion rates from around 20% to around 40% above around 1 workspace per user.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Proportion of accounts that convert as a function of workspaces per user. There seems to be a bump in conversion rates from around 20% to around 40% above around 1 workspace per user.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To identify struggling accounts for outreach, I applied the GBT algorithm to all data and focused on the confusion matrix, shown below. Different actions could be taken with respect to accounts in each quadrant. Accounts in the &lt;em&gt;bottom right quadrant&lt;/em&gt; are currently customers, and were identified as such by the algorithm. These are the accounts with users who were successful in identifying value in the product and establishing an ongoing relationship with my client. Accounts in the &lt;em&gt;top right quadrant&lt;/em&gt; are those who have not yet become customers, but are engaging with the product in ways that are similar to those who have become customers. Users in these accounts have likely identified valuable aspects of the product, and could be contacted by a customer success team to help them find a subscription plan that suits their needs.&lt;/p&gt;
&lt;p&gt;Accounts on the &lt;em&gt;top left&lt;/em&gt; and &lt;em&gt;bottom left quadrants&lt;/em&gt; were identified as accounts that would not continue with the product after the free trial – &lt;strong&gt;these are the accounts that may benefit from outreach&lt;/strong&gt; that demonstrates the value this product can add to their workflows. Accounts in the &lt;em&gt;top left quadrant&lt;/em&gt; were correctly identified as accounts that would not continue with the product. Users in these accounts may have failed to identify valuable aspects of the product, and may have had a better experience if they had been contacted early by customer support or had access to educational resources that could have helped them use the product. Accounts in the &lt;em&gt;bottom left quadrant&lt;/em&gt; are those that became paying customers, but were incorrectly identified as accounts that would not continue with the product based on their activity in the first 7 days. Misclassifying these accounts has essentially no harm, as it would only encourage efforts to improve their experience early in their trial.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../confusion_matrix.png&#34; alt=&#34;Confusion matrix based on all data, for identifying who to contact.&#34; /&gt; &lt;!--- comments help caption output ---&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;I began with an intuition that accounts that converted would have a different distribution of product feature usage per user, relative to those that did not convert, when feature usage was assessed at cross sections based on account age. To explore this, I looked at the quantiles of distributions across feature usage. Focusing on this with formal models, I found that tree-based algorithms could perform well across accuracy, precision, recall, and F1 scores, in identifying the accounts that converted, based on daily snapshots of aggregate feature usage.&lt;/p&gt;
&lt;p&gt;To derive insights from this data and analysis, I focused on confusion matrices which identified free trial accounts that acted as if they would continue product usage, and accounts that were currently unlikely to continue with the product (those in need of educational resources, and/or contact from support teams). I focused on the features found to be important by the tree based algorithms in order to identify specific features to target when reaching out to users in struggling accounts. To discover how feature usage relates to a propensity to continue using the product, I looked at the proportion of accounts that continued after trial across levels of engagement with the important product features.&lt;/p&gt;
&lt;p&gt;Overall, my work provided valuable tools for identifying accounts to connect with for long term relationships or for onboarding and educational assistance, as well as feature usage patterns indicative of success with the product.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hypothesis Testing as Classifier Evaluation</title>
      <link>/MacStrelioff/unlisted/hypothesesaremodels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/hypothesesaremodels/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Here I breed ideas from hypothesis testing, with those from the machine learning community on evaluating classifiers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis Testing&lt;/h1&gt;
&lt;p&gt;Hypothesis testing is based on a notion of accepting or rejecting a hypothesis based on data from an experiment.&lt;/p&gt;
&lt;p&gt;accept, reject&lt;/p&gt;
&lt;p&gt;Type I error –&lt;/p&gt;
&lt;p&gt;Type II error –&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; power&lt;/p&gt;
&lt;p&gt;base rates&lt;/p&gt;
&lt;p&gt;false discovery rate&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-evaluation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Classification Evaluation&lt;/h1&gt;
&lt;p&gt;Calssification evaluation is based on selecting a model based on performance in out-of-sample performance. Many performance measures, that I’ll cover throughout, as they relate to concepts from hypothesis testing.&lt;/p&gt;
&lt;p&gt;positive (true), negative (false) true positive, true negative, false positive, false negative, …&lt;/p&gt;
&lt;p&gt;false discovery rates probably related to one of – accuracy, precision, recall, sensitivity, specificity&lt;/p&gt;
&lt;p&gt;class imbalance (base rates)&lt;/p&gt;
&lt;p&gt;F1 score&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hybrid-ideas&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hybrid Ideas&lt;/h1&gt;
&lt;p&gt;Relate &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, and false discovery rate to accuracy, precision, recall.&lt;/p&gt;
&lt;p&gt;sensitivity and specificity&lt;/p&gt;
&lt;p&gt;Signal detection – hit, miss, false alarm?&lt;/p&gt;
&lt;p&gt;Resources with more detail on each metric; &lt;a href=&#34;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&#34;&gt;Most ML metrics&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Insight</title>
      <link>/MacStrelioff/unlisted/insight/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/insight/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview:&lt;/h1&gt;
&lt;p&gt;Made this doc to document my journey through the Insight Data Science program.&lt;/p&gt;
&lt;p&gt;1-3 work on project and professional skills. Week 4 work on demo. Weeks 5-8 deliver demo, work on interview preparation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;professional-skills&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Professional Skills&lt;/h1&gt;
&lt;div id=&#34;mindset-day-1-632019&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mindset (Day 1: 6/3/2019)&lt;/h2&gt;
&lt;p&gt;Imposter syndrome&lt;/p&gt;
&lt;p&gt;Fail quickly and iterate&lt;/p&gt;
&lt;p&gt;Open mindedness – new problem space, …&lt;/p&gt;
&lt;p&gt;Adaptability, identify weakenesses and adapt&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;applying&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applying&lt;/h2&gt;
&lt;p&gt;Identify strengths, and demonstrantions of value to a company&lt;/p&gt;
&lt;div id=&#34;linkedin&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LinkedIn&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/insightdatascience.com/professional-skills-hub-sv-19b/resumes/linkedin?authuser=0&#34;&gt;workshop&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resume&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Resume&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/insightdatascience.com/professional-skills-hub-sv-19b/resumes&#34;&gt;hub page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LEhTuQ4kvUU&amp;amp;feature=youtu.be&#34;&gt;workshop lecture&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Resume is a marketing tool to get an interview. Resumes should be understood in 15-45 seconds.&lt;/p&gt;
&lt;p&gt;Audience:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Hiring managers&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Wants: quickly hire, culture fit, compliments skillsets on team&lt;/li&gt;
&lt;li&gt;Dislike: lack of detail, ‘fluff’, verbose&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Recruiter&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Wants: Pass high quality&lt;/li&gt;
&lt;li&gt;Dislikes: Poor writing or grammer, “creative” resumes that take time to orient to, verbose&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Convey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Technical fit for role&lt;/li&gt;
&lt;li&gt;Potential for learning and growth&lt;/li&gt;
&lt;li&gt;Unique professional value (differentiation)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Components:&lt;/p&gt;
&lt;p&gt;Order from most to least impactful. Focus on content relevant for next role.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Header, contact information. Location that is local to the company being applied to. LinkedIn should also include anything not on resume.&lt;/li&gt;
&lt;li&gt;Skills, core competencies for recruiter to check off required skills. Sort into meaningful clusters.&lt;/li&gt;
&lt;li&gt;Experience, for hiring manager. Include evidence of the skills. Talk about publications here, and emphasize their impact or value. Start with past-tense verb. Situation, Task, Action, Result.&lt;/li&gt;
&lt;li&gt;Education, also mainly for recruiters. Could include a section on specific courses, but would be more impactful as projects in the experience section.&lt;/li&gt;
&lt;li&gt;NO SUMMARY, experience is more valuable. Can put one on LinkedIn. Avoid making too specific (pidgenhole) or general (cliche).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Include numbers to quantify impacts, numbers can be salient relative to text.&lt;/li&gt;
&lt;li&gt;Include URLs, for those who read on paper.&lt;/li&gt;
&lt;li&gt;Consistent puncuation, … .&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pitch&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pitch&lt;/h3&gt;
&lt;p&gt;A pitch is a quick and compelling story used to start conversations.&lt;/p&gt;
&lt;p&gt;10 second:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Hi I’m (). I got my PhD in () where I used () to ()”&lt;/li&gt;
&lt;li&gt;“Hi I’m Mac. I got my PhD in Cognitive Science at UC Irvine, where I used reinforcement learning algorithms as models of how people learn about and interact with a new technology.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;30 second:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Appeals to imagination and empathy to facilitate recall and decision-making&lt;/li&gt;
&lt;li&gt;Differentaites you as a leader rather than hopeful employee&lt;/li&gt;
&lt;li&gt;Creates a story from past experiences that clearly leads to the target job&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;demos&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Demos&lt;/h3&gt;
&lt;p&gt;A demo is a presentation of work to a hiring committee.&lt;/p&gt;
&lt;p&gt;Kinds of questions to anticipate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Very techinical questions&lt;/li&gt;
&lt;li&gt;Business sense&lt;/li&gt;
&lt;li&gt;Thought process – diagnostics, model selection, validation or definition of success&lt;/li&gt;
&lt;li&gt;Alternative approaches, and their strengths and weaknesses&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;opportunities-networking-and-company-visits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Opportunities: Networking and Company Visits&lt;/h2&gt;
&lt;p&gt;At Insight, data science hiring teams frequently visit. During the first week, we had visits from Square, Lab 41, and App Annie.&lt;/p&gt;
&lt;p&gt;Attempt:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask questions to assess fit: size, hierarchy, business model / monetization&lt;/li&gt;
&lt;li&gt;Ask about experiences – “Tell me about a time when someone …”. This leads to an opportunity to make them feel heard and important, or to foster a sense of connection around a similar shared experience.&lt;/li&gt;
&lt;li&gt;Demonstrating curiosity and engagement, to foster a connection and sense of trust.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Avoid:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Questions” purely intended to demonstrate your own knowledge. Instead, introduce yourself after the talk.&lt;/li&gt;
&lt;li&gt;Trying to prove you are smarter than the conterparty, which can make them view you as a threat. Instead cooperate with them to foster trust.&lt;/li&gt;
&lt;li&gt;Implying that their company is inferior to a competitor, this can make them feel defensive and mitigate the potential for a trusting connection.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;profiling-an-opportunity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Profiling an opportunity&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Project lifecycles and autonomy?&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How do data scientists get projects?&lt;/li&gt;
&lt;li&gt;What is the hierarchy?&lt;/li&gt;
&lt;li&gt;How often are check-ups with the team or managers?&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Who are the colleagues?&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Team composition – team size and roles?&lt;/li&gt;
&lt;li&gt;Background and expertise of team members?&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Work culture&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Company sponsored activities?&lt;/li&gt;
&lt;li&gt;Leanring or mentoring opportunities?&lt;/li&gt;
&lt;li&gt;Work hours / work from home policies?&lt;/li&gt;
&lt;li&gt;Vacation policies?&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Next steps&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Interview and onboarding process?&lt;/li&gt;
&lt;li&gt;Current projects?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;project&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Project&lt;/h1&gt;
&lt;div id=&#34;ideation-521-65&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ideation (5/21-6/5)&lt;/h2&gt;
&lt;div id=&#34;prediction-market&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction Market&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sentiment Analysis&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c&#34;&gt;tools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e&#34; class=&#34;uri&#34;&gt;https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://sentdex.com/sentiment-analysis/&#34; class=&#34;uri&#34;&gt;http://sentdex.com/sentiment-analysis/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stats-website&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stats Website&lt;/h3&gt;
&lt;p&gt;Shiny app that generates practice problems and contains lessons&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bandit-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bandit Analysis&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://fastml.com/ab-testing-with-bayesian-bandits-in-google-analytics/&#34;&gt;good example&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d&#34; class=&#34;uri&#34;&gt;https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.insightdatascience.com/visualizing-machine-learning-thresholds-to-make-better-business-decisions-4ab07f823415&#34;&gt;good dashboarding&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;car-price-recommender&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Car price recommender&lt;/h3&gt;
&lt;p&gt;(this has already been done on fb…)&lt;/p&gt;
&lt;p&gt;Sync data from Facebook, Amazon, and Kelly Blue Book to find distributions over car prices, to empower buyers and sellers in social markets.&lt;/p&gt;
&lt;p&gt;I wanted to sell a car and didn’t know what it was worth.&lt;/p&gt;
&lt;p&gt;Learn the KBB API, setup a shiny app that allows one to enter KBB info, and query KBB and FB and Amazon for KBB price, and amazon and fb postings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kaggle-competition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kaggle competition?&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings&#34;&gt;Air bnb new user bookings&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model a user’s behavior from landing on site to their first booking.&lt;/li&gt;
&lt;li&gt;Look for ways to improve bookings? – book in fewer searches, book in less time, …&lt;/li&gt;
&lt;li&gt;Do all NDF locations have a missing ‘date first booked’ variable?&lt;/li&gt;
&lt;li&gt;Does a missing ‘date_first_booked’ mean that the person did not book?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/ga-customer-revenue-prediction/data&#34;&gt;Google store revenue&lt;/a&gt; - few items - makeup items? - predict probability of purchasing an item&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/facebook-v-predicting-check-ins/data&#34;&gt;Facebook checkin prediction&lt;/a&gt; - fake data…&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection&lt;/h2&gt;
&lt;p&gt;Data scientists make data products.&lt;/p&gt;
&lt;p&gt;Assess value&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What is the product?&lt;/li&gt;
&lt;li&gt;Why is this useful to a specific company?&lt;/li&gt;
&lt;li&gt;Why is it useful to users?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assess feasibility&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What data is needed for this product?&lt;/li&gt;
&lt;li&gt;How much data is there, where does it come from?&lt;/li&gt;
&lt;li&gt;What technical methods are used?&lt;/li&gt;
&lt;li&gt;How could AI/ML improve the product?&lt;/li&gt;
&lt;li&gt;Are there any limits on data access or ethical constraints?&lt;/li&gt;
&lt;li&gt;How could it be monetized?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assess potential&lt;/p&gt;
&lt;ol start=&#34;10&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How might a company expand the product?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-principles-for-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Guiding principles for analysis&lt;/h2&gt;
&lt;p&gt;This wasn’t taught at insight, so I’m drawing on my training in statistics and model building here.&lt;/p&gt;
&lt;div id=&#34;thinking-about-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thinking about Variables&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Think of the variables and their structure, before seeing data; confounds, precision, neusance, …&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;thinking-about-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thinking about Models&lt;/h3&gt;
&lt;p&gt;Estimand&lt;/p&gt;
&lt;p&gt;interpretation&lt;/p&gt;
&lt;p&gt;validation (CV)&lt;/p&gt;
&lt;p&gt;efficiency – computational, memory load&lt;/p&gt;
&lt;p&gt;deployment&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;consulting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Consulting&lt;/h2&gt;
&lt;p&gt;I applied for, and was awarded, a consulting project. This section describes notes on professional consulting relationships.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;populate from consulting class…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Constant communication to clarify goals and stay on track. It can be tempting or habitual to conduct an extravagent analysis without regard for its usefulness to the cliant – avoid this waste of time by continually checking in, presenting work, and clarifying the client’s goals and desired outcomes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minimum-viable-product-mvp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Minimum Viable Product (MVP)&lt;/h2&gt;
&lt;p&gt;Working model and actionable insights&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-deliverable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final Deliverable&lt;/h2&gt;
&lt;p&gt;Blog post describing the project&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;presentation-or-demo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Presentation or Demo&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;context&lt;/li&gt;
&lt;li&gt;need&lt;/li&gt;
&lt;li&gt;vision (‘stretch goal’)&lt;/li&gt;
&lt;li&gt;outcome&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Demonstrate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Value of the solution&lt;/li&gt;
&lt;li&gt;Value of you – 1-2 years of salary, benefits, …&lt;/li&gt;
&lt;li&gt;Reasoning throughout the project.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;business-value-statement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Business Value Statement&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/insightdatascience.com/professional-skills-hub-sv-19b/communication/business-value-statement&#34;&gt;hub page&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Summarize project and its use case&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;“I’m using [data] to develop [deliverable] that [outcome] so that [client] can [solution to problem]”&lt;/p&gt;
&lt;p&gt;Mine was: “I’m using daily user activity during a free trial to estimate the probability that the user will convert to a paid user, so that my client can decide when and how to nudge free users that are unlikely to convert”&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Quantify impact;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Statistics that give the scope of the problem&lt;/li&gt;
&lt;li&gt;What is the market, or how many potential customers exist?&lt;/li&gt;
&lt;li&gt;What resources, and how much, will the solution save?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“The fate of a start-up hinges on user acquisition.”&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Broaden use case. Convey the abstract or general challenge that was solved.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Combine for final pitch&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My value statement for this project was:&lt;/p&gt;
&lt;p&gt;“The fate of a start-up depends on user acquisition. I’m using data on user behavior during a free trial to estimate the probability that a free user will convert to a paid user, and determine which product features drive this conversion. My analysis will guide decisions about when and how to nudge the free users that are unlikely to convert, so that my client can acquire more paying users”&lt;/p&gt;
&lt;p&gt;Things to consider;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Concise description of project&lt;/li&gt;
&lt;li&gt;Who are the potential users, how many exist?&lt;/li&gt;
&lt;li&gt;What problem is solved, evidence that it is a real problem&lt;/li&gt;
&lt;li&gt;What solutions already exist, how is yours better? How would this solution save time, money, emotional headache, … ?&lt;/li&gt;
&lt;li&gt;After watching the demo, what skills would the audiance know you can offer on Day 1? How will your demo make these skills explicit?&lt;/li&gt;
&lt;li&gt;Given the technical challenges of executing the project, who is going to be most excited to interview you? How does your project relate to the challenges they face? (specific companies, types of teams, industries, … )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Product: I’m conducting analyses on user demographics, and making a model to predict user conversion after a free trial.&lt;/p&gt;
&lt;p&gt;Potential users: My client. Various teams will use the demographic analyses.&lt;/p&gt;
&lt;p&gt;Problem: Want to know what features or experiences with the product are driving sales.&lt;/p&gt;
&lt;p&gt;competitors: none here.&lt;/p&gt;
&lt;p&gt;After demo: Summary stat skills for demographics, root cause analysis for finding features that drive conversion, feature engineering for the predictive model, ML model building and fitting. Interpreting data for a general audience.&lt;/p&gt;
&lt;p&gt;Who will like the presentation?: Collaborative teams, since I delivered a little for mutltiple teams. Teams that rely on data scientists for business decisions, since I worked actionably product insights.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interview-preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interview Preparation&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recommending: Searching, Sorting, Ranking</title>
      <link>/MacStrelioff/unlisted/searchsortrank/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/searchsortrank/</guid>
      <description>


&lt;div id=&#34;recommendation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recommendation&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Recommender_system&#34;&gt;recommendation algorithms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data science methods play a major role in discovering recommendations for users.&lt;/p&gt;
&lt;p&gt;Econ, compliments and substitutes (competitors).&lt;/p&gt;
&lt;p&gt;Types of recommendation; compliments (these go together), competitors (you might also like…), temporal (might buy this again in the future). Horizontal and vertical focus&lt;/p&gt;
&lt;p&gt;Temporal involves forecasting, which is a topic for another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sort-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sort Algorithms&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://www.geeksforgeeks.org/sorting-algorithms/&#34;&gt;sort algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;search-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Search Algorithms&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Category:Search_algorithms&#34;&gt;search algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking Algorithms&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Ranking_(information_retrieval)&#34;&gt;ranking algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Shiny Apps</title>
      <link>/MacStrelioff/data-science/shiny_apps_rps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/data-science/shiny_apps_rps/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#shiny-apps&#34;&gt;Shiny apps&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ui-user-interface&#34;&gt;UI: User Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#server&#34;&gt;Server&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#components&#34;&gt;Components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rock-paper-scissors-agent-logic&#34;&gt;Rock Paper Scissors Agent Logic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hosting&#34;&gt;Hosting&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hosting-locally&#34;&gt;Hosting locally&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hosting-online&#34;&gt;Hosting online&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Here I document what I learned, and what resources I found helpful, while I was making my first Shiny app. Shiny apps are an easy way to make web apps from RStudio, with a syntax geared towards online dashboards. As a graduate student in an experimental psychology lab, I wondered if shiny apps could be used to host online experiments. My experiments were essentially simple games – there would be buttons that participants could press, and feedback that they would see based on their actions. I decided to make a rock-paper-scissors app to gain experience with Shiny apps and probe their utility as tools for hosting experiments. You can check out the app &lt;a href=&#34;https://macstrelioff.shinyapps.io/rockpaperscissorsagent/&#34;&gt;here (https://macstrelioff.shinyapps.io/rockpaperscissorsagent/)&lt;/a&gt; though I only have 25 hours a month of free hosting, so it may be down occasionally.&lt;/p&gt;
&lt;p&gt;The source code for this app can be found on my GitHub &lt;a href=&#34;https://github.com/MacStrelioff/RockPaperScissors&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-apps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Shiny apps&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://shiny.rstudio.com/&#34;&gt;Shiny apps&lt;/a&gt; are primarily a tool for dashboarding. A dashboard is a tool that a data professional could create in order to communicate insights and actionable results to decision-makers. Ideally these apps enable decison-makers to easily interact with the data in a way that streamlines their decision making process. Many examples, with code, can be seen in the &lt;a href=&#34;http://shiny.rstudio.com/gallery/&#34;&gt;shiny app gallary&lt;/a&gt; – this was the primary resource I turned to while putting together my app.&lt;/p&gt;
&lt;p&gt;Shiny apps are comprised of a user interface (UI) component and a server component; a common layout might look like this;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(shiny)

ui &amp;lt;- fluidPage()

server &amp;lt;- function(input, output){}

shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;ui&lt;/code&gt; will contain functions that control the layout of the page and the names of interactive components like buttons and sliders. The &lt;code&gt;server&lt;/code&gt; defines how the page changes in response to events and user actions, and the final line &lt;code&gt;shinyApp(ui=ui,server=server)&lt;/code&gt; runs the app. For hosting on shinyapps.io (described below), the script that runs the app must be called &lt;code&gt;App.R&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;ui-user-interface&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UI: User Interface&lt;/h2&gt;
&lt;p&gt;My code for the user interface is shown below and broken down in this section;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ui &amp;lt;- fluidPage(
  # Application title
  titlePanel(&amp;quot;Rock Paper Scissors!&amp;quot;),
  # figure
  fluidRow(
    column(width=5,
           plotOutput(&amp;quot;distPlot&amp;quot;)
    )
  ),
  # buttons
  fluidRow(
    column(width=5,offest=2,
           actionButton(&amp;quot;rock&amp;quot;,&amp;quot;Rock&amp;quot;),
           actionButton(&amp;quot;paper&amp;quot;,&amp;quot;Paper&amp;quot;),
           actionButton(&amp;quot;scissors&amp;quot;,&amp;quot;Scissors&amp;quot;)
    )
  ),
  fluidRow(width=5,offset=5,
           textOutput(&amp;quot;result&amp;quot;),br(),
           p(&amp;quot;Source code available at: https://github.com/MacStrelioff/RockPaperScissors&amp;quot;)
           )
  
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;fluidPage()&lt;/code&gt; function is named after a type of layout that adjusts to the dimensions of a browser. Functions within &lt;code&gt;fluidPage()&lt;/code&gt; add elements to the webpage. &lt;code&gt;titlePanel(&amp;quot;TITLE&amp;quot;)&lt;/code&gt; controls the large title at the top of a page. &lt;code&gt;fluidRow()&lt;/code&gt; adds rows of elements to the page, the length of which are determined by the &lt;code&gt;column()&lt;/code&gt; function. I add the figure with;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;column(width=5,
       plotOutput(&amp;quot;distPlot&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;plotOutput(&amp;quot;OUTPUT_NAME&amp;quot;)&lt;/code&gt; takes an output from the &lt;code&gt;server&lt;/code&gt; function, described in the next section, and plots it. More information on reactive output can be found &lt;a href=&#34;http://shiny.rstudio.com/tutorial/written-tutorial/lesson4/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next I create the row of buttons with;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# buttons
fluidRow(
  column(width=5,offest=2,
         actionButton(&amp;quot;rock&amp;quot;,&amp;quot;Rock&amp;quot;),
         actionButton(&amp;quot;paper&amp;quot;,&amp;quot;Paper&amp;quot;),
         actionButton(&amp;quot;scissors&amp;quot;,&amp;quot;Scissors&amp;quot;)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;offset&lt;/code&gt; controls spacing from a previous &lt;code&gt;column&lt;/code&gt; call (not used here since everything is in one &lt;code&gt;column()&lt;/code&gt; call). Each &lt;code&gt;actionButton()&lt;/code&gt; call draws one of the buttons – the first argument is the variable name of this button, which is used as input for the server, and the second argument is the text that appears on the button. An overview of the many kinds of interactive elements you can add is avilable &lt;a href=&#34;http://shiny.rstudio.com/tutorial/written-tutorial/lesson3/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally I add some text to describe game events and point interested users to the source code, with;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fluidRow(width=5,offset=5,
         textOutput(&amp;quot;result&amp;quot;),br(),
         p(&amp;quot;Source code available at: https://github.com/MacStrelioff/RockPaperScissors&amp;quot;)
         )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;textOutput(&amp;quot;OUTPUT_NAME&amp;quot;)&lt;/code&gt; takes the text ouput variable &lt;code&gt;OUTPUT_NAME&lt;/code&gt; from the server and displays it. &lt;code&gt;br()&lt;/code&gt;, named after a line break in HTML (&lt;code&gt;&amp;lt;br&amp;gt;&lt;/code&gt;) adds a line break. And &lt;code&gt;p()&lt;/code&gt; adds static text, again named after a paragraph tag (&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt;) in HTML.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;server&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Server&lt;/h2&gt;
&lt;p&gt;The server defines how page elements interact with user input and server events.&lt;/p&gt;
&lt;div id=&#34;components&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Components&lt;/h3&gt;
&lt;p&gt;My code for the server is shown below and broken down in this section with an emphasis on the functions used. In the code below, I replaced opponent logic with &lt;code&gt;...&lt;/code&gt; to keep the emphasis here on understanding server functions. This logic is revealed and detailed in the next section.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server &amp;lt;- function(input, output) {
  
  # initialize variables (runs once when app visited)
  values &amp;lt;- reactiveValues()
  values$round =0; # track round
  values$opp_actions = c() # track opponent actions
  values$score =0; # track score
  values$scores=0; # track score history for feedback
  values$grams = data.frame(&amp;#39;rrrrr&amp;#39;=rep(0,3)) # initialize to store gram counts
  values$a = &amp;quot;init&amp;quot;;
  values$as = c(&amp;quot;r&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;) # possible actions

  observeEvent(input$rock | input$paper | input$scissors,{
      # if any action taken (done to block the first run when these are all NULL-&amp;gt;0)
      if(input$rock | input$paper | input$scissors){
        # increment round
        values$round  = values$round+1;
        # policy -- code to greedily pick best action
        ## if fewer than 5 actions taken, draw uniformly
        if(length(values$opp_actions)&amp;lt;5){
          values$a=sample(values$as,1)
          } else{ # if at least 5 actions taken
          nobs = length(values$opp_actions)
          ngram = paste(values$opp_actions[(nobs-4):nobs],collapse = &amp;quot;&amp;quot;)
          #cat(&amp;quot;\n&amp;quot;,ngram)
          # if this pattern not observed before, initialize it and choose randomly
          if(!any(names(values$grams)==ngram)){
            values$grams[ngram]=rep(0,3)
            values$a=sample(values$as,1)
          } else { # if at least 5 actions taken, and this pattern has been seen before, 
            pred = values$as[which.max(values$grams[ngram][[1]])]
            values$a=switch(pred,&amp;quot;r&amp;quot;=&amp;quot;p&amp;quot;,&amp;quot;p&amp;quot;=&amp;quot;s&amp;quot;,&amp;quot;s&amp;quot;=&amp;quot;r&amp;quot;)
          }
          #cat(&amp;quot;\n&amp;quot;,names(values$grams))
          #cat(&amp;quot;\n&amp;quot;,values$grams[ngram][[1]])
        }
        
        # get opponent action and outcome
        if(input$rock    -sum(values$opp_actions==&amp;quot;r&amp;quot;)==1){
          opp_action=&amp;quot;r&amp;quot;
          dscore = switch(values$a,&amp;quot;r&amp;quot;=0,&amp;quot;p&amp;quot;=-1,&amp;quot;s&amp;quot;=1)
          }
        if(input$paper   -sum(values$opp_actions==&amp;quot;p&amp;quot;)==1){
          opp_action=&amp;quot;p&amp;quot;
          dscore = switch(values$a,&amp;quot;r&amp;quot;=1,&amp;quot;p&amp;quot;=0,&amp;quot;s&amp;quot;=-1)
        }
        if(input$scissors-sum(values$opp_actions==&amp;quot;s&amp;quot;)==1){
          opp_action=&amp;quot;s&amp;quot;
          dscore = switch(values$a,&amp;quot;r&amp;quot;=-1,&amp;quot;p&amp;quot;=1,&amp;quot;s&amp;quot;=0)
        }
        
        # evaluate outcome
        values$score  = values$score+dscore
        values$scores = c(values$scores,values$score);
        
        # update opponent model 
        values$opp_actions = c(values$opp_actions,opp_action);
        
        if(length(values$opp_actions)&amp;gt;5){
          if(any(names(values$grams)==ngram)){
            values$grams[ngram][[1]]=values$grams[ngram][[1]]+(values$as==opp_action)
          }
        }
        
      }
    
    # use strings to code, then just take last 5 strings and use as the key for the dictionary of 5-grams...
    output$distPlot &amp;lt;- renderPlot({
      try({
      x = seq(0,values$round);
      y = values$scores;
      cat(&amp;quot;\n round:&amp;quot;,values$round, &amp;quot;, score:&amp;quot;,values$score,&amp;quot;, len(x): &amp;quot;,length(x),&amp;quot; len(y):&amp;quot;,length(y),&amp;quot;, opp_act:&amp;quot;,values$opp_actions,
          &amp;quot;\n a: &amp;quot;,values$a,
          sep=&amp;quot;&amp;quot;)
      # draw the histogram with the specified number of bins
      plot(x,y,type=&amp;quot;l&amp;quot;,xlab = &amp;quot;Rounds&amp;quot;,ylab=&amp;quot;Score&amp;quot;,main=&amp;quot;Cumulative Score&amp;quot;)
      })
    })
    })
  
  output$result = renderText({
    paste(&amp;quot;Opponent chose: &amp;quot;,switch(values$a,&amp;quot;r&amp;quot;=&amp;quot;Rock&amp;quot;,&amp;quot;p&amp;quot;=&amp;quot;Paper&amp;quot;,&amp;quot;s&amp;quot;=&amp;quot;Scissors&amp;quot;,&amp;quot;init&amp;quot;=&amp;quot;Nothing yet, ...&amp;quot;))
  })
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first chunk here, shown below, uses the &lt;code&gt;reactiveValues()&lt;/code&gt; function to create a named list of variables that can be updated throughout the app session.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# initialize variables (runs once when app visited)
values &amp;lt;- reactiveValues()
values$round =0; # track round
values$opp_actions = c() # track opponent actions
values$score =0; # track score
values$scores=0; # track score history for feedback
values$grams = data.frame(&amp;#39;rrrrr&amp;#39;=rep(0,3)) # initialize to store gram counts
values$a = &amp;quot;init&amp;quot;;
values$as = c(&amp;quot;r&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;) # possible actions&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next section uses &lt;code&gt;observeEvent(LOGICAL)&lt;/code&gt; to check if an event has occurred. Here the events are a &lt;code&gt;TRUE&lt;/code&gt; value from any of the buttons, which would represent that the button had been pressed. The variables &lt;code&gt;input$NAME&lt;/code&gt; represent the button whose label is &lt;code&gt;NAME&lt;/code&gt;. The second argument is a script to run when an event is detected. Note that the values for variables that reflect button presses, here &lt;code&gt;input$rock&lt;/code&gt;, &lt;code&gt;input$paper&lt;/code&gt;, &lt;code&gt;input$scissors&lt;/code&gt;, are initialized as &lt;code&gt;NULL&lt;/code&gt;, so the server will first run once with these values as &lt;code&gt;NULL&lt;/code&gt;. To avoid the game from starting on this run, I included a conditional that required one of their values to be &lt;code&gt;TRUE&lt;/code&gt;, which would indicate that the player clicked one of the buttons. The rest of this block is replaced with &lt;code&gt;...&lt;/code&gt; because it is game logic that is described below, however, it heavily relies on access to the reactive values stored in the &lt;code&gt;values&lt;/code&gt; structure.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  observeEvent(input$rock | input$paper | input$scissors,{
      # if any action taken (done to block the first run when these are all NULL-&amp;gt;0)
      if(input$rock | input$paper | input$scissors){
          ...
        }
      }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last two chunks use the &lt;code&gt;renderPlot()&lt;/code&gt; function to produce the &lt;code&gt;distPlot&lt;/code&gt; variable, stored in the &lt;code&gt;output&lt;/code&gt; structure, which is referenced in the &lt;code&gt;ui&lt;/code&gt; when drawing the figure. This code occasionally crashed when button presses happened rapidly, so I wrapped it in a &lt;code&gt;try()&lt;/code&gt; block. The &lt;code&gt;cat()&lt;/code&gt; function was used to print values to the console while debugging. Finally, I used the &lt;code&gt;renderText()&lt;/code&gt; function to assign textual feedback on the agent’s actions to the output variable &lt;code&gt;result&lt;/code&gt; which is referenced in the &lt;code&gt;ui&lt;/code&gt; when displaying the text that is rendered here.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output$distPlot &amp;lt;- renderPlot({
  try({
  x = seq(0,values$round);
  y = values$scores;
  cat(&amp;quot;\n round:&amp;quot;,values$round, &amp;quot;, score:&amp;quot;,values$score,&amp;quot;, len(x): &amp;quot;,length(x),&amp;quot; len(y):&amp;quot;,length(y),&amp;quot;, opp_act:&amp;quot;,values$opp_actions,
      &amp;quot;\n a: &amp;quot;,values$a,
      sep=&amp;quot;&amp;quot;)
  # draw the histogram with the specified number of bins
  plot(x,y,type=&amp;quot;l&amp;quot;,xlab = &amp;quot;Rounds&amp;quot;,ylab=&amp;quot;Score&amp;quot;,main=&amp;quot;Cumulative Score&amp;quot;)
  })
})
})
  
output$result = renderText({
  paste(&amp;quot;Opponent chose: &amp;quot;,switch(values$a,&amp;quot;r&amp;quot;=&amp;quot;Rock&amp;quot;,&amp;quot;p&amp;quot;=&amp;quot;Paper&amp;quot;,&amp;quot;s&amp;quot;=&amp;quot;Scissors&amp;quot;,&amp;quot;init&amp;quot;=&amp;quot;Nothing yet, ...&amp;quot;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rock-paper-scissors-agent-logic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rock Paper Scissors Agent Logic&lt;/h3&gt;
&lt;p&gt;Here I return to the logic inside the &lt;code&gt;observeEvent()&lt;/code&gt; call that implements the game. This code is run whenever a user chooses an action. First I increment the round, which is stored in the &lt;code&gt;reactiveValue&lt;/code&gt; structure, &lt;code&gt;values&lt;/code&gt;;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# increment round
values$round  = values$round+1;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then implemented the agent’s policy. In a reinforcement learning context, a policy is an agent’s probability distribution over actions. The actions here are stored in the reactive value &lt;code&gt;as&lt;/code&gt; which consists of “r”,“p”, and “s”, respectively representing the actions Rock, Paper, or Scissors. The policy I implemented here depends on the last 5 actions that a user takes. The entire history of a user’s actions, excluding the current action, is stored in the reactive value &lt;code&gt;opp_actions&lt;/code&gt; which is a string consisting of the charasters “r”, “p”, or “s”. For example, if &lt;code&gt;opp_actions&lt;/code&gt; is “rrps”, it would mean that the user chose rock twice, then paper, then scissors, before picking the current action which is not yet part of the action history. The agent’s action is stored in the reactive value &lt;code&gt;a&lt;/code&gt;. On the first 5 rounds, the agent picks uniformly from the actions Rock, Paper, Scissors;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if(length(values$opp_actions)&amp;lt;5){
  values$a=sample(values$as,1)
  } &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After 5 rounds, the agent uses 5-grams (sequences of the last 5 user actions), to pick an action that will beat the most likely player action based on the previous times the player has emitted this sequence of 5 actions. To implement this, I first get the number of observed actions, &lt;code&gt;nobs&lt;/code&gt;, and then get a string that represents the last 5 actions the user has taken, &lt;code&gt;ngram&lt;/code&gt;. If theis sequence has not been observed, the agent creates an instance of the sequence in its memory of the users’s 5-grams (&lt;code&gt;grams&lt;/code&gt;), initializes counts of the uses subsequent Rock, Paper, Scissors actions to 0, and finally uniformly samples an action. Otherwise, if this sequence has been observed before, the agent predicts what the user will choose on this round (&lt;code&gt;pred&lt;/code&gt;) by finding the action that the player most frequently chose in the past after an identical sequence of 5 actions, and then picks the action &lt;code&gt;a&lt;/code&gt; that would beat what it expects the player to chose. The code that implements this process is shown below;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  else{ # if at least 5 actions taken
  nobs = length(values$opp_actions)
  ngram = paste(values$opp_actions[(nobs-4):nobs],collapse = &amp;quot;&amp;quot;)
  # if this pattern not observed before, initialize it and choose randomly
  if(!any(names(values$grams)==ngram)){
    values$grams[ngram]=rep(0,3)
    values$a=sample(values$as,1)
  } else { # if at least 5 actions taken, and this pattern has been seen before, 
    pred = values$as[which.max(values$grams[ngram][[1]])]
    values$a=switch(pred,&amp;quot;r&amp;quot;=&amp;quot;p&amp;quot;,&amp;quot;p&amp;quot;=&amp;quot;s&amp;quot;,&amp;quot;s&amp;quot;=&amp;quot;r&amp;quot;)
  }
  #cat(&amp;quot;\n&amp;quot;,names(values$grams))
  #cat(&amp;quot;\n&amp;quot;,values$grams[ngram][[1]])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The game environment then processes the user’s action and the agent’s action, and determines an outcome of +1 if the user won, 0 if the agent and user tied, and -1 if the agent won;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# get opponent action and outcome
if(input$rock    -sum(values$opp_actions==&amp;quot;r&amp;quot;)==1){
  opp_action=&amp;quot;r&amp;quot;
  dscore = switch(values$a,&amp;quot;r&amp;quot;=0,&amp;quot;p&amp;quot;=-1,&amp;quot;s&amp;quot;=1)
  }
if(input$paper   -sum(values$opp_actions==&amp;quot;p&amp;quot;)==1){
  opp_action=&amp;quot;p&amp;quot;
  dscore = switch(values$a,&amp;quot;r&amp;quot;=1,&amp;quot;p&amp;quot;=0,&amp;quot;s&amp;quot;=-1)
}
if(input$scissors-sum(values$opp_actions==&amp;quot;s&amp;quot;)==1){
  opp_action=&amp;quot;s&amp;quot;
  dscore = switch(values$a,&amp;quot;r&amp;quot;=-1,&amp;quot;p&amp;quot;=1,&amp;quot;s&amp;quot;=0)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The outcome value is added to the cumulative score, &lt;code&gt;score&lt;/code&gt;, and the score is appended to the &lt;code&gt;scores&lt;/code&gt; variables which stores the score on every round for plotting.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# evaluate outcome
values$score  = values$score+dscore
values$scores = c(values$scores,values$score);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the agent updates its model of the user by appending the current action to the user’s action history, &lt;code&gt;opp_actions&lt;/code&gt;, and updating it’s memory of user behavior stored in &lt;code&gt;grams&lt;/code&gt; by incrementing the count of the user’s action associated with the 5-gram for this round, &lt;code&gt;ngram&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# update opponent model 
values$opp_actions = c(values$opp_actions,opp_action);

if(length(values$opp_actions)&amp;gt;5){
  if(any(names(values$grams)==ngram)){
    values$grams[ngram][[1]]=values$grams[ngram][[1]]+(values$as==opp_action)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hosting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hosting&lt;/h2&gt;
&lt;div id=&#34;hosting-locally&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hosting locally&lt;/h3&gt;
&lt;p&gt;Running the function &lt;code&gt;shinyApp(ui = ui, server = server)&lt;/code&gt; from RStudio will run the application on a local host. This is great for debugging, but not great for making the app avilable to users on other computers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hosting-online&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hosting online&lt;/h3&gt;
&lt;p&gt;RStudio supports many ways of &lt;a href=&#34;https://shiny.rstudio.com/deploy/&#34;&gt;hosting a shiny app&lt;/a&gt;, but the one that worked best for me was to host through the free plan at &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinnyapps.io&lt;/a&gt;. A detailed walkthrough on deploying shiny apps can be found &lt;a href=&#34;https://docs.rstudio.com/shinyapps.io/getting-started.html#deploying-applications&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is also possible to host a Shiny app through Amazon Web Services. More resources on that can be found &lt;a href=&#34;https://medium.com/@CharlesBordet/how-to-deploy-a-shiny-app-on-aws-part-1-4893d0a7432f&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.charlesbordet.com/en/shiny-aws-1/&#34;&gt;here&lt;/a&gt;, or &lt;a href=&#34;https://stackoverflow.com/questions/47725234/understanding-the-scalability-of-rshiny-apps-hosted-on-shinyserver&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why I Like Bayesian Statistics</title>
      <link>/MacStrelioff/unlisted/why_bayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/why_bayes/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;div id=&#34;coherent&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coherent&lt;/h2&gt;
&lt;p&gt;Examples where frequentist stats is incoherent?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flexible&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Flexible&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;easy to model complex processes&lt;/li&gt;
&lt;li&gt;easy to test non-standard hypotheses, e.g. “does every”, or “two groups higher, one group lower”&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;integration-with-decision-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integration with decision theory&lt;/h2&gt;
&lt;p&gt;Thompson sampling example&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
