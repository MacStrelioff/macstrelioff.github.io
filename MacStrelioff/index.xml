<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mac Strelioff</title>
    <link>/MacStrelioff/</link>
    <description>Recent content on Mac Strelioff</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Jun 2030 13:00:00 -0700</lastBuildDate>
    
	    <atom:link href="/MacStrelioff/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Example Talk</title>
      <link>/MacStrelioff/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cookie Clicker Agent</title>
      <link>/MacStrelioff/unlisted/cookie-clicker-bot/</link>
      <pubDate>Fri, 02 Aug 2019 08:59:58 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/cookie-clicker-bot/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background-and-setup&#34;&gt;Background and Setup&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cookie-clicker-game&#34;&gt;Cookie Clicker Game&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#helper-functions&#34;&gt;Helper Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#strategies&#34;&gt;Strategies&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#naive-buy-all-affordible-investments&#34;&gt;Naive: Buy all affordible investments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maxroi-buy-best-return-on-investment&#34;&gt;MaxROI: Buy best return on investment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minwait-buy-what-minizes-the-time-to-the-highest-revenue-purchase&#34;&gt;MinWait: Buy what minizes the time to the highest revenue purchase&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Lately I’ve been interested in writing algorithms (agents) that interact with websites. The game &lt;a href=&#34;https://orteil.dashnet.org/cookieclicker/&#34;&gt;Cookie Clicker&lt;/a&gt; is a great testing ground for such algorithms. The game is played by clicking a big cooke to earn money (cookies) that can be used to invest in instruments (buy buildings), which in turn generate revenue (more cookies). Here I’ll describe three agents that I made for this game and assess their performance. The code that implements these agents and reproduces all figures in this blog post can be found on my GitHub, here: &lt;a href=&#34;https://github.com/MacStrelioff/CookieClickerAgent&#34; class=&#34;uri&#34;&gt;https://github.com/MacStrelioff/CookieClickerAgent&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background-and-setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background and Setup&lt;/h1&gt;
&lt;div id=&#34;cookie-clicker-game&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cookie Clicker Game&lt;/h2&gt;
&lt;p&gt;The interface for &lt;a href=&#34;https://orteil.dashnet.org/cookieclicker/&#34;&gt;Cookie Clicker&lt;/a&gt; is shown below.&lt;/p&gt;
&lt;!--- UPDATE IMAGE POINTER TO ONE THAT IS CORRECT IN THE FILE STRUCTURE OF THE HOSTED FILES ---&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Cookie_Clicker_Images/CookieClickerGameEdited.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The goal of the game is to amass wealth (cookies) and your balance and revenue are shown above the large cookie. Cookies are earned each time you click the large cookie. Investments, shown at the bottom right, are unlocked as you acquire wealth. Purchasing an investment (Cursor, Grandma, …) provides recurring revenue (cookies per second). For any specific investment, the price of the next investment increases each time the investment is purchased. In developing the algorithms below I focused on maximizing revenue, which would over time maximize wealth.&lt;/p&gt;
&lt;p&gt;The game has other mechanics (upgrades, ascension, golden cookies, …). I also programed functions to purchase upgrades and click golden cookies for temporary buffs, but these functions were disabled in the analyses below to allow for a more controlled comparison of the algorithms, which only differed in their strategy for building purchases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;helper-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Helper Functions&lt;/h1&gt;
&lt;p&gt;This section can be skipped if you are only interested in the different strategies and their performance, which are covered completely in the next two sections. This section explains the code used for numerous helper functions that extracted information from the game interface.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;strategies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Strategies&lt;/h1&gt;
&lt;!--
While the goal is to earn as many cookies as possible, the game also allows revenue to accrue player is away, and the amount of cookies earned by a click can scale with the revenue. For these reasons, I focused on maximizing revenue as the overall goal for any strategy.
--&gt;
&lt;div id=&#34;naive-buy-all-affordible-investments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naive: Buy all affordible investments&lt;/h2&gt;
&lt;p&gt;The simplest investment strategy was to purchase any affordible building. In the code below, &lt;code&gt;products&lt;/code&gt; is a list of the web elements that represent the affordible buildings, and the while loop cycles over them, buying the most expensive ones first with &lt;code&gt;products[-1].click()&lt;/code&gt;, until no more buildings are affordible.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def buy_products(self):
    products = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;)
    while products: # if there are affordible products, buy them
        products[-1].click()
        products = driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;maxroi-buy-best-return-on-investment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MaxROI: Buy best return on investment&lt;/h2&gt;
&lt;p&gt;The second, slightly more advanced strategy would be to buy the option that will have the best return on investment (revenue to price ratio).&lt;/p&gt;
&lt;p&gt;One challenge was that the revenue is unknown for buildings that have not been purchased yet. I addressed this by imputing a CPS of infinity, which would incentivize buying at least one of these buildings once they become unlocked.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class agent_class_max_rps_price_ratio(agent_class_naive):
    # overwrite the buy_products method
    def buy_products(self):
        ## update building info
        self.get_building_info()
        # while best is affordible, buy the best rps/price building
        best_building_affordible = True
        while best_building_affordible:
            ## get unlocked products
            products = (driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;) + 
                        driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked disabled&amp;quot;]&amp;#39;))
            # find max rps/price building
            max_rps_pp,building_to_buy,product_to_buy = 0,[],[]
            for i,building in enumerate(self.building_info):
                # get rps/price for building
                cur_rps_pp = self.building_info[building][&amp;#39;cps/price&amp;#39;] 
                # if it&amp;#39;s the best so far, update max and building id
                if cur_rps_pp &amp;gt; max_rps_pp:
                    max_rps_pp,building_to_buy = cur_rps_pp,building
                    product_to_buy = products[i] # store element to click
            # update balance
            self.log_balance_and_revenue()
            # check if best building is affordible.
            if self.building_info[building_to_buy][&amp;#39;price&amp;#39;]&amp;lt;=self.balance:
                # buy building_to_buy (click on this product)
                product_to_buy.click()
                # update building info (including rps per price rps_pp)
                self.get_building_info()
            else: best_building_affordible=False # if not affordible, break the loop  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;minwait-buy-what-minizes-the-time-to-the-highest-revenue-purchase&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MinWait: Buy what minizes the time to the highest revenue purchase&lt;/h2&gt;
&lt;p&gt;This final strategy is a bit more complex. The intuition of this strategy is that buildings which are not the best may still increase revenue enough to decrease the amount of time before the highest revenue option can be purchased.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;formulas for the purchase logic&lt;/li&gt;
&lt;li&gt;&lt;p&gt;possible figure of two rates to a threshold, or one that simulates an intermittent purchase that leads to the larger purchase occurring earlier than it would have with the base revenue – i.e. plot the images I’ve sketched.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;rationale: differences in revenue per building are quite large, so … .&lt;/li&gt;
&lt;li&gt;pics from ipad of concept figure&lt;/li&gt;
&lt;li&gt;proof that balance doesn’t matter&lt;/li&gt;
&lt;li&gt;&lt;p&gt;sketches of preference as rps and cost of another building change.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code to implement this, which makes ample use of helper functions, is provided below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Max RPS/price agent
class agent_class_min_wait(agent_class_naive):
    # overwrite the buy_products method for this agent&amp;#39;s purchase logic
    def buy_products(self):
        ## update building info
        self.get_building_info()
        # while best building affordible, buy it and look for next best building
        best_building_affordible = True
        while best_building_affordible:
            ## get unlocked products
            products = (driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked enabled&amp;quot;]&amp;#39;) + 
                        driver.find_elements_by_xpath(&amp;#39;//div[@class=&amp;quot;product unlocked disabled&amp;quot;]&amp;#39;))
            # find building with max revenue per second and it&amp;#39;s cost
            max_rps,cost_max_rps = 0, float(&amp;#39;inf&amp;#39;)
            building_to_buy,product_to_buy = [], []
            for i,building in enumerate(self.building_info):
                # get rps for building
                cur_rps = self.building_info[building][&amp;#39;cps&amp;#39;] 
                # if it&amp;#39;s the best rps far, update max, cost, and building id
                if cur_rps &amp;gt; max_rps:
                    max_rps,building_to_buy = cur_rps,building
                    cost_max_rps = self.building_info[building][&amp;#39;price&amp;#39;] 
                    product_to_buy = products[i] # queue this building to buy

            # update revenue for computations below
            self.log_balance_and_revenue()
            # check if any other purchase would reduce wait time to buying max_rps product
            wait_max = float(cost_max_rps) / self.revenue if self.revenue else 0 # stops division by 0
            for i,building in enumerate(self.building_info):
                cost_cur = self.building_info[building][&amp;#39;price&amp;#39;]
                rps_cur  = self.revenue + self.building_info[building][&amp;#39;cps&amp;#39;]
                # conditional to stop division by 0
                wait_till_cur = float(cost_cur) / self.revenue if self.revenue else 0
                wait_cur = (wait_till_cur + 
                             cost_max_rps / rps_cur)
                if wait_cur &amp;lt;= wait_max: 
                    wait_max = wait_cur # update minimum wait
                    building_to_buy = building
                    product_to_buy = products[i] # queue this building to buy instead
            # update balance for checking if building affordible
            self.log_balance_and_revenue()
            # buy either max_rps product, or the building that would reduce wait time
            # check if best building is affordible
            if self.building_info[building_to_buy][&amp;#39;price&amp;#39;]&amp;lt;=self.balance:
                # buy building_to_buy (click on this product)
                product_to_buy.click()
                # update building info (including rps per price rps_pp)
                self.get_building_info()
            else: best_building_affordible=False # if not affordible, break purchase loop &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Performance&lt;/h1&gt;
&lt;p&gt;Each agent ran for 100,000 big cookie clicks. The Naive algorithm ran for approximately 3,550 seconds, while the other two ran for around 3,700-3,800. The differences in runtime were small enough that I wasn’t worried about differences in the performance metrics below being attributable to extra income earned from a longer runtime.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Cookie_Clicker_Images/Revenue.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The MaxCps and MinWait strategies clearly performed better than the Naive strategy in terms of maximizing revenue over time.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Cookie_Clicker_Images/BuildingCount.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Naive agent purchases each investment at about the same rate. At the other extreme, MinWait agent generally saves for the highest revenue option, then occasionally buys cheaper options to reduce the wait until the next purchase of the highest revenue option. This is most clearly seen in the spikes across buildings soon after a new, expensive, building is purchased. Like the Naive agent, the MaxCps agent also buys each investment frequently, however this agent prioritizes the investments that give the best return on investment.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Cookie_Clicker_Images/Price.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The Naieve strategy equalizes price, while both the MaxCps and MinWait strategies can save to purchase the most expensive investment.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Cookie_Clicker_Images/RevenuePerPrice.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Most telling, the MaxCPS strategy seems to equalize the ratio between revenue and price across the investments. The MinWait strategy similarly picks options with a good ratio here, but it also frequently chooses worse deals.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/MacStrelioff/data-science/working-environment/</link>
      <pubDate>Thu, 01 Aug 2019 18:03:35 +0000</pubDate>
      
      <guid>/MacStrelioff/data-science/working-environment/</guid>
      <description>


&lt;!---
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/LgYl1ffS_6Y&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
---&gt;
&lt;div id=&#34;working-environment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Working Environment&lt;/h1&gt;
&lt;p&gt;My working environment relies on bash and git. Here I provide some background on these tools and a walkthrough for automatic, scheduled pushes to git.&lt;/p&gt;
&lt;div id=&#34;working-with-bash-and-git&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working With Bash and Git&lt;/h2&gt;
&lt;div id=&#34;useful-bash-commands&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Useful Bash Commands&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;login     # prompts for username and then password to log in
man       # man (command) returns documentation a command&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;directory-commands&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Directory Commands&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;pwd       # print working directory
cd        # changes directory to home
cd \      # changes directory to (/path)
cd ..     # moves up one folder in the directory
ls -al    # lists contencts of directory, -a includes hidden items, -l includes details&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;file-operations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;File Operations&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;mkdir    # make a new directory
touch    # create a file
open     # open a file
mv       # move (file) (to), can also be used to rename a file
cp       # copy (file) (to) 
rm       # delete a file&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-utilities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other Utilities&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;echo    # print commands screen
date    # print date
cal     # print calendar (year)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-custom-commands&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating Custom Commands&lt;/h2&gt;
&lt;p&gt;This section is partially based on &lt;a href=&#34;https://medium.com/devnetwork/how-to-create-your-own-custom-terminal-commands-c5008782a78e&#34;&gt;this Medium blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use the alias command to map an execution of bash commands to a single command. For example, this will create a command ‘hi’ where the Luca voice slowly says ‘hello world’;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;alias hi=&amp;quot;say -v Luca  -r 100 &amp;#39;hello world&amp;#39;&amp;quot;
hi # test this alias&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Functions can be created with the following syntax;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;function function_name() {
 &amp;lt;args&amp;gt; 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inputs are denoted with $1, $2, … . For example, when called from the terminal, this function will print ‘Your Input Was:’ followed by the first argument it receives;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;function print_my_input() {
 echo &amp;#39;Your Input Was:&amp;#39; $1 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a file where custom commands are defined, navigate to the home directory, create, and open the file that will hold function definitions;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd 
touch .custom_commands
open .custom_commands&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define commands you wish to use in future Terminal sessions in the .custom_commands file. The next section describes how to source this file automatically when a Terminal session is initialized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sourcing-files-during-terminal-initialization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sourcing Files During Terminal Initialization&lt;/h2&gt;
&lt;p&gt;This section is partially based on &lt;a href=&#34;https://stackoverflow.com/questions/19662713/where-do-i-find-the-bashrc-file-on-mac&#34;&gt;this StackOverflow post&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bash sources a run control file (~/.bashrc) at initialization. Check if this file exists in the home directory with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd
ls -a&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If there is no .bashrc file listed, then first open the .bash_profile file in a text editor and add the following;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then create and open a .bashrc file in the home directory with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd
touch .bashrc
open .bashrc&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a text editor add the following lines to the .bashrc file;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Source global definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
fi

# source file with function definitions
source ~/.custom_commands&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This last line will source the .custom_commands file described at the end of the previous section, thereby defining any aliases or functions in the .custom_commands file whenever a Terminal session is initialized.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scheduling-bash-functions-with-crontab&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling Bash Functions With Crontab&lt;/h2&gt;
&lt;p&gt;Some basic crontab commands;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;crontab -l # lists crontab jobs
crontab -r # removes all crontab jobs
crontab -e # edits list of crontab jobs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The crontab -e command is somewhat hard to navigate, the following code is an easier way to create crontab jobs from ther terminal;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;crontab -l | { cat; echo &amp;quot;* * * * * some entry&amp;quot;; } | crontab -

# format for string argument for crontab;
* * * * * &amp;quot;some entry&amp;quot;
- - - - -
| | | | |
| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)
| | | ------- Month (1 - 12)
| | --------- Day of month (1 - 31)
| ----------- Hour (0 - 23)
------------- Minute (0 - 59)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I simplify this farther in a custom function called ‘schedule’ that I define in .&lt;/p&gt;
&lt;p&gt;To execute functions from a user, the user must be specified in /etc/cron.allow – however, this file can only be edited by a superuser. To create and edit this file, use;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd /etc
sudo touch cron.allow
sudo nano cron.allow&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nano opens a text editor in ther terminal, and that can be used to edit and save changes to cron.allow.&lt;/p&gt;
&lt;p&gt;More information on crontab &lt;a href=&#34;https://www.computerhope.com/unix/ucrontab.htm&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;git-and-github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Git and GitHub&lt;/h2&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create user name and email for tagging contributions later
git config --global user.name &amp;quot;USERNAME&amp;quot;   
git config --global user.email &amp;quot;EMAIL&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Connecting to Git can be done through a username/password or through SSH verification.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;usernamepassword&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Username/Password&lt;/h3&gt;
&lt;p&gt;This didn’t work for ubdates scheduled through crontab.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ssh&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SSH&lt;/h3&gt;
&lt;p&gt;First, &lt;a href=&#34;https://help.github.com/articles/checking-for-existing-ssh-keys/&#34;&gt;check for SSH keys with&lt;/a&gt;;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd ~/.ssh  # navigate to user/.ssh folder
ls -a      # list all files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A public SSH key is in id_rsa.pub, which can be opened in a text editor.&lt;/p&gt;
&lt;p&gt;If no id_rsa file exists, or if you wish to create a new SSH key, you can use the following command to generate a new SSH key;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ssh-keygen -t rsa -b 4096 -C &amp;quot;your_email@example.com&amp;quot; # run this if id_rsa file does not exist&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will prompt you for the file location where the SSH key should be saved – id_rsa is the default file.&lt;/p&gt;
&lt;p&gt;From here, follow the steps &lt;a href=&#34;https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/&#34;&gt;to add an SSH key&lt;/a&gt;, summarized below.&lt;/p&gt;
&lt;p&gt;First, create a config file for the SSH agent with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;touch ~/.ssh/config&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Open the config file in a text editor and add the the following commands;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Host *
 AddKeysToAgent yes
 UseKeychain yes
 IdentityFile ~/.ssh/id_rsa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, from Terminal, add the key from the file (e.g. id_rsa) to the ssh agent with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ssh-add -K ~/.ssh/id_rsa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, &lt;a href=&#34;https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/&#34;&gt;add the ssh key to the GitHub account&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And &lt;a href=&#34;https://help.github.com/articles/testing-your-ssh-connection/&#34;&gt;test the connection&lt;/a&gt; with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ssh -T git@github.com&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;working-from-a-repository&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Working From a Repository&lt;/h3&gt;
&lt;p&gt;Create a repository through GitHub’s user interface (log in and select ‘new repository’).&lt;/p&gt;
&lt;p&gt;Then create a directory on a local computer, possibly using bash commands; mkdir and cd.&lt;/p&gt;
&lt;p&gt;The simplest way is to clone the repo from GitHub with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git clone (url)     # this will clone from the url to the current directory&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way, from inside the directory that is to be the local repo, issue these commands;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git init   # initialize a repo
git remote add origin (url or SSH address) # point local repo to remote repo&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;switching-between-ssh-and-https-access&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Switching between SSH and HTTPS Access&lt;/h3&gt;
&lt;p&gt;In both examples below, replace ‘USERNAME/REPOSITORY’ with the appropriate values. From within the local repo folder,&lt;/p&gt;
&lt;p&gt;Change from HTTPS to SSH with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git remote set-url origin git@github.com:USERNAME/REPOSITORY.git
git remote -v # list fetch and push destinations, should start with git@github.com&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Change from SSH to HTTPS with;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git remote set-url origin https://github.com/USERNAME/REPOSITORY.git
git remote -v # list fetch and push destinations, should start with https://github.com/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://help.github.com/articles/changing-a-remote-s-url/&#34;&gt;More detailed guide here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other methods for addressing ‘Device not configured’ errors &lt;a href=&#34;https://stackoverflow.com/questions/40274484/fatal-could-not-read-username-for-https-github-com-device-not-configured&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;repository-maintanence-and-collaboration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Repository Maintanence And Collaboration&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/MacStrelioff/y.jpg&#34; alt=&#34;Git workflow&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Git workflow&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Useful commands;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git pull    # adds files from the configured remote repo to your local repo and workspace
git add .   # adds all new files to be tracked
git add -u  # updates all files
git add -A  # does both &amp;#39;.&amp;#39; and &amp;#39;-u&amp;#39;
git commit -m &amp;quot;message&amp;quot; # saves changes local repo
git push    # updates repo on GitHub&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Collaborators can all access the GitHub repo through push/pull commands. In collaborative settings you might need to pull, so that your repository is current, before pushing changes from the local repository to the remote repository on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;useful-custom-aliases-and-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Useful Custom Aliases and Functions&lt;/h2&gt;
&lt;div id=&#34;gitup-combines-add-commit-and-push-into-one-command&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;gitup: combines add, commit, and push into one command&lt;/h4&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;alias gitup=&amp;quot;git add -A; git commit -m &amp;#39;auto&amp;#39;; git push&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gitupall-updates-all-git-repos-in-my-git-folder&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;gitupall: updates all git repos in my git folder&lt;/h4&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;function gitupall{
for filename in /Users/mac/git/*; do
echo &amp;quot;**************************updating $filename&amp;quot;
cd &amp;quot;$filename&amp;quot;; 
gitup
cd ..
done
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;schedule-schedules-a-command-to-be-executed-periodically&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;schedule: schedules a command to be executed periodically&lt;/h4&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;function schedule(){
crontab -l | { cat; echo &amp;quot;$1&amp;quot;; } | crontab -
}

# format for string argument for crontab;
* * * * * &amp;quot;command to be executed&amp;quot;
- - - - -
| | | | |
| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)
| | | ------- Month (1 - 12)
| | --------- Day of month (1 - 31)
| ----------- Hour (0 - 23)
------------- Minute (0 - 59)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example use;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# e.g. use: schedule updating of all repos each hour
schedule &amp;quot;0 0-23 * * * gitupall&amp;quot;
crontab -l # lists crontab jobs after schedule call&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;my-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My Setup&lt;/h2&gt;
&lt;div id=&#34;bash-aliases-functions-scheduled-commands&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bash Aliases, Functions, Scheduled Commands&lt;/h3&gt;
&lt;p&gt;In my root directory, I have a file called .custom_commands with contents;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# aliases

# goes to my folder of git projects
alias gitgo=&amp;quot;cd /Users/mac/git; ls&amp;quot;
echo &amp;#39;sourced alias gitgo&amp;#39;

# pushes updates on a git project
alias gitup=&amp;quot;git add -A; git commit -m &amp;#39;auto&amp;#39;; git push&amp;quot;
echo &amp;#39;sourced alias gitup&amp;#39;

# functions

# schedule &amp;quot;(m h DoM M DoW command)&amp;quot;
# schedules command to be executed at 
# minute m of hour h on DoM days of the month in months M or DoW days of the week.
function schedule(){
crontab -l | { cat; echo &amp;quot;$1&amp;quot;; } | crontab -
}
echo &amp;#39;sourced function schedule&amp;#39;

# for updating all git projects, schedule this to be done periodically
function gitupall(){
for filename in /Users/mac/git/*; do
echo &amp;quot;************************** updating $filename&amp;quot;
cd &amp;quot;$filename&amp;quot;;
git remote -v    # prints the fetch and push destinations
gitup
done
cd # return to home directory
}
echo &amp;#39;sourced function gitupall&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also have a .bashrc file in my root directory, which sources the custom commands. Particularly the contents of the .bashrc file are;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Source system-wide definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
fi

# source file with function definitions
source ~/.custom_commands&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also have a file in my root directory for scheduling, called .scheduled with contents;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# put all scheduled functions here

# source custom commands
source ~/.custom_commands

# activate key agent
eval &amp;quot;$(ssh-agent -s)&amp;quot;

# test SSH key / print username
ssh -T git@github.com

# update all git repos
gitupall&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then scheduled this file to be read hourly by opening a terminal session, which sources .bashrc at initialization thereby sourcing my custom commands, and running;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;schedule &amp;quot;0 0-23 * * * source ~/.scheduled&amp;quot;
crontab -l # the scheduled job should be listed as a crontab job&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probability and Statistics</title>
      <link>/MacStrelioff/insightstudying/probability-statistics/</link>
      <pubDate>Thu, 01 Aug 2019 15:39:36 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/probability-statistics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#probability-foundations&#34;&gt;Probability Foundations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#probability-rules&#34;&gt;Probability Rules&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#odds-and-other-transformations&#34;&gt;Odds and other transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expectation-and-variance&#34;&gt;Expectation and Variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#central-limit-theorem&#34;&gt;Central limit theorem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-tests&#34;&gt;Common Tests&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#z-test&#34;&gt;Z test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#t-test&#34;&gt;t-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chi-square&#34;&gt;Chi-Square&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models&#34;&gt;Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#experiment-design&#34;&gt;Experiment Design&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objective-demand-and-value&#34;&gt;Objective, demand, and value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#constructs-metrics-and-scoping&#34;&gt;Constructs, metrics, and scoping&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#user-flow-and-target-metrics&#34;&gt;User flow and target metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#invariants&#34;&gt;Invariants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confounding-variables&#34;&gt;Confounding variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#precision-variables&#34;&gt;Precision variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#metric-validation&#34;&gt;Metric Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditions&#34;&gt;Conditions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#treatment-units&#34;&gt;Treatment Units&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assignment-mechanism&#34;&gt;Assignment Mechanism&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#population&#34;&gt;Population&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cohorts&#34;&gt;Cohorts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#synthetic-control-groups&#34;&gt;Synthetic Control Groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;Implementation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#size&#34;&gt;Size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#duration&#34;&gt;Duration&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#temporal-variation&#34;&gt;Temporal variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optional-stopping&#34;&gt;Optional stopping&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimands&#34;&gt;Estimands&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#effiencicy&#34;&gt;Effiencicy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing&#34;&gt;Testing&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#binary-or-proportions&#34;&gt;Binary or Proportions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fishers-z&#34;&gt;Fisher’s Z&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#real-valued&#34;&gt;Real-Valued&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#regression&#34;&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models-1&#34;&gt;Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glms&#34;&gt;GLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emperical-variance-estimation&#34;&gt;Emperical Variance Estimation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Fermi problems (order of magnitude)&lt;/p&gt;
&lt;div id=&#34;probability-foundations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Probability Foundations&lt;/h1&gt;
&lt;p&gt;combinatorics (permutations and combinations)&lt;/p&gt;
&lt;div id=&#34;probability-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Probability Rules&lt;/h2&gt;
&lt;p&gt;basic probability rules&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;odds-and-other-transformations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Odds and other transformations&lt;/h2&gt;
&lt;p&gt;odds, log odds, relative risk, … .&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;expectation-and-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expectation and Variance&lt;/h2&gt;
&lt;p&gt;expectation, variance, conditional or iterated expectations, law of total variance, variance of multiple variables&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;central-limit-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Central limit theorem&lt;/h2&gt;
&lt;p&gt;Sum of any random variable converges to normal distribution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;show for binomial case, to justify Z test&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-tests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Common Tests&lt;/h1&gt;
&lt;div id=&#34;z-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Z test&lt;/h2&gt;
&lt;p&gt;Normal data&lt;/p&gt;
&lt;p&gt;Binary data&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;t-test&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;different variance formulations.&lt;/li&gt;
&lt;li&gt;one-sample&lt;/li&gt;
&lt;li&gt;two independent sampels&lt;/li&gt;
&lt;li&gt;two dependent samples&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chi-square&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chi-Square&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Models&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;experiment-design&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Experiment Design&lt;/h1&gt;
&lt;div id=&#34;objective-demand-and-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objective, demand, and value&lt;/h2&gt;
&lt;p&gt;Objective is based on a metric (increase clickthrough or revenue per user).&lt;/p&gt;
&lt;p&gt;Demand – if adding a new user feature, how can demand for the feature be assessed?&lt;/p&gt;
&lt;p&gt;value = benefit - cost.&lt;/p&gt;
&lt;p&gt;Expected value helps with decisions about the size and duration of an experiment&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;constructs-metrics-and-scoping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Constructs, metrics, and scoping&lt;/h2&gt;
&lt;p&gt;Once an objective is clear, the details of what can be measured need to be flushed out.&lt;/p&gt;
&lt;p&gt;Metrics (fill in from udamy course section on metrics)&lt;/p&gt;
&lt;p&gt;Netflix metrics – streaming hours, retention (users staying on platform), viewing for a title (e.g. effected by artwork – but may be at the detrement of general viewing?)&lt;/p&gt;
&lt;div id=&#34;user-flow-and-target-metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;User flow and target metrics&lt;/h3&gt;
&lt;p&gt;Think of the sequence of actions a user might take on the site. Experiments can target transtition probabilities between any user/platform states.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of clicks&lt;/li&gt;
&lt;li&gt;time on page&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;invariants&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Invariants&lt;/h3&gt;
&lt;p&gt;Metrics that shouldn’t change or differ across groups. For example, demographic variables should be the same across groups if the randomization or balancing worked properly. Also, many application performance metrics and business metrics should be unchanged, or monitored just in case they change.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confounding-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confounding variables&lt;/h3&gt;
&lt;p&gt;Mitigated by randomization.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;precision-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Precision variables&lt;/h3&gt;
&lt;p&gt;Can also highlight features that one would want to match treatment and control on.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of posts&lt;/li&gt;
&lt;li&gt;number of followers&lt;/li&gt;
&lt;li&gt;visibility / impressions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;metric-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metric Validation&lt;/h2&gt;
&lt;p&gt;User expreience research, retrospective analyses of past data or log files, … .&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditions&lt;/h2&gt;
&lt;p&gt;Define the experimental manipulations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;treatment-units&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Treatment Units&lt;/h2&gt;
&lt;p&gt;What is the unit at which we assign treatments?&lt;/p&gt;
&lt;p&gt;Ideally independent individuals, but online it is hard to know who is visiting a webpage, or crucial to keep experiences comparable across devices. Also, in networks, individuals are not independent and it is important to keep user experience consistent across connected individuals.&lt;/p&gt;
&lt;p&gt;Proxies for individuals include;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User ID or account – most clearly tied to a user, but&lt;/li&gt;
&lt;li&gt;cookies – device and browser specific, so these could differ across a user’s browsers or devices.&lt;/li&gt;
&lt;li&gt;IP address (device request return address) – device specific, so a user’s experience might differ across devices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In networks, loosely connected clusters of individuals can be used as experimental units (see unofficial google data science blog post on this).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assignment-mechanism&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assignment Mechanism&lt;/h2&gt;
&lt;p&gt;The assignment mechanism samples members from a population and assignes them to conditions.&lt;/p&gt;
&lt;p&gt;How are units assigned to treatments?&lt;/p&gt;
&lt;p&gt;Randomized control trials are the ideal, but many issues arise in online experimentation settings.&lt;/p&gt;
&lt;p&gt;(look up desirable properties form causal inference notes)&lt;/p&gt;
&lt;div id=&#34;population&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Population&lt;/h3&gt;
&lt;p&gt;What group is being sampled from?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cohorts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cohorts&lt;/h3&gt;
&lt;p&gt;Random sample, cluster-based, stratified, serial, balanced, …&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;synthetic-control-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Synthetic Control Groups&lt;/h3&gt;
&lt;p&gt;Propensity matching&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Limitations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Non-compliance: Those assicned to treatment may not actually experience the treatment.&lt;/li&gt;
&lt;li&gt;Crossover: Those assigned to control might gain access to the treatment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;p&gt;Batched, or real-time&lt;/p&gt;
&lt;div id=&#34;size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Size&lt;/h2&gt;
&lt;p&gt;power, sample size&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;duration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Duration&lt;/h2&gt;
&lt;div id=&#34;temporal-variation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Temporal variation&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;optional-stopping&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Optional stopping&lt;/h3&gt;
&lt;p&gt;Why it’s an issue for frequentists&lt;/p&gt;
&lt;p&gt;Optamizely using a threshold on FDR from likelihood ratio tests in frequentist setting&lt;/p&gt;
&lt;p&gt;Bayesian justifications&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;estimands&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimands&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Average treatment effect&lt;/li&gt;
&lt;li&gt;Treatment on treated&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;effiencicy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Effiencicy&lt;/h2&gt;
&lt;p&gt;e.g. with a two sample t-test, &lt;span class=&#34;math inline&#34;&gt;\(Y_t - Y_{t-1} = \beta_0\)&lt;/span&gt;, versus a model &lt;span class=&#34;math inline&#34;&gt;\(Y_t = \beta_0 + \beta_1 Y_{t-1}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;– get table for different tests of means from soc sci 10 notes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Testing&lt;/h1&gt;
&lt;div id=&#34;binary-or-proportions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binary or Proportions&lt;/h2&gt;
&lt;div id=&#34;fishers-z&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fisher’s Z&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;real-valued&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Real-Valued&lt;/h2&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Linear Models&lt;/h1&gt;
&lt;p&gt;regression&lt;/p&gt;
&lt;p&gt;t-tests&lt;/p&gt;
&lt;p&gt;ANOVA&lt;/p&gt;
&lt;p&gt;t-test, ANOVA&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;glms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;GLMs&lt;/h1&gt;
&lt;p&gt;Stats: descriptive, R-squared, chi-squared;&lt;/p&gt;
&lt;p&gt;Probability: distributions, CLT, sampling distributions, p-value,&lt;/p&gt;
&lt;p&gt;Stats: k-s, Q-Q plot, hypothesis testing, experimentation;&lt;/p&gt;
&lt;p&gt;Probability: Bayes, bootstrap&lt;/p&gt;
&lt;p&gt;Probability: maximum likelihood estimation; time series analysis (ARIMA models), granger causality&lt;/p&gt;
&lt;p&gt;Dynamic experimentation (multi-armed bandits)&lt;/p&gt;
&lt;p&gt;Unit testing; EDA visualization (seaborn, Plotly, Bokeh)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;emperical-variance-estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Emperical Variance Estimation&lt;/h1&gt;
&lt;p&gt;bootstrapping&lt;/p&gt;
&lt;p&gt;A/A testing&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Yelp Notes</title>
      <link>/MacStrelioff/insightstudying/yelp-deep-dive/</link>
      <pubDate>Thu, 01 Aug 2019 15:33:43 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/yelp-deep-dive/</guid>
      <description>


&lt;div id=&#34;yelp&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Yelp&lt;/h1&gt;
&lt;div id=&#34;company-visit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Company Visit&lt;/h2&gt;
&lt;div id=&#34;business-model-culture-and-vision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Business Model, Culture, and Vision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mission: Connect people with great local places&lt;/li&gt;
&lt;li&gt;Main source of revenue is business ads, &lt;a href=&#34;https://www.thedailybeast.com/yelps-new-ad-products-can-increase-potential-customer-engagement-by-38&#34;&gt;overview of offerings here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biz.yelp.com/guide_to_success&#34;&gt;More services for businesses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Want: openness, curiosity, approachable&lt;/li&gt;
&lt;li&gt;DS team has “Office hours” to facilitate communication&lt;/li&gt;
&lt;li&gt;Journal clubs and learning groups&lt;/li&gt;
&lt;li&gt;Can move to PM or ML teams&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;projects-tasks-and-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Projects, Tasks, and Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Working on personalization&lt;/li&gt;
&lt;li&gt;e.g. project: decrease experiment duration&lt;/li&gt;
&lt;li&gt;not much collaboration with UX other than maybe getting data from them&lt;/li&gt;
&lt;li&gt;data: crowd-sourced data on businesses&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ds-role&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DS Role&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Looking for 2 roles&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;DS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Job consists of reining in management, helping with experimentation. Common deliverables are a Jupyter notebook and reports.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deliverables are reports and Jupyter notebooks&lt;/li&gt;
&lt;li&gt;More confident decisions based on data&lt;/li&gt;
&lt;li&gt;Investigate and define metrics&lt;/li&gt;
&lt;li&gt;Design and analyze experiments on centralized platform.&lt;/li&gt;
&lt;li&gt;Helps with scaling up experiments and interpreting results.&lt;/li&gt;
&lt;li&gt;Predictive modeling&lt;/li&gt;
&lt;li&gt;Qualities: Generalist, communication, statistical inference, experimental design, empathy of understanding of other roles in the business&lt;/li&gt;
&lt;li&gt;Skills: SQL, Python (possibly R for analysis), clean code, reproducible results.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;ML&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Back end of production models&lt;/li&gt;
&lt;li&gt;Assists DE team with feature engineering&lt;/li&gt;
&lt;li&gt;Want: ML, DE, Python, Java, CS and mathematics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recruiter-call&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recruiter Call&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Q: why did you apply to Yelp? Show enthusiasm for yelp during interview.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interview topics and evaluation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Technical skills (analysis, stats, programming)&lt;/li&gt;
&lt;li&gt;product/bus intuition – impact for business opportunities&lt;/li&gt;
&lt;li&gt;communication – clarity and insight&lt;/li&gt;
&lt;li&gt;Can discuss past work with large data or complicated work&lt;/li&gt;
&lt;li&gt;Might get metrics or experimentation based on questions Yelp is facing at the moment&lt;/li&gt;
&lt;li&gt;A/B testing, Python&lt;/li&gt;
&lt;li&gt;Enthusiasm for company – why yelp versus another company&lt;/li&gt;
&lt;li&gt;Communicate while going along while solving the problems.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Other info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Team: ~ 600 engs, 70 PMs, 20 DS.&lt;/li&gt;
&lt;li&gt;DS mostly work with feature team, to create, calculate, and validate metrics for feature work. Modeling user engagement, predicting delivery time,&lt;/li&gt;
&lt;li&gt;DS works on sub team – Bus, consumer, core&lt;/li&gt;
&lt;li&gt;DS works with product groups, growth, contributions, ads.&lt;/li&gt;
&lt;li&gt;PM prioritizes projects based on impact.&lt;/li&gt;
&lt;li&gt;DS Helps with A/B testing, experimentation.. Looking to solve&lt;/li&gt;
&lt;li&gt;Can work with 1-3PMs within group. 50-70% of time working on projects for product groups. Most time is getting and giving feedback on approach to specific problems.&lt;/li&gt;
&lt;li&gt;Expert on knowing the data we can work with to address PM problems.&lt;/li&gt;
&lt;li&gt;Measure “goodness” of a feature in scientific and principled way.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What role am I being considered for?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Don’t have anything posted on careers page. This role just opened. Role just opened up. Data scientist rec.&lt;/p&gt;
&lt;p&gt;can send panel and links to their linkedin – meeting with four folks. Sebastian, Inhan, Nick, Peter.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What topics might be covered?&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Should I bring anything? Laptop, … ? Could bring laptop, but there are laptops that can be borrowed as well. Whiteboards will be in the room.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-dive&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deep Dive&lt;/h2&gt;
&lt;div id=&#34;users&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Users&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;website visitors who write and read reviews&lt;/li&gt;
&lt;li&gt;businesses who pay for exposure&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Metrics&lt;/h3&gt;
&lt;p&gt;Overall, consider units of individuals (accounts, unique visitors, cookies, IPs…), units of time (daily, weekly), and referents (week over week, …)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monthly unique visitors&lt;/li&gt;
&lt;li&gt;Web Reservations&lt;/li&gt;
&lt;li&gt;Pad Reservations&lt;/li&gt;
&lt;li&gt;Wait times at restaurants&lt;/li&gt;
&lt;li&gt;Traffic to business pages&lt;/li&gt;
&lt;li&gt;Engagement (from potential customers)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://biz.yelp.com/&#34;&gt;Some metrics for business partners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;current-issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Current issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Trust, businesses can game Yelp by &lt;a href=&#34;https://thehustle.co/botto-bistro-1-star-yelp/&#34;&gt;asking for low ratings&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Trust, Businesses accuse Yelp of &lt;a href=&#34;https://www.businessinsider.com/court-rules-yelp-can-manipulate-reviews-2014-9&#34;&gt;maliciously manipulating reviews&lt;/a&gt;, to such an extent that, in response to a documentary about this, Yelp redirected BillionDollerBully.com to &lt;a href=&#34;https://www.yelp.com/extortion&#34;&gt;this response&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Influence: “Such ubiquity has consequences. A study from last year found that a measly half-star difference made it 19% more likely that a San Francisco restaurant would be busy at peak times.” - &lt;a href=&#34;https://www.buzzfeed.com/sandraeallen/is-yelp-evil-or-just-misunderstood&#34;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lack of expected growth: “He cited a lack of growth from several of the site’s advertising efforts for his downgrade, as well as a lack of “consequential” new products “to reduce churn or drive revenue.” &lt;a href=&#34;https://www.barrons.com/articles/what-the-democratic-debates-mean-for-health-care-investors-51564665029&#34;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Balancing review usefulness, reliability, and legitimacy -&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;competitors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Competitors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;“On one extreme, Angie’s List features reviews of local products and services provided by and visible to its fee-paying members. On the other, Urbanspoon and CitySearch, Tripadvisor, and Amazon don’t really scrutinize reviewers or reviews.” - &lt;a href=&#34;https://www.buzzfeed.com/sandraeallen/is-yelp-evil-or-just-misunderstood&#34;&gt;this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Full paragraph: “For better and for worse, strangers’ opinions affect our purchasing behavior to an unprecedented degree. Such feedback, of course, is easily corrupted. Websites like Yelp face the problem of whether and how to scrutinize the data they receive to ensure reliability. On one extreme, Angie’s List features reviews of local products and services provided by and visible to its fee-paying members. On the other, Urbanspoon and CitySearch, Tripadvisor, and Amazon don’t really scrutinize reviewers or reviews. While more democratic, they’re also much more susceptible to manipulation and therefore theoretically less useful to consumers. If a business owner or her friend or someone she pays can pose as a happy customer — or sabotage her competition — a review is no more useful than an ad.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;onsite&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Onsite&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Expect from company visit: SQL, Python, statistics, experiment design&lt;/li&gt;
&lt;li&gt;Expect from recruiter call: A/B testing, product sense&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;meeting-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;meeting notes&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Computer Science</title>
      <link>/MacStrelioff/insightstudying/computer-science/</link>
      <pubDate>Sun, 28 Jul 2019 17:10:52 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/computer-science/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#foundations&#34;&gt;Foundations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#complexity&#34;&gt;Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#computational-complexity&#34;&gt;Computational Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#constant-time-o1&#34;&gt;Constant Time: O(1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logrithmic-time-ologn&#34;&gt;Logrithmic Time: O(log(n))&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polynomial-time-on-on2&#34;&gt;Polynomial Time: O(n), O(n^2), …&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exponential-time-oxn&#34;&gt;Exponential Time: &lt;span class=&#34;math inline&#34;&gt;\(O(x^n)\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logrithmic-time-ologn-1&#34;&gt;Logrithmic Time: O(log(N))&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-structures&#34;&gt;Data Structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trees&#34;&gt;Trees&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#binary-search-trees&#34;&gt;Binary search trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#database-and-sql&#34;&gt;Database and SQL&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#general-structure-of-sql-querys&#34;&gt;General Structure of SQL querys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aggregation-functions&#34;&gt;Aggregation functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#window-functions&#34;&gt;Window functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sorting&#34;&gt;Sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#searching&#34;&gt;Searching&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear-search-binary-search&#34;&gt;Linear Search: Binary Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graph-search-dfs-and-bfs&#34;&gt;Graph Search: DFS and BFS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ranking-algorithms&#34;&gt;Ranking Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recommendation&#34;&gt;Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#algorithms&#34;&gt;Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tools&#34;&gt;Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#general-practice-problems&#34;&gt;General Practice Problems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;foundations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Foundations&lt;/h1&gt;
&lt;p&gt;Data type manipulation (char, string, numeric, binary, ascii) regular expressions;&lt;/p&gt;
&lt;div id=&#34;complexity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Complexity&lt;/h2&gt;
&lt;p&gt;The worst-case complexity of an algorithm is represented with big-O notation. Big-O notation is adapted from mathematics where &lt;span class=&#34;math inline&#34;&gt;\(O(f(n))\)&lt;/span&gt; is used to represent the terms that remain relevant when taking a limit of the computations required by the algorithm, &lt;span class=&#34;math inline&#34;&gt;\(f(n)\)&lt;/span&gt;, as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; approaches &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. The same notation is used for time complexity (number of operations) and for space complexity (memory requirements).&lt;/p&gt;
&lt;div id=&#34;computational-complexity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Computational Complexity&lt;/h3&gt;
&lt;p&gt;Here I’ll check the number of computations required by algorithms of different complexities.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ns_to_test = [0,1,10,20]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;constant-time-o1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Constant Time: O(1)&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(O(1)\)&lt;/span&gt; represents comstant time complexity – a component of an algorithm that is only performed once, regardless of the input size.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def constant_example(n):
    num_ops = 1      # 1 operation
    num_ops +=1      # 1 operation
    return num_ops   # total: 2 operations


for n in ns_to_test:
    out=constant_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 2
&amp;gt;&amp;gt;&amp;gt; f(1): 2
&amp;gt;&amp;gt;&amp;gt; f(10): 2
&amp;gt;&amp;gt;&amp;gt; f(20): 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logrithmic-time-ologn&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Logrithmic Time: O(log(n))&lt;/h4&gt;
&lt;p&gt;Very slowly increases in computational demand. It can result from splitting the input on each recusrive call.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def log_example(n,num_ops=0):
    # 1 operation per call
    num_ops += 1
    # if n&amp;gt;1, half n and recursively call again
    if n&amp;gt;=1:
        n/=2
        num_ops += log_example(n)
    return num_ops
        
for n in ns_to_test:
    out=log_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 1
&amp;gt;&amp;gt;&amp;gt; f(1): 2
&amp;gt;&amp;gt;&amp;gt; f(10): 5
&amp;gt;&amp;gt;&amp;gt; f(20): 6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-time-on-on2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Polynomial Time: O(n), O(n^2), …&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=P8Xa2BitN3I&amp;amp;list=PLI1t_8YX-ApvMthLj56t1Rf-Buio5Y8KL&amp;amp;index=11&#34;&gt;good hackerrank video with explanation of polynomial complexity for a recursive solution to fibbinochi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Each for loop scales complexity by a factor of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, so one loop would be linear (&lt;span class=&#34;math inline&#34;&gt;\(O(n)\)&lt;/span&gt;), two loops would be quadratic &lt;span class=&#34;math inline&#34;&gt;\(O(n^2)\)&lt;/span&gt;, and three loops would be cubic (&lt;span class=&#34;math inline&#34;&gt;\(O(n^3)\)&lt;/span&gt;), and so on. The example below is a quadratic time example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def linear_example(n): 
    num_ops = 1         # 1 operation
    for i in range(n):  # n times, perform...
        num_ops +=1     # 1 operation (n*1)
    return num_ops      # 1+n*1 = 1+n operations, O(n)


for n in ns_to_test:
    out=linear_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 1
&amp;gt;&amp;gt;&amp;gt; f(1): 2
&amp;gt;&amp;gt;&amp;gt; f(10): 11
&amp;gt;&amp;gt;&amp;gt; f(20): 21&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def quadratic_example(n):
    num_ops = 1             # 1 operation
    for i in range(n):      # n times
        num_ops+=1          # 1 operation (n*1)
        for j in range(n):  # n times (n*n)
            num_ops+=1      # 1 operation (1*n*n)
    return num_ops          # total 1 + 1*n + 1*n*n, O(n^2)


for n in ns_to_test:
    out=quadratic_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 1
&amp;gt;&amp;gt;&amp;gt; f(1): 3
&amp;gt;&amp;gt;&amp;gt; f(10): 111
&amp;gt;&amp;gt;&amp;gt; f(20): 421&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-time-oxn&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Exponential Time: &lt;span class=&#34;math inline&#34;&gt;\(O(x^n)\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;With exponential time, the number of operations increases by a constant &lt;em&gt;factor&lt;/em&gt; whith the length of the input. An example of this is in recursion, where a function iteratively calls itself &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times for each element of the input.&lt;/p&gt;
&lt;p&gt;In the example below, I call the function twice within each call. The complexity is then a geometric series, and the closed for solution for the number of operations can be found with;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_{i=1}^N ar^{i-1} = \frac{a(1-r^{N})}{1-r} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(r=2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(a=2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is looped over, and we start the index at 0 (&lt;span class=&#34;math inline&#34;&gt;\(i-1\)&lt;/span&gt;) instead of 1, so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_{i=0}^n 2*2^{i} = \frac{(1-2^{n+1})}{1-2} = -(1-2^{n+1}) = 2^{n+1}-1 \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def exponential_example(n,num_ops=0):
    num_ops += 1 # 1 operation per call
    # recursively called twice for each n&amp;gt;0 (2**n times)    
    if n &amp;gt; 0:
        n-=1
        num_ops += exponential_example(n) # 2**(n)
        num_ops += exponential_example(n) # 2**(n)
    return num_ops # 
    
for n in ns_to_test:
    out=exponential_example(n)
    print(&amp;#39;f({}): {}&amp;#39;.format(n,out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; f(0): 1
&amp;gt;&amp;gt;&amp;gt; f(1): 3
&amp;gt;&amp;gt;&amp;gt; f(10): 2047
&amp;gt;&amp;gt;&amp;gt; f(20): 2097151&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logrithmic-time-ologn-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Logrithmic Time: O(log(N))&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-structures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Structures&lt;/h2&gt;
&lt;p&gt;basic structures (tuples, list, arrays, dict), list comprehensions,&lt;/p&gt;
&lt;p&gt;Good descriptions of &lt;a href=&#34;https://runestone.academy/runestone/books/published/pythonds/AlgorithmAnalysis/Lists.html&#34;&gt;lists&lt;/a&gt;, &lt;a href=&#34;https://runestone.academy/runestone/books/published/pythonds/AlgorithmAnalysis/Dictionaries.html&#34;&gt;dicts&lt;/a&gt;, and &lt;a href=&#34;https://stackoverflow.com/questions/8929284/what-makes-sets-faster-than-lists&#34;&gt;why some operations are much faster in sets or dict keys&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In depth description of &lt;a href=&#34;https://en.wikipedia.org/wiki/Hash_table?fbclid=IwAR1N5-jAoM9e6iY58CDP9MAwycenvOXXJwmkpa0eBVDKed3RBs9uVBm9Kkc&#34;&gt;hash tables&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Data structures (hash, stack, queue, tree, linked list);&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trees&lt;/h2&gt;
&lt;div id=&#34;binary-search-trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binary search trees&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;database-and-sql&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Database and SQL&lt;/h2&gt;
&lt;p&gt;SQL training resources;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://classroom.udacity.com/courses/ud198&#34;&gt;Udacity course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pgexercises.com/&#34;&gt;pgexercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sqlbolt.com/lesson/select_queries_introduction&#34;&gt;sqlbolt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/&#34;&gt;Mode Analytics&lt;/a&gt; HackerRank (basic select, aggregation)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.hackerrank.com/domains/sql&#34;&gt;Hacker Rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.w3schools.com/sql/exercise.asp?filename=exercise_select1&#34;&gt;W3School&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;general-structure-of-sql-querys&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;General Structure of SQL querys&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sqlbolt.com/lesson/select_queries_order_of_execution&#34;&gt;Order of execution for clauses below&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;```{, eval=FALSE, echo=TRUE} SELECT (column names to select) [AS (name for selection)]&lt;/p&gt;
&lt;p&gt;FROM (table to select from)&lt;/p&gt;
&lt;p&gt;WHERE (logical condition on rows) column = (value or subquery)&lt;/p&gt;
&lt;p&gt;GROUP BY&lt;/p&gt;
&lt;p&gt;HAVING (constraint on grouped rows)&lt;/p&gt;
&lt;p&gt;LIMIT n [OFFSET m]&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;SELECT&lt;/code&gt; clause can contain aggregations, and subqueries.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;FROM&lt;/code&gt; clause can contain joins.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;WHERE&lt;/code&gt; clause can also contain sub queries.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregation-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Aggregation functions&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;GROUP BY&lt;/code&gt; clause can contain &lt;code&gt;GROUPING SETS&lt;/code&gt;, such as a &lt;code&gt;ROLLUP()&lt;/code&gt; and &lt;code&gt;CUBE()&lt;/code&gt; functions. These hierarchically group the rows passed to the aggregation functions. &lt;code&gt;ROLLUP()&lt;/code&gt; hierarchically groups by arguments, starting left to right. &lt;code&gt;CUBE()&lt;/code&gt; passes all permutations of its arguments to the aggregation functions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pgexercises.com/questions/aggregates/fachoursbymonth3.html&#34;&gt;Grouping sets are described a little here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;window-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Window functions&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://pgexercises.com/questions/aggregates/countmembers.html&#34;&gt;Orver() explained here&lt;/a&gt;. Some more detailed information &lt;a href=&#34;http://www.sqlservertutorial.net/sql-server-window-functions/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Window functions include &lt;code&gt;OVER()&lt;/code&gt; and &lt;code&gt;ROW_NUMBER()&lt;/code&gt;. They take two optional arguments, &lt;code&gt;PARTITION BY&lt;/code&gt; and &lt;code&gt;ORDER BY&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sorting&lt;/h1&gt;
&lt;p&gt;sorting (bubble, selection, insertion, merge, quick)&lt;/p&gt;
&lt;p&gt;Sorting and graph algos (depth-first and breadth-first seach)&lt;/p&gt;
&lt;p&gt;sorting (heap, tim)&lt;/p&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://www.geeksforgeeks.org/sorting-algorithms/&#34;&gt;sort algorithms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;e.g. ‘find median in a large dataset’ – sort half of it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;searching&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Searching&lt;/h1&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Category:Search_algorithms&#34;&gt;search algorithms&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;linear-search-binary-search&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Search: Binary Search&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=P3YID7liBug&#34;&gt;good hackerrank video&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;requires sorting before applying the search&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;graph-search-dfs-and-bfs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Graph Search: DFS and BFS&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zaBhtODEL0w&#34;&gt;good hackerrank video&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DFS can use a dictionary or set to&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-algorithms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ranking Algorithms&lt;/h1&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Ranking_(information_retrieval)&#34;&gt;ranking algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recommendation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recommendation&lt;/h1&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Recommender_system&#34;&gt;recommendation algorithms&lt;/a&gt;&lt;/p&gt;
&lt;!---
Data science methods play a major role in discovering recommendations for users. 

Econ, compliments and substitutes (competitors). 

Types of recommendation; compliments (these go together), competitors (you might also like...), temporal (might buy this again in the future). Horizontal and vertical focus 

Temporal involves forecasting, which is a topic for another post. 
---&gt;
&lt;/div&gt;
&lt;div id=&#34;algorithms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Algorithms&lt;/h1&gt;
&lt;p&gt;lambda functions;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
nums=[1,1,2,2,1]
val=1

count = 0
for i in range(len(nums)):
  if nums[i] != val:
    nums[count] = nums[i]
    count += 1

print(count)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(nums)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; [2, 2, 2, 2, 1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;recursion;&lt;/p&gt;
&lt;p&gt;Dijkstra’s algorithm, dynamic programming (knapsack problem, Fibonacci sequence)&lt;/p&gt;
&lt;p&gt;Functional programming&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tools&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tools&lt;/h1&gt;
&lt;p&gt;git for version control (resolving merge conflicts)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-practice-problems&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;General Practice Problems&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.hackerrank.com/dashboard&#34;&gt;HackerRank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://leetcode.com/&#34;&gt;LeetCode&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Behavioral Interviewing</title>
      <link>/MacStrelioff/insightstudying/behavioral-interviews/</link>
      <pubDate>Sun, 21 Jul 2019 22:29:35 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/behavioral-interviews/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#principles&#34;&gt;Principles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-behavioral-questions&#34;&gt;Common Behavioral Questions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tell-me-a-little-about-your-background.&#34;&gt;“Tell me a little about your background.”&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#child-bandits&#34;&gt;Child bandits:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tell-me-about-the-project-you-worked-on-at-insight.&#34;&gt;“Tell me about the project you worked on at Insight.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-are-you-interested-in-a-career-as-a-data-scientist&#34;&gt;“Why are you interested in a career as a data scientist?”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tell-me-about-your-research-talk-to-me-about-your-last-role.&#34;&gt;“Tell me about your research / talk to me about your last role.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-are-you-interested-in-joining-this-company&#34;&gt;“Why are you interested in joining this company?”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#talk-about-a-time-you-were-on-a-team-that-struggled-with-effective-communication.&#34;&gt;“Talk about a time you were on a team that struggled with effective communication.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tell-me-about-a-time-you-made-a-mistake-at-work.&#34;&gt;“Tell me about a time you made a mistake at work.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#talk-about-a-situation-that-required-you-to-explain-your-technical-work-to-someone-outside-your-industry-or-field.&#34;&gt;“Talk about a situation that required you to explain your technical work to someone outside your industry or field.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#talk-about-a-time-you-were-asked-to-manage-an-important-project.&#34;&gt;“Talk about a time you were asked to manage an important project.”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-dives&#34;&gt;Deep-Dives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;principles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Principles&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Interviews are conversations, not tests&lt;/li&gt;
&lt;li&gt;Use stories to establish a connection and demonstrate fit for a role and company&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tell me about yourself, your academic research, why data science?; 30 second elevator pitch vs. 2 minute phone screen/on-site&lt;/p&gt;
&lt;p&gt;Why this company? what value can you add? Tell me about a time when…&lt;/p&gt;
&lt;p&gt;STAR chart (Situation, Task, Action, Result)&lt;/p&gt;
&lt;p&gt;Mock interviews: Practice, practice, practice&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-behavioral-questions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Common Behavioral Questions&lt;/h1&gt;
&lt;div id=&#34;tell-me-a-little-about-your-background.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Tell me a little about your background.”&lt;/h2&gt;
&lt;p&gt;This question gives an opportunity to demonstrate clear communication, curiosity, enthusiasm, transferrable skills, and value adds. This is also an opportunity to align past experience with the current job opportunity.&lt;/p&gt;
&lt;p&gt;Template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Problem: Concepts and context&lt;/li&gt;
&lt;li&gt;Curiosity: Open question&lt;/li&gt;
&lt;li&gt;Value: Why should people care&lt;/li&gt;
&lt;li&gt;Project Outline: Methods, results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open: I recently graduates from UC Irvine, where I studied statistics and cognitive science. I worked on a few lines of research, the one that was most interesting to me came out of an interest in prediction markets and arbitrage.&lt;/li&gt;
&lt;li&gt;Problem: So prediction markets are a platform where people can buy and sell contracts on events that pay a dollar if the event occurs and nothing if it doesn’t occur. So for example, one market might be Who will win the 2020 election, and contracts might be ‘Bernie Sanders’, ‘Andrew Yang’, ‘Donnald Trump’, … . Since the contracts pay out $1, the theory is that the price should estimate the underlying probability of the event occuring. And since the market is aggregating information across individuals, it should be a pretty good estimate. However, individuals have shown a number of biases in probabilistic reasoning in lab studies, so I was wondering if these same biases would exist in prediction market prices. If they did, then I could make profitable trading algorithms.&lt;/li&gt;
&lt;li&gt;So I partnered with a large online prediction market to get access to their data, and found evidence of some biases consistent with those observed in lab studies.&lt;/li&gt;
&lt;li&gt;Trajectory: Overall, I realized that I enjoy these kinds of impactful analyses of real-world data, moreso than theory building through laboratory studies, so I decided to bridge into a career in data science, and joined Insight to become part of a network of data scientists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Communication: Clearly explain prediction market mechanics.&lt;/li&gt;
&lt;li&gt;Enthusasism: Interest in markets and complex problems.&lt;/li&gt;
&lt;li&gt;Transferrable skills: Mention some of the models used.&lt;/li&gt;
&lt;li&gt;Value adds: Realized prediction markets are biased close to expiration – their signal is weaker than previously thought. Also able to profit literally by rolling out trading algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;child-bandits&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Child bandits:&lt;/h3&gt;
&lt;p&gt;Bandit tasks are a common paradigm for studying individual level decision making and making claims about the way people make decisions. But in lots of contexts, the results you get when you addregate over trials can be different from the actual result on any trial. So I developed a model to infer on a trial by trial level, what decision making strategy kids were using to choose between two ‘bandits’ that gave out stickers. – I have an interest in using more customized models and Bayesian inference to overcome issues that arise from aggregating data at too high a level. If you had averaged over trials, then there may have been evidence for one or another model, but the dynamic changes in decision making strategies would have gone unnoticed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Similar approaches could be useful for dynamic experimental designs, OR in the context of Netflix, for personalization. If we could dynamically infer something like the strategy or emotional state of a user, then we could use that inference to personalize our recommended content or things like cover art or other aesthetic aspects of Netflix.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-me-about-the-project-you-worked-on-at-insight.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Tell me about the project you worked on at Insight.”&lt;/h2&gt;
&lt;p&gt;This question gives an opportunity to demonstrate what you’ll be capable of on Day 1.&lt;/p&gt;
&lt;p&gt;Template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open: At Insight I worked on a consulting project for an external company. This company acquired users by offering a free trial, after which users would subscribe.&lt;/li&gt;
&lt;li&gt;Purpose: Goals of the project were to to 1) identify, early on, who would become a subscriber, and 2) identify how the successful users were using the product.&lt;/li&gt;
&lt;li&gt;Approach: I usually think about a generative process. Here I framed the problem as a classification problem and thought about using LDA or QDA because I thought the distributions of features would be different based on the classes.&lt;/li&gt;
&lt;li&gt;Methods: Later thought about other algorithms because this was primarily a prediction problem, and because the assumption of normally distributed features was violated. So I tried tree-based algorithms and those performed the best across all metrics – F1 score, accuracy, precision, and recall.&lt;/li&gt;
&lt;li&gt;Challenges: Non-normal features (could break LDA, QDA)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How could I have used a mixed effects model here? (See Maime’s blog?)&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clarity:&lt;/li&gt;
&lt;li&gt;Relevance: Do the skills add value to the company?&lt;/li&gt;
&lt;li&gt;Logic: Were sensible solutions tried in a sensible order?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-are-you-interested-in-a-career-as-a-data-scientist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Why are you interested in a career as a data scientist?”&lt;/h2&gt;
&lt;p&gt;This is an opportunity to show that becoming a data sceientist at the company is the logical next career move, and to demonstrate that you know what to expect in the new role.&lt;/p&gt;
&lt;p&gt;Outline of topics that may resonate with a hiring manager:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Skills, Technical: Tools, technologies, methods you have used.&lt;/li&gt;
&lt;li&gt;Skills, Conceptual: Your approach to understanding problems and navigating to solutions&lt;/li&gt;
&lt;li&gt;Interests: What are you eager to do in this next phase of your professional life?&lt;/li&gt;
&lt;li&gt;Interests, Broad: Knowledge of problems tackled by data professionals in this industry&lt;/li&gt;
&lt;li&gt;Interests, Specific: Knowledge of technical challenges that will be faced by someone in this role&lt;/li&gt;
&lt;li&gt;Growth, Skills: What you’re excited to learn in the next few years&lt;/li&gt;
&lt;li&gt;Growth, Trajectory: Relate this job to an ultimate career trajectory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example answer: When I was in my PhD, I worked on a variety of projects and realized that I most enjoyed the ones that derived some impactful analysis from larger scale data – namely the prediction market project.&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Educated Enthusasiam:&lt;/li&gt;
&lt;li&gt;Professional Goals:&lt;/li&gt;
&lt;li&gt;Niche area:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-me-about-your-research-talk-to-me-about-your-last-role.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Tell me about your research / talk to me about your last role.”&lt;/h2&gt;
&lt;p&gt;Same as in ‘tell me about your background..’&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Skills:&lt;/li&gt;
&lt;li&gt;Relevance: Show how previous work relates to the responsibilities of the current role.&lt;/li&gt;
&lt;li&gt;Clarity:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-are-you-interested-in-joining-this-company&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Why are you interested in joining this company?”&lt;/h2&gt;
&lt;p&gt;This is an opportunity to demonstrate knowledge of and interest in the current role.&lt;/p&gt;
&lt;p&gt;Depends on deep-dive.&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Industry knowledge:&lt;/li&gt;
&lt;li&gt;Company knowledge:&lt;/li&gt;
&lt;li&gt;Potential for impact:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;talk-about-a-time-you-were-on-a-team-that-struggled-with-effective-communication.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Talk about a time you were on a team that struggled with effective communication.”&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Situation: Where / when did this happen? What were you working on, and with whom?&lt;/li&gt;
&lt;li&gt;Task: What were you responsible for? What challenges did you face?&lt;/li&gt;
&lt;li&gt;Action: How did you decide the appropriate response to the situation? How did you implement it?&lt;/li&gt;
&lt;li&gt;Result: How do you know your solution was successful?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Takeaways: What did you learn from this situation? What would you do the same or differently next time?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Can’t think of one.. In community college I worked as a TA for my mentor, and I lead a research project which is very rare in community college settings. She was extremely supportive and I think communication and goals were clear across the team – I would make outlines / agendas for team meetings.&lt;/li&gt;
&lt;li&gt;At UC Davis, for my Bachelors, my mentor was really bad with email. But he was nice, and his office door was always open, so whenever I needed something I would stop by and chat.&lt;/li&gt;
&lt;li&gt;At Irvine people were always available via email or even phone if needed.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also I had taken a consulting class as part of the stats masters, and a portion of that emphasized the importance of clear communication with clients, so in consulting relationships I maintain clear goals and deliverables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowledge of company values around communication:&lt;/li&gt;
&lt;li&gt;Your principles for communication:&lt;/li&gt;
&lt;li&gt;Awareness of problem:&lt;/li&gt;
&lt;li&gt;Initiative:&lt;/li&gt;
&lt;li&gt;Lessons Learned:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-me-about-a-time-you-made-a-mistake-at-work.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Tell me about a time you made a mistake at work.”&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Situation: Where/when did this happen? What were you working on, and with whom?&lt;/li&gt;
&lt;li&gt;Task: What were you responsible for? What challenges did you face?&lt;/li&gt;
&lt;li&gt;Action: How did you decide the appropriate response to the situation? How did you implement it?&lt;/li&gt;
&lt;li&gt;Result: How do you know your solution was successful?&lt;/li&gt;
&lt;li&gt;Takeaways: What did you learn from this situation? What would you do the same or differently next time?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Forgot to add code to save a questionnaire at the end of an experiment. From that point on I ran myself and analyzed my data before running any real subjects – so kind of like I learned the importance of unit tests from that experience.&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your principles for mistakes:&lt;/li&gt;
&lt;li&gt;Knowledge of company values around rapid iteration, learning through failure, and troubleshooting&lt;/li&gt;
&lt;li&gt;Judgment: How did you explain the mistake (misunderstanding, incomplete information, technical errors)? Was it due to carelessness.&lt;/li&gt;
&lt;li&gt;Responsibility: Did you make excuses, or make solutions?&lt;/li&gt;
&lt;li&gt;Wisdom: Are you likely to avoid a similar mistake in the future? Can lessons here apply more broadly?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;talk-about-a-situation-that-required-you-to-explain-your-technical-work-to-someone-outside-your-industry-or-field.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Talk about a situation that required you to explain your technical work to someone outside your industry or field.”&lt;/h2&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ability to communicate with stakeholders:&lt;/li&gt;
&lt;li&gt;Impact: Understanding of what the other party needed to know&lt;/li&gt;
&lt;li&gt;Adaptability: Techniques (analogy, visualization, …) used to simply communicate ideas. How did you decide what to include. Did you clarify that the explanation made sense.&lt;/li&gt;
&lt;li&gt;Attitude: Will you patiently explain your ideas?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;talk-about-a-time-you-were-asked-to-manage-an-important-project.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Talk about a time you were asked to manage an important project.”&lt;/h2&gt;
&lt;p&gt;PredictIt proejct, since that’s one that I really drove.&lt;/p&gt;
&lt;p&gt;Evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowledge of company values on leadership&lt;/li&gt;
&lt;li&gt;Trust: Why were you chosen to lead?&lt;/li&gt;
&lt;li&gt;Respect and responsibility:&lt;/li&gt;
&lt;li&gt;Delegation:&lt;/li&gt;
&lt;li&gt;People management:&lt;/li&gt;
&lt;li&gt;Executive decision-making:&lt;/li&gt;
&lt;li&gt;Team dynamics:&lt;/li&gt;
&lt;li&gt;Potential: Strengths and weaknesses in leading.&lt;/li&gt;
&lt;li&gt;Interest in management roles:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-dives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Deep-Dives&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/insightdatascience.com/interviewstrategies/interviews/interview-logistics/deep-dives?authuser=0&#34;&gt;Deep Dive guide&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Business Sense</title>
      <link>/MacStrelioff/insightstudying/business-sense/</link>
      <pubDate>Thu, 18 Jul 2019 10:52:53 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/business-sense/</guid>
      <description>


&lt;div id=&#34;problem-solving-strategy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Problem Solving Strategy&lt;/h1&gt;
&lt;p&gt;Overall product sense is about what to build and why.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Overarching goals / stage of the company&lt;/li&gt;
&lt;li&gt;Contextualize: Draw a user funnel or user flow for the specific product&lt;/li&gt;
&lt;li&gt;Draw a dashboard, then populate with platform and product metrics&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;overall-resources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overall resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLOhHNjZItNnM5r2JCX9KzrFH5WYWKrpAA&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/playlist?list=PLOhHNjZItNnM5r2JCX9KzrFH5WYWKrpAA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;platform-concepts-and-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Platform Concepts and Metrics&lt;/h1&gt;
&lt;div id=&#34;users&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Users&lt;/h2&gt;
&lt;div id=&#34;user-funnel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;User Funnel&lt;/h3&gt;
&lt;p&gt;Churn,&lt;/p&gt;
&lt;p&gt;funnel analysis&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;user-flow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;User Flow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traffic source: Where users come from (search, referral, … )&lt;/li&gt;
&lt;li&gt;number new visits, percent new visits&lt;/li&gt;
&lt;li&gt;Page visits: users arriving at page&lt;/li&gt;
&lt;li&gt;Bounce rate: the percentage of visitors to a particular website who navigate away from the site after viewing only one page.&lt;/li&gt;
&lt;li&gt;Pages viewed / visit&lt;/li&gt;
&lt;li&gt;Time on pages&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Time on site&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;lifetime-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lifetime Value&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kpis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;KPIs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Daily active users (DAU)&lt;/li&gt;
&lt;li&gt;Support ticiets: new tickets + unresolved tickets&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Time to ticket resolution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.klipfolio.com/resources/kpi-examples&#34; class=&#34;uri&#34;&gt;https://www.klipfolio.com/resources/kpi-examples&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;monetization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monetization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;customer lifetime value (LTV)&lt;/li&gt;
&lt;li&gt;monthly recurring revenue (MRR)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;product-concepts-and-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Product Concepts and Metrics&lt;/h1&gt;
&lt;div id=&#34;demand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Demand&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;engagement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Engagement&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;clicks&lt;/li&gt;
&lt;li&gt;bounce rate on subsequent page – used combined with clicks to mitigate click bait&lt;/li&gt;
&lt;li&gt;Shares&lt;/li&gt;
&lt;li&gt;Upvotes&lt;/li&gt;
&lt;li&gt;DAU&lt;/li&gt;
&lt;li&gt;time of sessions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Value&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Shares&lt;/li&gt;
&lt;li&gt;Upvotes&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;old-notes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;OLD NOTES:&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;insight-topics-list&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Insight topics list:&lt;/h1&gt;
&lt;p&gt;engagement, search engine optimization (SEO), key-performance indicators (KPIs),&lt;/p&gt;
&lt;p&gt;Experiment design, A/A and A/B testing, power analysis;&lt;/p&gt;
&lt;p&gt;ML focused business case study (features, algorithms, validation, value), business focused data challenges&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;template&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Template&lt;/h1&gt;
&lt;div id=&#34;value-of-a-new-feature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Value of a new feature&lt;/h2&gt;
&lt;p&gt;e.g. higher engagement, better labels for training data or other user behaivors&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deamand-for-a-feature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deamand for a feature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Surveys&lt;/li&gt;
&lt;li&gt;Focus Groups&lt;/li&gt;
&lt;li&gt;UX interviews / shadows&lt;/li&gt;
&lt;li&gt;Retrospective analysis (looking at past user behavior)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;treatment-design&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Treatment Design&lt;/h2&gt;
&lt;p&gt;How exactly will this be implemented in the treatment group?&lt;/p&gt;
&lt;p&gt;Specify treatment and control groups.&lt;/p&gt;
&lt;div id=&#34;issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Networks: Users that interact might be in different conditions&lt;/li&gt;
&lt;li&gt;Learning effects: Users might respond differently initially than they do asymptotically. Can run experiments longer until the treatment effect seems to stabalize.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;constructs-and-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Constructs and Metrics&lt;/h2&gt;
&lt;p&gt;Engagement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lift&lt;/li&gt;
&lt;li&gt;CTR&lt;/li&gt;
&lt;li&gt;Duration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Monetization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CTR&lt;/li&gt;
&lt;li&gt;revenue per user&lt;/li&gt;
&lt;li&gt;add revenue&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Guard Rail Metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;metric-evaluation-and-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metric evaluation and selection&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What if we optimize for … ?: The choice of metrics determines what the platform is being optimized for, so think about what will happen if the platform is optimized for the chosen metric.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Method&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Size&lt;/h3&gt;
&lt;p&gt;Power analysis&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;duration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Duration&lt;/h3&gt;
&lt;p&gt;Bandits versus A/B tests,&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-considerations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other considerations:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Heterogenous treatment effects&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-question-formats&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example Question Formats&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;mock-questions-and-templates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mock questions and templates&lt;/h1&gt;
&lt;div id=&#34;monitoring-twitter-serach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monitoring Twitter serach&lt;/h2&gt;
&lt;p&gt;Metrics:&lt;/p&gt;
&lt;p&gt;Measure health:&lt;/p&gt;
&lt;p&gt;Working well + being used …&lt;/p&gt;
&lt;p&gt;Track number of users using search!&lt;/p&gt;
&lt;p&gt;Consider people who have used the search once.. (can assess need)&lt;/p&gt;
&lt;p&gt;Use of search&lt;/p&gt;
&lt;p&gt;# users that use search&lt;/p&gt;
&lt;p&gt;Usage (prop of users that interact with search)&lt;/p&gt;
&lt;p&gt;# users using search / # active users&lt;/p&gt;
&lt;p&gt;Proportion of times a user comes and uses search&lt;/p&gt;
&lt;p&gt;Time interval(s):&lt;/p&gt;
&lt;p&gt;Daily&lt;/p&gt;
&lt;p&gt;Weekly&lt;/p&gt;
&lt;p&gt;# number of search sessions&lt;/p&gt;
&lt;p&gt;Total search volume (server traffic, … – important!)&lt;/p&gt;
&lt;p&gt;Quality of search&lt;/p&gt;
&lt;p&gt;Chose any suggestion&lt;/p&gt;
&lt;p&gt;Rank of suggestion that they chose&lt;/p&gt;
&lt;p&gt;Look at total number of searches, and searches in different categories / segmentations&lt;/p&gt;
&lt;p&gt;Depends on infrastructure / ease of cutting later&lt;/p&gt;
&lt;p&gt;– favorites, retweets, … Other metrics from a deep-dive.&lt;/p&gt;
&lt;p&gt;CUTS IN DATA&lt;/p&gt;
&lt;p&gt;Think of groups that might be using feature differently&lt;/p&gt;
&lt;p&gt;New vs old users&lt;/p&gt;
&lt;p&gt;Device type (iphone vs android)&lt;/p&gt;
&lt;p&gt;Browser&lt;/p&gt;
&lt;p&gt;Conditions where the feature may be behaving differently (languages, countries, …) ..&lt;/p&gt;
&lt;p&gt;Note: countries would have different bandwidths (country easier to measure than bandiwdth itself)&lt;/p&gt;
&lt;p&gt;Language that the search is using.&lt;/p&gt;
&lt;p&gt;SPIKE IN THE METRIC # of searches&lt;/p&gt;
&lt;p&gt;Bug&lt;/p&gt;
&lt;p&gt;Event&lt;/p&gt;
&lt;p&gt;Think of more people on platform moving with searches…&lt;/p&gt;
&lt;p&gt;# clicks and # searches – if it’s a bug, then clicks prob wouldn’t also spike.&lt;/p&gt;
&lt;p&gt;Impressions, clicks, …&lt;/p&gt;
&lt;p&gt;Top of funnel,&lt;/p&gt;
&lt;p&gt;Logging in, going to search, writing stuff, hitting search .&lt;/p&gt;
&lt;p&gt;‘Ok lets write down some ideas and see how we feel about them … ‘&lt;/p&gt;
&lt;p&gt;Looking for intuition about how things will move and interact?&lt;/p&gt;
&lt;p&gt;Looking for:&lt;/p&gt;
&lt;p&gt;Organized in thinking&lt;/p&gt;
&lt;p&gt;Incl understanding of problem space&lt;/p&gt;
&lt;p&gt;Communicating thinking well&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://datamasked.com/&#34;&gt;Giulio Palombo’s collection of business focused data challenges&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Company deep dives; data+schema brainstorming; ML algorithms; Insights;&lt;/p&gt;
&lt;p&gt;Company blogs are a great resource: check out how StitchFix, Instacart use data science.&lt;/p&gt;
&lt;p&gt;More company deep dives!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>/MacStrelioff/insightstudying/machine-learning/</link>
      <pubDate>Sun, 14 Jul 2019 11:52:10 +0000</pubDate>
      
      <guid>/MacStrelioff/insightstudying/machine-learning/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#considerations&#34;&gt;Considerations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bias-variance-trade-off&#34;&gt;Bias-variance trade-off&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overfitting-and-underfitting&#34;&gt;Overfitting and Underfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cost-functions-and-metrics&#34;&gt;Cost Functions and Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering&#34;&gt;Feature Engineering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#seperability&#34;&gt;Seperability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions-about-form-of-relationship&#34;&gt;Assumptions about form of relationship&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unsupervised&#34;&gt;Unsupervised&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#clustering&#34;&gt;Clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#k-means&#34;&gt;k-means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agglomerative&#34;&gt;Agglomerative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#devisive&#34;&gt;Devisive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-decomposition&#34;&gt;Matrix Decomposition&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#pca-principle-components-analysis&#34;&gt;PCA Principle Components Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#svd-singular-value-decomposition&#34;&gt;SVD Singular Value Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-based&#34;&gt;Text-based&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lda-latent-direchilote-analysis&#34;&gt;LDA Latent Direchilote Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lsa-latent-semantic-analysis&#34;&gt;LSA Latent Semantic Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#supervised&#34;&gt;Supervised&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#regression&#34;&gt;Regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear&#34;&gt;Linear&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#support-vector-machine-svm&#34;&gt;Support Vector Machine (SVM)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification&#34;&gt;Classification&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#naive-bayes&#34;&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discriminant-analysis&#34;&gt;Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#knn&#34;&gt;KNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ensemble-methods&#34;&gt;Ensemble Methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#trees-and-forests&#34;&gt;Trees and Forests&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#regression-1&#34;&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification-1&#34;&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#open-ended-template&#34;&gt;Open ended Template&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#raw-data-structure&#34;&gt;Raw data structure?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#define-target-variable&#34;&gt;Define target variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#preprocession&#34;&gt;Preprocession,&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-engineering-1&#34;&gt;Feature engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance-metrics-and-how-they-apply&#34;&gt;Performance metrics and how they apply&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cautions&#34;&gt;Cautions:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imagine-rolling-the-model-out&#34;&gt;Imagine rolling the model out&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;considerations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Considerations&lt;/h1&gt;
&lt;div id=&#34;bias-variance-trade-off&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bias-variance trade-off&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;overfitting-and-underfitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overfitting and Underfitting&lt;/h2&gt;
&lt;p&gt;Cross Validation; Classification&lt;/p&gt;
&lt;p&gt;Assess by comparing a performance metric on training and testing data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cost-functions-and-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cost Functions and Metrics&lt;/h2&gt;
&lt;p&gt;Modeling: Validation metrics,&lt;/p&gt;
&lt;p&gt;metrics (precision, recall, F1)&lt;/p&gt;
&lt;p&gt;R squared, RMSE&lt;/p&gt;
&lt;p&gt;Likelihood&lt;/p&gt;
&lt;p&gt;regularisation (ridge, lasso),&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Engineering&lt;/h2&gt;
&lt;div id=&#34;seperability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Seperability&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-about-form-of-relationship&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assumptions about form of relationship&lt;/h3&gt;
&lt;p&gt;Process: feature selection, data cleaning / imputation with common pitfalls, model training, bias-variance trade-off&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unsupervised&lt;/h1&gt;
&lt;div id=&#34;clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clustering&lt;/h2&gt;
&lt;div id=&#34;k-means&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;k-means&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;agglomerative&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Agglomerative&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;devisive&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Devisive&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrix Decomposition&lt;/h2&gt;
&lt;div id=&#34;pca-principle-components-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PCA Principle Components Analysis&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;svd-singular-value-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SVD Singular Value Decomposition&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;text-based&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Text-based&lt;/h2&gt;
&lt;div id=&#34;lda-latent-direchilote-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LDA Latent Direchilote Analysis&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;lsa-latent-semantic-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LSA Latent Semantic Analysis&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Supervised&lt;/h1&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression&lt;/h2&gt;
&lt;div id=&#34;linear&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linear&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;support-vector-machine-svm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Support Vector Machine (SVM)&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;div id=&#34;naive-bayes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Naive Bayes&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;discriminant-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Discriminant Analysis&lt;/h3&gt;
&lt;div id=&#34;lda&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;LDA&lt;/h4&gt;
&lt;p&gt;When to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;qda&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;QDA&lt;/h4&gt;
&lt;p&gt;When to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;knn&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;KNN&lt;/h3&gt;
&lt;p&gt;When to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;When to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ensemble-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ensemble Methods&lt;/h1&gt;
&lt;p&gt;boosting, bagging&lt;/p&gt;
&lt;p&gt;XGBoost, NLP (bag of words, vector-space models, sentiment analysis), Rec systems, Collab filtering, Optimization, XGBoost&lt;/p&gt;
&lt;div id=&#34;trees-and-forests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trees and Forests&lt;/h2&gt;
&lt;p&gt;Gini Impurity: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
m:&amp;amp;\text{ Region} \\
k:&amp;amp; \text{ Class} \\
G =&amp;amp; \Sigma_k \hat{p}_{m,k}(1-\hat{p}_{m,k})
\\=&amp;amp; 1 - \Sigma_k \hat{p}_{m,k}^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Decrease in Impurity &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
t:&amp;amp;\text{ Node} \\
N_t:&amp;amp;\text{ Samples at node }t \\
s_t:&amp;amp;\text{ Split }t \text{ into } t_L,t_R\\
\Delta i(s_t,t)=&amp;amp; i(t) - \frac{N_L}{N_t} i(t_L) - \frac{N_R}{N_t} i(t_R)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Mean Decrease in Impurity: &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
T:&amp;amp; \text{Trees} \\
N_T:&amp;amp; \text{Number of trees} \\
t:&amp;amp;\text{ Nodes} \\
s_t:&amp;amp;\text{ Split }t \text{ into } t_L,t_R\\
v(s_t):&amp;amp;\text{Variable split on at } s_t \\
p(t):&amp;amp; \text{Proportion of total samples at node } t \\
Imp(X_m)=&amp;amp;\frac{1}{N_T} \Sigma_{T}\Sigma_{t\in T:v(s_t)\in X_m} p(t) \Delta i(s_t,t)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e&#34;&gt;Good blog on analysis of feature importance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also see &lt;a href=&#34;https://github.com/slundberg/shap&#34;&gt;SHAP&lt;/a&gt; for interpreting predictions from tree models!&lt;/p&gt;
&lt;p&gt;Benefits of trees over OLS&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quora.com/How-does-a-random-forest-fix-regression-problems-non-normality-heteroscedasticity-multicollinearity-outliers-missing-values-and-categorical-variables&#34; class=&#34;uri&#34;&gt;https://www.quora.com/How-does-a-random-forest-fix-regression-problems-non-normality-heteroscedasticity-multicollinearity-outliers-missing-values-and-categorical-variables&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;regression-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Classification&lt;/h3&gt;
&lt;p&gt;Deep learning, Rec systems, Content and collaborative filtering, Optimization, Probabalistic programming&lt;/p&gt;
&lt;div id=&#34;assessment&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Assessment&lt;/h4&gt;
&lt;p&gt;Confusion matrix, F1, precision-recall curve&lt;/p&gt;
&lt;p&gt;ROC, AUC&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;open-ended-template&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Open ended Template&lt;/h1&gt;
&lt;div id=&#34;raw-data-structure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Raw data structure?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Table of information about consumer&lt;/li&gt;
&lt;li&gt;Table of transaction history&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;define-target-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Define target variable&lt;/h2&gt;
&lt;p&gt;e.g. Fraud would be if a consumer calls and labels a transaction as fraud.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocession&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocession,&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EDA - distributions, correlations&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature engineering&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-metrics-and-how-they-apply&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance metrics and how they apply&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Precision&lt;/li&gt;
&lt;li&gt;Recall&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;cautions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cautions:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Don’t mention models you aren’t familiar with&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;imagine-rolling-the-model-out&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imagine rolling the model out&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What issues might arise?&lt;/li&gt;
&lt;li&gt;What actions do you take in production?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Twitter, Poisson Processes, and Conjugacy</title>
      <link>/MacStrelioff/data-science/twitter-poisson-processes-and-conjugacy/</link>
      <pubDate>Sat, 29 Jun 2019 14:37:56 +0000</pubDate>
      
      <guid>/MacStrelioff/data-science/twitter-poisson-processes-and-conjugacy/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background-and-setup&#34;&gt;Background and Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#twitter-data&#34;&gt;Twitter Data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#pulling-data-from-twitters-api&#34;&gt;Pulling Data from Twitter’s API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#working-with-twitter-status-objects&#34;&gt;Working with Twitter Status objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#embed-a-status&#34;&gt;Embed a Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convert-a-status-to-a-dict&#34;&gt;Convert a Status to a dict&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#access-status-attributes-how-i-accessed-the-data-used-below&#34;&gt;Access Status Attributes (How I accessed the data used below)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#poisson-process&#34;&gt;Poisson Process&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions&#34;&gt;Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#specification-and-properties&#34;&gt;Specification and Properties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-the-homogeneity-assumption&#34;&gt;Checking the Homogeneity Assumption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#checking-the-exponential-distribution-of-intervals&#34;&gt;Checking the exponential distribution of intervals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-of-tweet-fequency-using-conjugacy&#34;&gt;Model of Tweet Fequency Using Conjugacy&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#inference-on-tweet-rate-lambda-over-time&#34;&gt;Inference On Tweet Rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; Over Time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predicting-number-of-tweets-in-interval-s&#34;&gt;Predicting Number Of Tweets In Interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;background-and-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background and Setup&lt;/h2&gt;
&lt;p&gt;In this notebook I focus on explaining Poisson processes and conjugacy applied to my Twitter activity.&lt;/p&gt;
&lt;p&gt;To get started, I followed directions from three main sources that walked through the &lt;code&gt;twitter&lt;/code&gt; and &lt;code&gt;python-twitter&lt;/code&gt; libraries, and described how to apply for Twitter API access and use the keys;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;python-twitter&lt;/code&gt;: Blogpost &lt;a href=&#34;https://medium.com/@YashSharma8388/collecting-data-from-twitter-using-python-twitter-library-and-twitter-api-42376c68d910&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tweepy&lt;/code&gt;: Blogpost &lt;a href=&#34;https://medium.com/@ssola/playing-with-twitter-streaming-api-b1f8912e50b0&#34;&gt;here&lt;/a&gt; and docs &lt;a href=&#34;http://docs.tweepy.org/en/v3.4.0/api.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Additional information on obtaining API keys and authenticating Twitter connections in a blogpost &lt;a href=&#34;https://medium.com/@fbilesanmi/how-to-login-with-twitter-api-using-python-6c9a0f7165c5&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I include my code to import the required libraries and set up API access keys, though the data used here were pulled and saved before writing the notebook. The main focus is on understanding Poisson processes and adaptive modeling of such processes using conjucacy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;reticulate&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Setup
# for working with timestamps
import pandas as pd
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
# for basic math
import numpy as np
# for plotting
import matplotlib.pyplot as plt
import seaborn as sns

# for working with distributions
from scipy.stats import expon,gamma,poisson,nbinom

# for general web data pulling
import requests

# for pulling tweets from Twitter API
import twitter

# keys for twitter API
# (removed for this public document)
api = twitter.Api(consumer_key=&amp;#39;&amp;#39;,
              consumer_secret=&amp;#39;&amp;#39;,
              access_token_key=&amp;#39;&amp;#39;,
              access_token_secret=&amp;#39;&amp;#39;)

# for saving and loading Python objects like dicts
import pickle

def save_obj(obj, name):
    with open(name + &amp;#39;.pkl&amp;#39;, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name):
    with open(name + &amp;#39;.pkl&amp;#39;, &amp;#39;rb&amp;#39;) as f:
        return pickle.load(f)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;twitter-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Twitter Data&lt;/h2&gt;
&lt;div id=&#34;pulling-data-from-twitters-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pulling Data from Twitter’s API&lt;/h3&gt;
&lt;p&gt;First I pulled data using the code below. For this code to work, your API keys will need to be specified in the setup above. To conceil my keys, I ran the commented code below earlier and saved the timeline object. The uncommented code loads my timeline and looks at the first element. The timeline is represented as a list of Status objects like the one output by the code below.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# # Twitter handel to pull data from
# handle = &amp;#39;@macstrelioff&amp;#39;
# 
# # get timeline
# timeline=api.GetUserTimeline(screen_name = handle, 
#                     count=200, # 200 is maximum 
#                     include_rts=True, 
#                     trim_user=True, 
#                     exclude_replies=False)
# 
# # save timeline object
# save_obj(timeline,&amp;#39;timeline_macstrelioff_20190406&amp;#39;)

# load timeline object
timeline=load_obj(&amp;#39;timeline_macstrelioff_20190406&amp;#39;)
timeline[0] # most recent tweet status object&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Status(ID=1113860296458756097, ScreenName=None, Created=Thu Apr 04 17:46:03 +0000 2019, Text=&amp;quot;@vboykis df.dropna(how=&amp;#39;brute force&amp;#39;) https://t.co/QOULUc5a0u&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-twitter-status-objects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Working with Twitter Status objects&lt;/h3&gt;
&lt;p&gt;I cover three ways to work with Status objects. 1. display the Status as a tweet! 2. Convert the Status to a dictionary and access values from keys 3. Access values directly as attributes of the Status&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;embed-a-status&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Embed a Status&lt;/h3&gt;
&lt;p&gt;First, many the Status attributes (&lt;code&gt;created_at&lt;/code&gt;, &lt;code&gt;favorite_count&lt;/code&gt;, &lt;code&gt;text&lt;/code&gt;, …) can be cleanly displayed as a tweet embedded in a notebook. Below I create function that takes a username and tweet ID then, using Twitter’s embedding API, displayes the tweet as it would be seen on Twitter. (Note: this will only work properly if the Python kernel is trusted)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# for displaying tweets based on username and tweet_id
class disp_tweet(object):
    def __init__(self, user_name, tweet_id):
        # see: https://dev.twitter.com/web/embedded-tweets
        api = &amp;#39;https://publish.twitter.com/oembed?url=https://twitter.com/&amp;#39;+ \
               user_name + &amp;#39;/status/&amp;#39; + tweet_id
        response  = requests.get(api)
        self.text = response.json()[&amp;quot;html&amp;quot;]

    def _repr_html_(self):
        return self.text&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;disp_tweet(user_name=&amp;#39;macstrelioff&amp;#39;,tweet_id=&amp;#39;981338927419109376&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; &amp;lt;__main__.disp_tweet object at 0x1a1ec00b00&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If run from a Jupyter notebook, this should embed a tweet as below;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
My desk is covered in random papers. It is the support of a stationery distribution.
&lt;/p&gt;
— mac strelioff (&lt;span class=&#34;citation&#34;&gt;@macstrelioff&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/macstrelioff/status/981338927419109376?ref_src=twsrc%5Etfw&#34;&gt;April 4, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;convert-a-status-to-a-dict&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Convert a Status to a dict&lt;/h3&gt;
&lt;p&gt;Status objects have a bound method, &lt;code&gt;.AsDict()&lt;/code&gt;, that will convert them to a Python dictionary. This way the structure of the information is easily seen. In the code below, I convert the first status to a dictionary and output it contents.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(timeline[0].AsDict())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; {&amp;#39;created_at&amp;#39;: &amp;#39;Thu Apr 04 17:46:03 +0000 2019&amp;#39;, &amp;#39;favorite_count&amp;#39;: 20, &amp;#39;hashtags&amp;#39;: [], &amp;#39;id&amp;#39;: 1113860296458756097, &amp;#39;id_str&amp;#39;: &amp;#39;1113860296458756097&amp;#39;, &amp;#39;in_reply_to_screen_name&amp;#39;: &amp;#39;vboykis&amp;#39;, &amp;#39;in_reply_to_status_id&amp;#39;: 1113822568211996672, &amp;#39;in_reply_to_user_id&amp;#39;: 19304217, &amp;#39;lang&amp;#39;: &amp;#39;da&amp;#39;, &amp;#39;media&amp;#39;: [{&amp;#39;display_url&amp;#39;: &amp;#39;pic.twitter.com/QOULUc5a0u&amp;#39;, &amp;#39;expanded_url&amp;#39;: &amp;#39;https://twitter.com/macstrelioff/status/1113860296458756097/photo/1&amp;#39;, &amp;#39;id&amp;#39;: 1113860285566046210, &amp;#39;media_url&amp;#39;: &amp;#39;http://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg&amp;#39;, &amp;#39;media_url_https&amp;#39;: &amp;#39;https://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg&amp;#39;, &amp;#39;sizes&amp;#39;: {&amp;#39;thumb&amp;#39;: {&amp;#39;w&amp;#39;: 150, &amp;#39;h&amp;#39;: 150, &amp;#39;resize&amp;#39;: &amp;#39;crop&amp;#39;}, &amp;#39;large&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;}, &amp;#39;medium&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;}, &amp;#39;small&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;}}, &amp;#39;type&amp;#39;: &amp;#39;animated_gif&amp;#39;, &amp;#39;url&amp;#39;: &amp;#39;https://t.co/QOULUc5a0u&amp;#39;, &amp;#39;video_info&amp;#39;: {&amp;#39;aspect_ratio&amp;#39;: [125, 99], &amp;#39;variants&amp;#39;: [{&amp;#39;bitrate&amp;#39;: 0, &amp;#39;content_type&amp;#39;: &amp;#39;video/mp4&amp;#39;, &amp;#39;url&amp;#39;: &amp;#39;https://video.twimg.com/tweet_video/D3U6BzqUUAIwYEC.mp4&amp;#39;}]}}], &amp;#39;retweet_count&amp;#39;: 2, &amp;#39;source&amp;#39;: &amp;#39;&amp;lt;a href=&amp;quot;http://twitter.com/download/android&amp;quot; rel=&amp;quot;nofollow&amp;quot;&amp;gt;Twitter for Android&amp;lt;/a&amp;gt;&amp;#39;, &amp;#39;text&amp;#39;: &amp;quot;@vboykis df.dropna(how=&amp;#39;brute force&amp;#39;) https://t.co/QOULUc5a0u&amp;quot;, &amp;#39;urls&amp;#39;: [], &amp;#39;user&amp;#39;: {&amp;#39;id&amp;#39;: 70255183, &amp;#39;id_str&amp;#39;: &amp;#39;70255183&amp;#39;}, &amp;#39;user_mentions&amp;#39;: [{&amp;#39;id&amp;#39;: 19304217, &amp;#39;id_str&amp;#39;: &amp;#39;19304217&amp;#39;, &amp;#39;name&amp;#39;: &amp;#39;Vicki Boykis&amp;#39;, &amp;#39;screen_name&amp;#39;: &amp;#39;vboykis&amp;#39;}]}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;created_at&amp;#39;: &amp;#39;Thu Apr 04 17:46:03 +0000 2019&amp;#39;,
 &amp;#39;favorite_count&amp;#39;: 20,
 &amp;#39;hashtags&amp;#39;: [],
 &amp;#39;id&amp;#39;: 1113860296458756097,
 &amp;#39;id_str&amp;#39;: &amp;#39;1113860296458756097&amp;#39;,
 &amp;#39;in_reply_to_screen_name&amp;#39;: &amp;#39;vboykis&amp;#39;,
 &amp;#39;in_reply_to_status_id&amp;#39;: 1113822568211996672,
 &amp;#39;in_reply_to_user_id&amp;#39;: 19304217,
 &amp;#39;lang&amp;#39;: &amp;#39;da&amp;#39;,
 &amp;#39;media&amp;#39;: [{&amp;#39;display_url&amp;#39;: &amp;#39;pic.twitter.com/QOULUc5a0u&amp;#39;,
   &amp;#39;expanded_url&amp;#39;: &amp;#39;https://twitter.com/macstrelioff/status/1113860296458756097/photo/1&amp;#39;,
   &amp;#39;id&amp;#39;: 1113860285566046210,
   &amp;#39;media_url&amp;#39;: &amp;#39;http://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg&amp;#39;,
   &amp;#39;media_url_https&amp;#39;: &amp;#39;https://pbs.twimg.com/tweet_video_thumb/D3U6BzqUUAIwYEC.jpg&amp;#39;,
   &amp;#39;sizes&amp;#39;: {&amp;#39;thumb&amp;#39;: {&amp;#39;w&amp;#39;: 150, &amp;#39;h&amp;#39;: 150, &amp;#39;resize&amp;#39;: &amp;#39;crop&amp;#39;},
    &amp;#39;large&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;},
    &amp;#39;medium&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;},
    &amp;#39;small&amp;#39;: {&amp;#39;w&amp;#39;: 250, &amp;#39;h&amp;#39;: 198, &amp;#39;resize&amp;#39;: &amp;#39;fit&amp;#39;}},
   &amp;#39;type&amp;#39;: &amp;#39;animated_gif&amp;#39;,
   &amp;#39;url&amp;#39;: &amp;#39;https://t.co/QOULUc5a0u&amp;#39;,
   &amp;#39;video_info&amp;#39;: {&amp;#39;aspect_ratio&amp;#39;: [125, 99],
    &amp;#39;variants&amp;#39;: [{&amp;#39;bitrate&amp;#39;: 0,
      &amp;#39;content_type&amp;#39;: &amp;#39;video/mp4&amp;#39;,
      &amp;#39;url&amp;#39;: &amp;#39;https://video.twimg.com/tweet_video/D3U6BzqUUAIwYEC.mp4&amp;#39;}]}}],
 &amp;#39;retweet_count&amp;#39;: 2,
 &amp;#39;source&amp;#39;: &amp;#39;&amp;lt;a href=&amp;quot;http://twitter.com/download/android&amp;quot; rel=&amp;quot;nofollow&amp;quot;&amp;gt;Twitter for Android&amp;lt;/a&amp;gt;&amp;#39;,
 &amp;#39;text&amp;#39;: &amp;quot;@vboykis df.dropna(how=&amp;#39;brute force&amp;#39;) https://t.co/QOULUc5a0u&amp;quot;,
 &amp;#39;urls&amp;#39;: [],
 &amp;#39;user&amp;#39;: {&amp;#39;id&amp;#39;: 70255183, &amp;#39;id_str&amp;#39;: &amp;#39;70255183&amp;#39;},
 &amp;#39;user_mentions&amp;#39;: [{&amp;#39;id&amp;#39;: 19304217,
   &amp;#39;id_str&amp;#39;: &amp;#39;19304217&amp;#39;,
   &amp;#39;name&amp;#39;: &amp;#39;Vicki Boykis&amp;#39;,
   &amp;#39;screen_name&amp;#39;: &amp;#39;vboykis&amp;#39;}]}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;access-status-attributes-how-i-accessed-the-data-used-below&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Access Status Attributes (How I accessed the data used below)&lt;/h3&gt;
&lt;p&gt;Since I’m interested in modeling expected number of tweets in a week, the most relevant attribute is the timestamps in the &lt;code&gt;created_at&lt;/code&gt; attribute. These attributes can be accessed directly from the Status object. Below I make a list of the times at which each tweet was created and check the first element of that list;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get list of time stamps
times = [pd.Timestamp(tweet.created_at) for tweet in timeline]
times.reverse() # sort s.t. times[0] is lowest, times[-1] is highest
times[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Timestamp(&amp;#39;2018-11-01 05:56:58+0000&amp;#39;, tz=&amp;#39;tzutc()&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a list of timestamps. By default, times from the twitter API are localized to the UTC timezone. Below I convert these to my local time in California;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;times = [time.tz_convert(&amp;quot;America/Los_Angeles&amp;quot;) for time in times]
times[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Timestamp(&amp;#39;2018-10-31 22:56:58-0700&amp;#39;, tz=&amp;#39;America/Los_Angeles&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a list of timestamps in local time! To get a sense of the duration over which this data spans, below I compute the time difference between the frist and last timestamp;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;times[-1]-times[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Timedelta(&amp;#39;154 days 11:49:05&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Woah, almost 155 days of my twitter activity!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson Process&lt;/h2&gt;
&lt;p&gt;A Poisson process is a common framework for modeling events that occurr in time or space. In this context, tweets are being created over time and we are interested in modeling the rate at which tweets are created in order to predict how many tweets will be created in a week.&lt;/p&gt;
&lt;div id=&#34;assumptions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assumptions&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No more than one event can occur at a single point in time.
&lt;ul&gt;
&lt;li&gt;This can be violated when a user publishes a thread of multiple tweets at once. This can be fixed by recoding threads as a single status.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Independence: The interval lengths for each event are not influenced by any other event.
&lt;ul&gt;
&lt;li&gt;This can be violated if, instead of using Twitter’s thread option, a user ends a tweet with “…” to indicate that they will soon create another tweet. In this case, there are some tweets that imply a shorter interval before the next tweet.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Homogeneity: The distribution of intervals is the same throughout the entire process.
&lt;ul&gt;
&lt;li&gt;I probe this assumption in depth below, and it almost certainly violated.&lt;/li&gt;
&lt;li&gt;There are methods for modeling inhomogeneous Poisson processes, but I ignore those here.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;specification-and-properties&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specification and Properties&lt;/h3&gt;
&lt;p&gt;In this context tweet events are occurring across time. I index tweets with &lt;span class=&#34;math inline&#34;&gt;\(i\in\{1,...,N\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the total number of tweets observed. Each tweet is created at a time, &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;, and the next tweet is observed after an interval &lt;span class=&#34;math inline&#34;&gt;\(s_{i}\)&lt;/span&gt;. That is, if tweet &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is created at time &lt;span class=&#34;math inline&#34;&gt;\(t_{i-1}\)&lt;/span&gt; then tweet &lt;span class=&#34;math inline&#34;&gt;\(i+1\)&lt;/span&gt; is created at time &lt;span class=&#34;math inline&#34;&gt;\(t_{i}=t_{i-1}+s_{i}\)&lt;/span&gt;. The interval between each tweet is &lt;span class=&#34;math inline&#34;&gt;\(s_i = t_{i}-t_{i-1} = (t_{i-1}+s_i)-t_{i-1}\)&lt;/span&gt;. The assumptions of a Poisson process permit the following distributions for three interesting features of this scenario.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The distribution of time between events, &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt;, is exponential; &lt;span class=&#34;math inline&#34;&gt;\(s_i\sim Expo(\lambda) \Rightarrow p(s_i|\lambda) = \lambda e^{-\lambda s_i}\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a parameter that describes the tweet rate.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Given an interval of length &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, the distribution of the count of events in that interval, &lt;span class=&#34;math inline&#34;&gt;\(c|s\)&lt;/span&gt;, is Poisson; &lt;span class=&#34;math inline&#34;&gt;\(c|s\sim Poisson(\lambda s) \Rightarrow p(c|s,\lambda) = \frac{(\lambda s)^{c}e^{-\lambda s}}{c!}\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;The count of events, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, in a fixed interval depends both on the rate of the events, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, and the duration of the interval, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The distribution of the total interval required for &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; events, &lt;span class=&#34;math inline&#34;&gt;\(s|c\)&lt;/span&gt;, is gamma; &lt;span class=&#34;math inline&#34;&gt;\(s|c \sim Gamma(c,\lambda) \Rightarrow p(s|c,\lambda) = \frac{\lambda^c}{\Gamma(c)}(s)^{c-1}e^{-\lambda s}\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;The interval, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, required for a fixed number of events depends on both the rate of the events, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, and the number of events, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;More information on these three kinds of distributions, and ways to implement them in Python, can be found in the &lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/stats.html&#34;&gt;scipy documentation on statistical functions&lt;/a&gt;. General information on each of these distributions can be found on the Wikipedia page for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Exponential_distribution&#34;&gt;exponential&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Poisson_distribution&#34;&gt;Poisson&lt;/a&gt;, or &lt;a href=&#34;https://en.wikipedia.org/wiki/Gamma_distribution&#34;&gt;gamma&lt;/a&gt; distribution. A key difference between the standard uses of these distributions and their roles in a Poisson process is that the rate parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is also scaled by the duration of an interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; when constructing a distribution for the count of events in interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; (2, above) or the duration of the interval required for &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; events (3, above).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-the-homogeneity-assumption&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Checking the Homogeneity Assumption&lt;/h3&gt;
&lt;p&gt;The homogeneity assumption strictly requires that tweet rates are constant across time. This would generate data that are uniform across meaningful intervals such as time in a week or time in a day. To check homogeneity, below I convert the timestamps into the hour within a week, minute within a day, and minute within an hour, and plot tweet counts across these representations of time.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# convert to hours in a week
hour_of_week     = [t.weekday()*24+t.hour+t.minute/60 for t in times]
minute_of_day    = [t.hour*60+t.minute+t.second/60 for t in times]
minute_of_hour   = [t.minute+t.second/60 for t in times]
plt.figure(figsize=(10,4));
plt.hist(hour_of_week,bins=80);&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.title(&amp;quot;My Tweet Counts By Hour Of Week&amp;quot;);
plt.ylabel(&amp;quot;Count&amp;quot;);
plt.xlabel(&amp;quot;Hour In Week&amp;quot;);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The histogram above indicates that there might be some hours of the week that have a higher rate than others. For example, I don’t seem to tweet much early on Sunday (hours 0-5), but I do seem to tweet a lot during the day on Sunday (around hours 6-20). Below I use rug plots, which represent a tweet event with a vertical line near the x-axis, and an imposed kernel density estimate, which is a continuous version of a histogram. I remake this plot in terms of hours within a week, minutes within a day, and minutes within an hour. If the tweet rate (&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) were homogeneous, then the kernel density estimate would be approximately flat.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def rug_plot_and_density(dat,bw,xlab,xlim):
    # rug plot + density
    plt.figure(figsize=(10,4))
    sns.distplot(dat, hist = False, kde = True, rug = True,
                 color = &amp;#39;darkblue&amp;#39;, 
                 kde_kws={&amp;#39;linewidth&amp;#39;: 3,&amp;quot;bw&amp;quot;:bw},
                 rug_kws={&amp;#39;color&amp;#39;: &amp;#39;black&amp;#39;})
    # formatting
    plt.title(&amp;#39;Tweet Density By &amp;#39;+xlab)
    plt.xlabel(xlab)
    plt.ylabel(&amp;#39;Kernel Density&amp;#39;)
    plt.xlim(xlim);
    plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# data and plot formatting arguments
dats = (hour_of_week,minute_of_day,minute_of_hour)
xlabs=(&amp;#39;Hour of Week&amp;#39;,&amp;#39;Minute of Day&amp;#39;,&amp;#39;Minute of Hour&amp;#39;)
xlims=([0,24*7],[0,24*60],[0,60]);
bws  = (4,40,4)

# make plots
for dat,xlab,xlim,bw in zip(dats,xlabs,xlims,bws):
    rug_plot_and_density(dat=dat,bw=bw,xlab=xlab,xlim=xlim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-13-2.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-13-3.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Are very sensitive to the choice of the bandwidth parameter that determines the window over which to aggregate events (similar to bin size when using a histogram). I’m using these plots to demonstrate possible violations of homogeneity that I would follow up on in a real analysis, but will not follow up on here.&lt;/p&gt;
&lt;p&gt;From the top plot, there seems to be two patterns. First, a series of peaks and troughs that roughly correspond to daytime and night-time hours. I probably tweet with a higher frequency when I am awake, rather than asleep – meaning that &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; may depend on time within a day. Second, a generally lower kernel density estimate during the middle than the edges. I may tweet more during the weekends (edge hours) than week days (middle hours).&lt;/p&gt;
&lt;p&gt;From the middle plot that displays tweet frequencies by minutes within a day, there again seem to be two trends. First, I rarely tweet before minute 400 (around 6:40AM). Second, I have peaks around minute 600 (10:00AM), 1000 (4:40PM), and 1350 (10:00PM). This might be related to the times that I take a break from working. I generally take a break around 5:00PM, and usually take another break before bed around 9:00-10:00PM.&lt;/p&gt;
&lt;p&gt;The bottom plot displays tweet frequencies by minutes within an hour. This seems more flat overall longer periods of time, but I may strangely tend to tweet more during the first half of hours.&lt;/p&gt;
&lt;p&gt;Overall, there are many reasons that the homogeneity assumption may be violated. For cases like this, the tweet rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; can be modeled as a function of time. However, to keep this example simple, I’ll ignore possible violations and proceede as if &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; were a constant with respect to time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-the-exponential-distribution-of-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Checking the exponential distribution of intervals&lt;/h3&gt;
&lt;p&gt;Let’s the distribution of the time between tweets, &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt;. If the assumptions of the Poisson process were satisfied, then the intervals between tweets would follow an exponential distribution. Below I compute the number of seconds between tweets and display each value as a black dash on the x-axis. I overlay a histogram, a kernel density, and a exponential density based on the observed mean interval.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# compute intervals between tweets
ss = [times[i]-times[i-1] for i in range(1,len(times))];
# convert from Timedelta to total time in seconds
ss = [si.total_seconds() for si in ss]

# histogram and density plot
plt.figure(figsize=(10,4))
sns.distplot(ss, hist = True, kde = True, rug = True,
             color = &amp;#39;darkblue&amp;#39;, bins=100,
             hist_kws={&amp;#39;color&amp;#39;:[0,.7,.5,.5],&amp;#39;label&amp;#39;:&amp;#39;Histogram&amp;#39;},
             kde_kws={&amp;#39;linewidth&amp;#39;: 3,&amp;quot;bw&amp;quot;:60*60,&amp;#39;label&amp;#39;:&amp;#39;Kernel Density&amp;#39;},
             rug_kws={&amp;#39;color&amp;#39;: &amp;#39;black&amp;#39;})
plt.xlim([0,680000]);&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.title(&amp;#39;Distribution of intervals between tweets&amp;#39;)
plt.xlabel(&amp;#39;Seconds&amp;#39;)
plt.ylabel(&amp;#39;Frequency&amp;#39;)

# overlay an exponential density
tmp_rate=np.mean(ss)
tmpx = np.linspace(0,680000,680000*5)
tmpy = expon.pdf(tmpx,scale=tmp_rate)
plt.plot(tmpx,tmpy,color=[.7,0,0,1],linewidth=3,label=&amp;#39;Exponential Density&amp;#39;);
# add legend
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on the relative heights of the kernel density and the exponential density, there seem to be more short intervals, fewer moderate length intervals, and more long intervals relative to the exponential distribution. The mean and standard deviation of an exponential distribution should be the same value. Below I check the standard deviaion and mean of the observed intervals.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# check standard deviation
np.std(ss),np.mean(ss),np.std(ss)/np.mean(ss)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; (90150.9122414953, 67076.1055276382, 1.3440093388300445)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The observed standard deviation is about 1.34 times larger (variance is about 1.80 times larger) than it would be if the data were exponentially distributed with the observed mean. While this could be accounted for with an overdispersion parameter, I will ignore this issue here for the sake of having a simple and fast online model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-of-tweet-fequency-using-conjugacy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model of Tweet Fequency Using Conjugacy&lt;/h2&gt;
&lt;p&gt;First, I’ll assume (despite the overdispersion) that the intervals between tweets follow an exponential distribution;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
s_i|\lambda &amp;amp;\sim Expo(\lambda) \\
\Rightarrow p(s_i|\lambda) &amp;amp;= \lambda e^{-s_i \lambda}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To account for uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, I’ll use a Gamma distribution with shape &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and rate &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda &amp;amp;\sim Gamma(\alpha,\beta) \\ 
\Rightarrow p(\lambda|\alpha,\beta) &amp;amp;= \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1}e^{-\lambda \beta}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The choice of a Gamma distribution allows for fast updates using conjugacy between the prior beliefs about &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; based on data observed up to time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and the exponential likelihood for the interval observed at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda | s_t &amp;amp;\sim Gamma(\alpha_t,\beta_t) \\ 
p(s_{t+1}|\lambda) &amp;amp;= \lambda e^{-s_{t+1} \lambda} \\
p(\lambda|\alpha_t,\beta_t,s_{t+1}) &amp;amp;\propto p(s_{t+1}|\lambda) p(\lambda|\alpha_t,\beta_t) \\
&amp;amp;= \lambda e^{-s_{t+1} \lambda} \frac{\beta_t^{\alpha_t}}{\Gamma(\alpha_t)} \lambda^{\alpha_t-1}e^{-\lambda \beta_t} \\
&amp;amp;= \lambda^{\alpha_t} e^{-\lambda(s_{t+1}+\beta_t)} \\ 
\Rightarrow \lambda | s_{t+1} &amp;amp;\sim Gamma(\alpha_t+1,\beta_t+s_{t+1})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This implies the following update rules for computing the parameters of the posterior over &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\alpha_{t+1} &amp;amp;\leftarrow \alpha_t +1 \\
\beta_{t+1}  &amp;amp;\leftarrow \beta_t + s_{t+1} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To answer the question of how many tweets might be observed in a period of time, I’ll assume that the count of tweets &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is Poisson distributed with rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Then the number of tweets expected in an interval of length &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; would be;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\theta &amp;amp;= s\lambda \\ 
c|\theta &amp;amp;\sim Poisson(\theta) \\
\Rightarrow p(c|\theta) &amp;amp;= \frac{\theta^c e^{-\theta}}{c!}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This means that, rather than &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, we are actually interested in the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta=s\lambda\)&lt;/span&gt;. I derive this below using a change of vairables;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\theta = s\lambda &amp;amp;\Rightarrow \lambda = \frac{\theta}{s} = \theta s^{-1} \\
p_\theta(\theta | \lambda,s) &amp;amp;= p_\lambda\left(\theta s^{-1} | s,\alpha,\beta\right) \left| \frac{d \lambda}{d \theta}\right|\\
&amp;amp;= \frac{\beta^\alpha}{\Gamma(\alpha)} \left(\frac{\theta}{s}\right)^{\alpha-1}e^{-\frac{\theta}{s}\beta} \left|s^{-1}\right| \\
&amp;amp;= \frac{\beta^\alpha}{\Gamma(\alpha)} \theta^{(\alpha-1)}s^{-({\alpha-1})-1}e^{-\frac{\theta}{s}\beta} \\
\Rightarrow p(\theta|s,\alpha,\beta)&amp;amp;= \frac{\left(\frac{\beta}{s}\right)^\alpha}{\Gamma(\alpha)} \theta^{(\alpha-1)}e^{-\theta\frac{\beta}{s}}\\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can find the distribution of &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; that accounts for uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; through the prior on &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
p(c|s,\alpha,\beta) &amp;amp;= \int_\theta p(c|\theta)p(\theta|s,\alpha,\beta)d\theta \\
&amp;amp;=\int_\theta \frac{\theta^c e^{-\theta}}{c!} \frac{\left(\frac{\beta}{s}\right)^\alpha}{\Gamma(\alpha)} \theta^{(\alpha-1)}e^{-\theta\frac{\beta}{s}} d\theta \\
&amp;amp;= \frac{\left(\frac{\beta}{s}\right)^\alpha}{c!\Gamma(\alpha)}
\int_\theta \theta^c e^{-\theta} \theta^{(\alpha-1)}e^{-\theta\frac{\beta}{s}} d\theta \\
&amp;amp;=\frac{\left(\frac{\beta}{s}\right)^\alpha}{c!\Gamma(\alpha)}
\int_\theta \theta^{c+\alpha-1} e^{-\theta\left(\frac{\beta+s}{s}\right)} d\theta \\
&amp;amp;=\frac{\left(\frac{\beta}{s}\right)^\alpha}{c!\Gamma(\alpha)}
\frac{\Gamma(c+\alpha)}{\left(\frac{\beta+s}{s}\right)^{c+\alpha}} \\
&amp;amp;=\frac{\Gamma(c+\alpha)}{c!\Gamma(\alpha)} \beta^\alpha s^{-\alpha} s^{c+\alpha}
(\beta+s)^{-(c+\alpha)} \\
&amp;amp;=\frac{\Gamma(c+\alpha)}{\Gamma(c+1)\Gamma(\alpha)}  \left(\frac{s}{\beta+s}\right)^c \left(\frac{\beta}{\beta+s}\right)^\alpha \\ 
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since a Poisson Process assumes that no more than one event can occur in an interval, intervals can be treated as discrete Bernoulli trials in which an event either occurs or does not occur. In this discrete settng, the distribution of the count of intervals with an event &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; for a given number of intervals without the event &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and a probability of the event in each interval &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; will follow a negative binomial distribution;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k &amp;amp;\sim NegBino(r,p) \\ 
\Rightarrow p(k|r,p) &amp;amp;= \frac{(k+r-1)!}{r! (k-1)!}p^{k}(1-p)^{r} \\
&amp;amp;= \frac{\Gamma(k+r)}{\Gamma(r+1) \Gamma(k)}p^{k}(1-p)^{r} \\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combining the two results above, we see that the count &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; in an interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; will follow a negative binomial distribution such that &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; fixes the number of intervals in which no event occurs, and &lt;span class=&#34;math inline&#34;&gt;\(\frac{s}{\beta+s}\)&lt;/span&gt; captures the probability of an event in any unit length interval;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
c &amp;amp;\sim NegBino\left(\alpha,\frac{s}{\beta+s}\right) \\ 
\Rightarrow p(c|s,\alpha,\beta) &amp;amp;= \frac{\Gamma(c+\alpha)}{\Gamma(c+1)\Gamma(\alpha)} \left(\frac{s}{\beta+s}\right)^c \left(\frac{\beta}{\beta+s}\right)^\alpha \\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In summary, the key components of this model are the exponential likelihood and gamma priors which allow for the fast and simple updating rules to compute the posterior over &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, and the negative binomial predictive distribution which accounts for the uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. So the applicable information from above is;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda &amp;amp;\sim Gamma(\alpha_t,\beta_t) \\
s_{t+1} &amp;amp;\sim Expo(\lambda) \\
\alpha_{t+1} &amp;amp;\leftarrow \alpha_t + 1      \\ 
\beta_{t+1}  &amp;amp;\leftarrow \beta_t + s_{t+1} \\ 
c|s,\alpha,\beta &amp;amp;\sim NegBino\left(\alpha,\frac{s}{\beta+s}\right) \\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here I investigate posterior predictive intervals for tweets over a period of time &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; while using different components of this model. In the code below, I compute all values of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; based on the update rules derived above from conjugacy.&lt;/p&gt;
&lt;!---
This formulation can be thought of as discretizing the Poisson Process into $c+\alpha$ trials defined as intervals of length $\frac{1}{\beta+s}$, where $\beta$ is the sum of the interval lengths for which no event occured, and $s$ is the sum of interval lengths for those intervals that included an event. 
---&gt;
&lt;!---
$\lambda$ has units! 

Gamma prior on the rate;
$$ \lambda \sim Gamma(\alpha,\beta)$$
$$p(\lambda)=\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta \lambda}$$

Observe an exponential interval;
$$p(s|\lambda) = \lambda e^{-\lambda s}$$

Then the posterior becomes; 

$$p(\lambda|s)\propto p(s|\lambda)p(\lambda) 
= \lambda^{\alpha-1}e^{-\beta \lambda} \lambda e^{-\lambda s} 
= \lambda^{(\alpha+1)-1}e^{-\lambda (\beta+s)}$$
$$\Rightarrow \lambda|s \sim Gamma(\alpha+1,\beta+s)$$

To incorporate more prior information, the prior $\alpha$ would reflect the number of observations, and $\beta$ can reflect the sum of intervals associated with each observation.

Given an estimate of $\lambda$, a poisson can be used to estimate the number of events in an interval (a week). 

A Poisson and a gamma make a negative binomial, which is used for predictive densities that account for uncertainty in the rate.
---&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# initialize alpha,beta as 0
alpha,beta = 0,0
alphas,betas = list(),list()
# for each observed interval in ss,
for si in ss:
    si = si/(60*60*24) # si s/1 * 1/60 m/s * 1/60 h/m * 1/24 d/h convert to days
    alpha+=1 # increment alpha by 1
    beta+=si # increment beta by the interval length
    # save parameters for analysis
    alphas.append(alpha)
    betas.append(beta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above converts intervals from seconds to days and comptutes &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; for all intervals in my dataset. This gives the parameters for posterior beliefs over &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; as each tweet is observed. Below I check the final parameters and some statistics from the last posterior.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;a,b,mode,median,mean=(alpha,beta,(alpha-1)/beta,gamma.ppf(.5,a=alpha,scale=1/beta),alpha/beta)
print(  &amp;#39;alpha : &amp;#39;+str(a)+
      &amp;#39;\nbeta  : &amp;#39;+str(b)+
      &amp;#39;\nmode  : &amp;#39;+str(mode)+
      &amp;#39;\nmedian: &amp;#39;+str(median)+
      &amp;#39;\nmean  : &amp;#39;+str(mean))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; alpha : 199
&amp;gt;&amp;gt;&amp;gt; beta  : 154.49241898148153
&amp;gt;&amp;gt;&amp;gt; mode  : 1.2816162845099446
&amp;gt;&amp;gt;&amp;gt; median: 1.2859321345366828
&amp;gt;&amp;gt;&amp;gt; mean  : 1.2880890940276715&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; correctly indicates 199 observed intervals, and the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; of 154.59 also correctly reflects the time difference in days that was computed in the first section. Lastly the ordinal relationship of the mode, median, and mean is consistent with that of a gamma distribution.&lt;/p&gt;
&lt;div id=&#34;inference-on-tweet-rate-lambda-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inference On Tweet Rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; Over Time&lt;/h3&gt;
&lt;p&gt;In the code below, I compute the maximum a posteriori (MAP) estimate, or posterior mode, across time. I plot this across time with the 97.5&lt;span class=&#34;math inline&#34;&gt;\(^{th}\)&lt;/span&gt; and 2.5&lt;span class=&#34;math inline&#34;&gt;\(^{th}\)&lt;/span&gt; percentiles as a shaded region representing the 95% credible interval.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get MAP lambda
lambda_map   = [(alpha-1)/beta for alpha,beta in zip(alphas,betas)]
# get CI upper and lower bounds
lambda_upper = [gamma.ppf(.975,a=alpha,scale=1/beta) for alpha,beta in zip(alphas,betas)]
lambda_lower = [gamma.ppf(.025,a=alpha,scale=1/beta) for alpha,beta in zip(alphas,betas)]
# plot over time
plt.figure(figsize=(10,4))
sns.distplot(times[1:], hist = False, kde = False, rug = True,
             color = &amp;#39;darkblue&amp;#39;, 
             rug_kws={&amp;#39;color&amp;#39;: &amp;#39;black&amp;#39;});
plt.plot(times[1:],lambda_map,color=[0,0,1],label=&amp;#39;MAP&amp;#39;);
plt.fill_between(times[1:], lambda_lower, lambda_upper,color=[0,0,1,.1],label=&amp;#39;95% CI&amp;#39;)
plt.ylabel(&amp;#39;Posterior on Lambda (tweets/day)&amp;#39;)
plt.xlabel(&amp;#39;Time&amp;#39;)
plt.title(&amp;#39;Summary of the posterior over $\lambda$ across time&amp;#39;)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The posterior seems to tighten dramatically around the mode during the first month, and the mode seems to stabalize after about that much time. However, this method seems slow to adjust to the slower rate of tweets in 2019. One way to address this might be to add weights to the parameter updates such that &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; are more sensitive to recent data than past data. These weights (or stepsize or learning rates) can be based on the surprisal, or likelihood, of a new tweet under the posterior. If the new interval was well anticipated, then little updating is needed, but if the new interval was very surprising or unlikely, then a sharpe change in &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; might be warrented.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-number-of-tweets-in-interval-s&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predicting Number Of Tweets In Interval &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Now I’ll visually compare predictions for the count of tweets in a week based on estimates of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; after the first 20 tweets and using all 200 tweets, using two models; 1. A Poisson using &lt;span class=&#34;math inline&#34;&gt;\(\theta=s\lambda_{MAP}\)&lt;/span&gt; 2. The negative binomial derived in the model section above&lt;/p&gt;
&lt;p&gt;First, an issue with the parameterization of the negative binomial has to be addressed. In my derivation, I ended with a negative binomial parameterized as;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k&amp;amp;:\text{Number of successes} \\
r&amp;amp;:\text{Number of failures} \\
p&amp;amp;:\text{Probability of failure} \\
p(k|r,p) &amp;amp;= \frac{(k+r-1)!}{r! (k-1)!}p^{k}(1-p)^{r} \\
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Scipy.stats.nbinom&lt;/code&gt; function defines a negative binomial with;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k&amp;amp;:\text{Number of failures} \\
n&amp;amp;:\text{Number of successes} \\
p&amp;amp;:\text{Probability of success} \\
p(k|n,p) &amp;amp;= \frac{(k+n-1)!}{(n-1)!k!} p^{n}(1-p)^{k}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recall in my derivation that the parameter corresponding to probability of failure was &lt;span class=&#34;math inline&#34;&gt;\(p=\frac{s}{\beta+s}\)&lt;/span&gt;. The differences in parameterization can be accounted for by supplying the &lt;code&gt;nbinom&lt;/code&gt; function with &lt;span class=&#34;math inline&#34;&gt;\(p=1-\frac{s}{\beta+s}=\frac{\beta}{\beta+s}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The code below plots predictive densities based on the two approaches above from estimates of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; based only on the first 20 observations.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# for the first 20 tweets
# parameters
cs = range(20)
s = 7
index = 19
theta = s*lambda_map[index]
plt.figure(figsize=(10,4))
# plot the Poisson density
plt.scatter(cs,poisson.pmf(cs,theta))
plt.plot(   cs,poisson.pmf(cs,theta),label=&amp;#39;Poisson&amp;#39;)
# plot the negative binomial density
plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])))
plt.plot(   cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=&amp;#39;Negative Binomial&amp;#39;);
# labels
plt.ylabel(&amp;#39;Mass&amp;#39;)
plt.xlabel(&amp;#39;Count&amp;#39;)
plt.title(&amp;#39;Predictions for Tweet Counts in a Week, Using 20 Observations&amp;#39;)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With a small amount of data, the uncertainty in the estimate of tweet rates carries through the negative binomial model, which gives more mass to a wider range of counts relative to the Poisson model that discards uncertainty when by &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{MAP}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Using all data
# parameters
index = 198
theta = s*lambda_map[index]
plt.figure(figsize=(10,4))
# plot the Poisson density
plt.scatter(cs,poisson.pmf(cs,theta))
plt.plot(   cs,poisson.pmf(cs,theta),label=&amp;#39;Poisson&amp;#39;)
# plot the negative binomial density
plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])))
plt.plot(   cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=&amp;#39;Negative Binomial&amp;#39;);
# labels
plt.ylabel(&amp;#39;Mass&amp;#39;)
plt.xlabel(&amp;#39;Count&amp;#39;)
plt.title(&amp;#39;Predictions for Tweet Counts in a Week, Using All Data&amp;#39;)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With a larger amount of data and predicting the tweet count over a short interval (7 days), the predictive distribution from the negative binomial and Poisson models are nearly indistinguishable. Below I compare the predictions of these model for an interval of 1 year (365 days), again using all data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Using all data
# parameters
cs=range(350,600)
s = 365
index = 198
theta = s*lambda_map[index]
plt.figure(figsize=(10,4))
# plot the Poisson density
plt.scatter(cs,poisson.pmf(cs,theta))
plt.plot(   cs,poisson.pmf(cs,theta),label=&amp;#39;Poisson&amp;#39;)
# plot the negative binomial density
plt.scatter(cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])))
plt.plot(   cs,nbinom.pmf(cs,n=alphas[index],p=betas[index]/(s+betas[index])),label=&amp;#39;Negative Binomial&amp;#39;);
# labels
plt.ylabel(&amp;#39;Mass&amp;#39;)
plt.xlabel(&amp;#39;Count&amp;#39;)
plt.title(&amp;#39;Predictions for Tweet Counts in a Year, Using All Data&amp;#39;)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/data-science/Twitter-Poisson-Processes-and-Conjugacy_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When considering a longer period of time, The uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; again carries through the negative binomial model, but is discarded in the Poisson model that uses &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{MAP}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;This was a beefy notebook!&lt;/p&gt;
&lt;p&gt;First I showed how to pull tweets as Status objects from Twitter’s API. Given Status objects, I then showed how to embed them in a notebook, view them as a dictionary, and otherwise access their attributes to construct a list of tweet timestamps. I then described Poisson Processes as they may apply to modeling user tweet rates and tweet counts over periods of time. I used kernel densities to check assumptions of a Poisson Process and found possible violations of homogeneity, and of the result that intervals between tweets should follow an exponential distribution.&lt;/p&gt;
&lt;p&gt;Punting these violations, I developed a model of tweet frequency using conjugacy between Gamma priors and Exponential likelihoods. To predict tweet counts over a period of time, I derived a negative binomial predictive distribution that accounted for uncertainty in a user’s tweet rate. I compared this distribution to a Poisson distribution that ignored uncertainty by taking only the maximum a posteriori estimate of a user’s tweet rate.&lt;/p&gt;
&lt;p&gt;Overall the model that discards posterior uncertainty attributes less mass to fringe counts, especially when there is little data or when the interval over which counts are being predicted is long. Incorporating posterior uncertainty broadens the predictive distribution to reflect uncertainty in the underlying tweet rate. That uncertainty exists regardless of the modeling approach – excluding it from predictive distributions only leads to narrow, overconfident predictions. This general principle of propagating uncertainty through a statistical process is one strong advantage of the Bayeian modeling approach that I developed and applied here.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/MacStrelioff/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>/MacStrelioff/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/post/jupyter/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./academic_0_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;install-python-and-jupyter&#34;&gt;Install Python and Jupyter&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and Jupyter notebook.&lt;/p&gt;

&lt;p&gt;Otherwise, for advanced users, install Jupyter notebook with &lt;code&gt;pip3 install jupyter&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;create-a-new-blog-post-as-usual-https-sourcethemes-com-academic-docs-managing-content-create-a-blog-post&#34;&gt;Create a new blog post &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-a-blog-post&#34; target=&#34;_blank&#34;&gt;as usual&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY_WEBSITE_FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;my-post&lt;/code&gt; with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd &amp;lt;MY_WEBSITE_FOLDER&amp;gt;
hugo new  --kind post post/my-post
cd &amp;lt;MY_WEBSITE_FOLDER&amp;gt;/content/post/my-post/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;

&lt;p&gt;Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (&lt;em&gt;New &amp;gt; Python Notebook&lt;/em&gt;) or upload a notebook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=.

# Copy the contents of Untitled.md and append it to index.md:
cat Untitled.md | tee -a index.md

# Remove the temporary file:
rm Untitled.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;

&lt;p&gt;Open &lt;code&gt;index.md&lt;/code&gt; in your text editor and edit the title etc. in the &lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34;&gt;front matter&lt;/a&gt; according to your preference.&lt;/p&gt;

&lt;p&gt;To set a &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;

&lt;p&gt;For other tips, such as using math, see the guide on &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/MacStrelioff/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/MacStrelioff/video-lectures/fun/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/video-lectures/fun/</guid>
      <description>&lt;p&gt;Check back later&amp;hellip;&lt;/p&gt;

&lt;!---
## My First Handstand

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/CgltP_bmfm8&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
---&gt;

&lt;!---
https://macstrelioff.github.io/MacStrelioff/files/CV.pdf
---&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/MacStrelioff/video-lectures/interview-prep/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/video-lectures/interview-prep/</guid>
      <description>&lt;p&gt;Check back later&amp;hellip;&lt;/p&gt;

&lt;!---
## My First Handstand

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/CgltP_bmfm8&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
---&gt;

&lt;!---
https://macstrelioff.github.io/MacStrelioff/files/CV.pdf
---&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/MacStrelioff/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/tutorial/example/</guid>
      <description>

&lt;h1 id=&#34;h1&#34;&gt;H1&lt;/h1&gt;

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;h3&#34;&gt;H3&lt;/h3&gt;

&lt;p&gt;123&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/MacStrelioff/tutorial2/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/tutorial2/example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
true
&lt;/div&gt;

&lt;div id=&#34;h1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;H1&lt;/h1&gt;
&lt;p&gt;In this tutorial, I’ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;div id=&#34;tip-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tip 1&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(rnorm(20),runif(20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStreliofftutorial2/example_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;div id=&#34;h3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;H3&lt;/h3&gt;
&lt;p&gt;123&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tip-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/MacStrelioff/files/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/files/privacy/</guid>
      <description>&lt;p&gt;Anything you post here can be seen by others.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/MacStrelioff/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>/MacStrelioff/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic: the website builder for Hugo</title>
      <link>/MacStrelioff/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/post/getting-started/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 &lt;em&gt;widgets&lt;/em&gt;, &lt;em&gt;themes&lt;/em&gt;, and &lt;em&gt;language packs&lt;/em&gt; included!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://sourcethemes.com/academic/#expo&#34; target=&#34;_blank&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#install&#34;&gt;&lt;strong&gt;Setup Academic&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;View the documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://discuss.gohugo.io/&#34; target=&#34;_blank&#34;&gt;Ask a question&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gcushen/hugo-academic/issues&#34; target=&#34;_blank&#34;&gt;Request a feature or report a bug&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Updating? View the &lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://sourcethemes.com/academic/updates/&#34; target=&#34;_blank&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support development of Academic:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://paypal.me/cushen&#34; target=&#34;_blank&#34;&gt;Donate a coffee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.patreon.com/cushen&#34; target=&#34;_blank&#34;&gt;Become a backer on Patreon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redbubble.com/people/neutreno/works/34387919-academic&#34; target=&#34;_blank&#34;&gt;Decorate your laptop or journal with an Academic sticker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://academic.threadless.com/&#34; target=&#34;_blank&#34;&gt;Wear the T-shirt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; alt=&#34;Screenshot&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://sourcethemes.com/academic/docs/jupyter/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 15+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;color-themes&#34;&gt;Color Themes&lt;/h2&gt;

&lt;p&gt;Academic comes with &lt;strong&gt;day (light) and night (dark) mode&lt;/strong&gt; built-in. Click the sun/moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34;&gt;Demo&lt;/a&gt; to see it in action!&lt;/p&gt;

&lt;p&gt;Choose a stunning color and font theme for your site. Themes are fully customizable and include:&lt;/p&gt;









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  

  
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Default&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-default.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-default.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Ocean&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-ocean.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-ocean.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Forest&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-forest.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-forest.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Dark&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-dark.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-dark.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Apogee&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-apogee.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-apogee.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;1950s&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-1950s.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-1950s.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Coffee theme with Playfair font&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-coffee-playfair.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-coffee-playfair.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
      
    
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34; data-caption=&#34;Cupcake&#34; href=&#34;/MacStrelioff/MacStrelioff/img/theme-cupcake.png&#34;&gt;
    &lt;img src=&#34;/MacStrelioff/MacStrelioff/img/theme-cupcake.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
&lt;/div&gt;

&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/sourcethemes/academic-admin&#34; target=&#34;_blank&#34;&gt;Academic Admin&lt;/a&gt;:&lt;/strong&gt; An admin tool to import publications from BibTeX or import assets for an offline site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/sourcethemes/academic-scripts&#34; target=&#34;_blank&#34;&gt;Academic Scripts&lt;/a&gt;:&lt;/strong&gt; Scripts to help migrate content to new versions of Academic&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;

&lt;p&gt;You can choose from one of the following four methods to install:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-web-browser&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;one-click install using your web browser (recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-git&#34; target=&#34;_blank&#34;&gt;install on your computer using &lt;strong&gt;Git&lt;/strong&gt; with the Command Prompt/Terminal app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-zip&#34; target=&#34;_blank&#34;&gt;install on your computer by downloading the &lt;strong&gt;ZIP files&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34;&gt;install on your computer with &lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then &lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34;&gt;personalize and deploy your new site&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;updating&#34;&gt;Updating&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34;&gt;View the Update Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Feel free to &lt;em&gt;star&lt;/em&gt; the project on &lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt; to help keep track of &lt;a href=&#34;https://sourcethemes.com/academic/updates&#34; target=&#34;_blank&#34;&gt;updates&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;

&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/MacStrelioff/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/MacStrelioff/post/2015-07-23/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/MacStrelioff/post/2015-07-23/2015-07-23-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/MacStrelioff/MacStrelioff/post/2015-07-23/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/MacStrelioff/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 -0700</pubDate>
      
      <guid>/MacStrelioff/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bandit Algos for Estimation, Hypothesis Testing, and Decision Making</title>
      <link>/MacStrelioff/unlisted/banditalgos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/banditalgos/</guid>
      <description>


&lt;div id=&#34;sources-alternatives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sources / Alternatives&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/netflix-techblog/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be&#34;&gt;Netflix Experimentation and Sequential Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Optamizely&lt;/li&gt;
&lt;li&gt;Google Analytics&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;todo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TODO:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;redo in Python, make an agent that uses each strategy as a method. Build the agent throughout the script.&lt;/li&gt;
&lt;/ol&gt;
&lt;!---
See: 
For Insight Bandit blogpost:
https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d

For Air BNB experimentation dashboard:
https://medium.com/airbnb-engineering/experiment-reporting-framework-4e3fcd29e6c0
---&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Scientists and business students are trained in decision making from an outdated perspective – classical decision making based on p-values.&lt;/p&gt;
&lt;p&gt;(make a case against p-values – inflated error rates, incoherence, difficulty integrating with expected value)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bandit-algorithms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bandit Algorithms&lt;/h1&gt;
&lt;p&gt;Much of this is from &lt;a href=&#34;https://sudeepraja.github.io/Bandits/&#34;&gt;this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here I evaluate different algorithms for bandit problems similar to those anticipated in industry or testing settings. Criteria of consideration are;&lt;/p&gt;
&lt;div id=&#34;problem-formalization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem Formalization&lt;/h2&gt;
&lt;p&gt;Bandit tasks can be cast as a Markov Decision Process.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
t &amp;amp;\in \{1,...,T\} \\
a_t &amp;amp;\in \mathcal{A} \\
s_t &amp;amp;\in \mathcal{S} \\
r_t &amp;amp;= R(s_{t+1}|a_t,s_t) = v(s_{t+1}|a_t,s_t) \\
T(s_{t+1}|a_t,s_t) &amp;amp;= p(s_{t+1}|a_t,s_t) \\
\pi(s_t)&amp;amp;=p(a_t|s_t)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is referred to as a policy or decision rule. Mathematically, it is a probability distribution over actions – a funciton that maps from states to actions.&lt;/p&gt;
&lt;p&gt;The decision maker’s goal here is to learn a policy (&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;) that is optimal for some criteria. A variety of possible criteria are discussed and evaluated below. This objective is considered when chooseing the value function &lt;span class=&#34;math inline&#34;&gt;\(v(s_{t+1}|a_t,s_t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Objective is to maximize &lt;span class=&#34;math display&#34;&gt;\[
E\left(\sum_t r_t\right)=\sum_tE(r_t)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a typical testing scenario, the policy replaces the assignment mechanism. Hence, I treat assignment mechanisms as policies here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-cases-and-evaluation-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use Cases and Evaluation Criteria&lt;/h2&gt;
&lt;p&gt;Bandit problems arise across many theoretical and applied fields. Everything from industry A/B testing, medical clinical trials, experimental lab studies, and toy problems for reinforcement learning algorithms can be cast as a bandit task. These different domains generally have different goals. A/B tests are conducted to find evidence for an advantage of one version of a product over another while emphasizing classical statistical objectives like minimizing type I error rates or false discovery rates. The goal of clinical experiments is to quickly discover the best treatment so that patient lives can be improved or saved. In simulation settings, bandit problems have been used to benchmark a variety of algorithms in terms of regret. Here I conduct similar benchmarks, while also evaluating standard ‘best proctices’ in terms of type I error rates and false discovery rates.&lt;/p&gt;

&lt;div id=&#34;type-i-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Type I Error&lt;/h3&gt;
&lt;p&gt;Among situations where there is no difference, how often is a difference detected?&lt;/p&gt;
&lt;p&gt;Type I error occurs when a null hypothesis is reongly rejected – so when a difference in outcomes of arms is detected when none actually exists.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;false-discovery-rate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;False Discovery Rate&lt;/h3&gt;
&lt;p&gt;Among detected differences, how many are real?&lt;/p&gt;
&lt;p&gt;A false discovery occurs when an arm is selected but is not the actual optimal arm.&lt;/p&gt;
&lt;p&gt;I’ll consider this on a trial-by-trial level as well as the result of the overall experiment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regret&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regret&lt;/h3&gt;
&lt;p&gt;Regret is the difference between the reward that would have been obtained had the optimal action been chosen, and the reward that was actually obtained from the chosen action.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(Regret) &amp;amp;= \sum_t E(r^*_{t}-r_t)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;uniform-policy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uniform Policy&lt;/h2&gt;
&lt;p&gt;This corresponds to a simple random sample type of assignment mechanism – the gold standard for causal inference from experimental data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\pi(s_t) &amp;amp;= \frac{1}{|\mathcal{A}|} \\
E(Regret) &amp;amp;= TE(r^*_t) - \sum_t E(r_t|\pi) \\
&amp;amp;= T(E(r^*_t) - E(r_t)) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The expected regret on each trial is the difference between the maximal reward and mean reward across actions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;greedy-policies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Greedy Policies&lt;/h2&gt;
&lt;div id=&#34;epsilon-greedy-policy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;-Greedy Policy&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;softmax-policy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Softmax Policy&lt;/h3&gt;
&lt;p&gt;Might be good for parameter estimation while also reducing regret.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;na=5      # Number of arms
as=1:na   # action IDs
beta = 5
R = runif(na) # arm probabilities
#R[1]=1; R[2]=.5; R[3]=0
T = 1500   # Number of trials
q=rep(0,na) 

pis = c()
qs = c()
ats = c()
rts = c()
regret = c()
regrett= 0;

for(ti in 1:T){
# select action
pi = exp(beta*q)/sum(exp(beta*q))
at = rmultinom(n=1,size=as,prob=pi)
at = which(at==1)
# observe outcome  
rt = rbinom(1,size=1,prob=R[at])
# save stats
pis = rbind(pis,pi)
ats = c(ats,at)
rts = c(rts,rt)
# update action values
q[at] = q[at] + .2 * (rt - q[at])
qs = rbind(qs,q)
# regret
regrett= regrett+max(R)-R[at]
regret = c(regret,regrett)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(regret,type=&amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cols = rainbow(na,s=1,v=.7)
for(ai in as){
if( ai&amp;gt;1){par(new=TRUE)}
plot(qs[,ai],type=&amp;quot;l&amp;quot;,ylim=c(0,1),col=cols[ai],
     main=&amp;quot;Estimation&amp;quot;,
     ylab=&amp;quot;Q-value&amp;quot;,
     xlab=&amp;quot;Trial&amp;quot;)
abline(h=R[ai],col=cols[ai],lty=2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(ai in as){
if( ai&amp;gt;1){par(new=TRUE)}
plot(pis[,ai],type=&amp;quot;l&amp;quot;,ylim=c(0,1),col=cols[ai],
     main=&amp;quot;Action Probabilities&amp;quot;,
     ylab=&amp;quot;Policy&amp;quot;,
     xlab=&amp;quot;Trial&amp;quot;)
#abline(h=R[ai],col=cols[ai],lty=2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ent = 0
ps=c()
for(ai in as){
p = sum(ats==ai)/length(ats)
ent = ent + p*(-log2(p))
ps=c(ps,p)
}
c(ent,ps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.41206826 0.01533333 0.68200000 0.03400000 0.14333333 0.12533333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(entropy.empirical(table(ats),unit=&amp;quot;log2&amp;quot;),freqs.empirical(table(ats)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     1          2          3          4          5 
## 1.41206826 0.01533333 0.68200000 0.03400000 0.14333333 0.12533333&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;upper-confidence-bound-ucb-policy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Upper Confidence Bound (UCB) Policy&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;thompson-sampling-policy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thompson Sampling Policy&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#na=5      # Number of arms
#as=1:na   # action IDs
#beta = 5
# keeps same as those above
# R = runif(na) # arm probabilities
#T = 5000   # Number of trials
#priors: rows: actions, col1:alpha, col2:beta
prior = matrix(1,nrow=na,ncol=2)

aposts=c()
bposts=c()
thompsonSamples = c()
ats = c()
rts = c()
regret = c()
regrett= 0;

for(ti in 1:T){
# select action
thompsonSample=c()
for(ai in as){
thompsonSample = c(thompsonSample,rbeta(1,prior[ai,1],prior[ai,2]))
}
at = which.max(thompsonSample)
# observe outcome
rt = rbinom(1,size=1,prob=R[at])
# save stats
thompsonSamples = rbind(thompsonSamples,thompsonSample)
ats = c(ats,at)
rts = c(rts,rt)
# update beta distributions
prior[at,1]=prior[at,1]+rt
prior[at,2]=prior[at,2]+(1-rt)
# save posterior parameters
aposts=rbind(aposts,prior[,1])
bposts=rbind(bposts,prior[,2])
# regret
regrett= regrett+max(R)-R[at]
regret = c(regret,regrett)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(regret,type=&amp;quot;l&amp;quot;,
     main=&amp;quot;Regret = max E(reward) -  E(reward|choice)&amp;quot;,
     ylab=&amp;quot;Cumulative Regret&amp;quot;,
     xlab=&amp;quot;Trial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# something might be wrong with rbeta(x,vec1,vec2)

cols = rainbow(na,s=1,v=.7)
for(ai in as){
if( ai&amp;gt;1){par(new=TRUE)}
plot(aposts[,ai]/(aposts[,ai]+bposts[,ai]),type=&amp;quot;l&amp;quot;,ylim=c(0,1),col=cols[ai],
     main=&amp;quot;Reward Probability Estimation&amp;quot;,
     ylab=&amp;quot;Posterior Means&amp;quot;,
     xlab=&amp;quot;Trial&amp;quot;)
abline(h=R[ai],col=cols[ai],lty=2)
}
legend(1100,.8,c(&amp;quot;Estimate&amp;quot;,&amp;quot;True&amp;quot;),lty=c(1,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(ai in as){
if( ai&amp;gt;1){par(new=TRUE)}
plot(thompsonSamples[,ai],type=&amp;quot;l&amp;quot;,ylim=c(0,1),col=cols[ai],
     main=&amp;quot;Posterior Samples&amp;quot;,
     #ylab=expression(&amp;#39;Sampled Value -- Policy=argmax&amp;#39;[&amp;#39;a&amp;#39;]*&amp;#39;(sample&amp;#39;[&amp;#39;a&amp;#39;]*&amp;#39;)&amp;#39;),
     ylab=expression(&amp;#39;Sampled Value&amp;#39;),
     xlab=&amp;quot;Trial&amp;quot;)
#abline(h=R[ai],col=cols[ai],lty=2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/MacStrelioff/MacStrelioff/Unlisted/BanditAlgos_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ent = 0
ps=c()
for(ai in as){
p = sum(ats==ai)/length(ats)
ent = ent + p*(-log2(p))
ps=c(ps,p)
}
c(ent,ps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.109426242 0.001333333 0.988666667 0.003333333 0.001333333 0.005333333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(entropy.empirical(table(ats),unit=&amp;quot;log2&amp;quot;),freqs.empirical(table(ats)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       1           2           3           4           5 
## 0.109426242 0.001333333 0.988666667 0.003333333 0.001333333 0.005333333&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;policy-comparisons&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Policy Comparisons&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Growing Smarter: Understanding User Acquisition</title>
      <link>/MacStrelioff/consultingproject/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/consultingproject/</guid>
      <description>


&lt;!---
NOTES: 
for online, add a backslish in image paths. 
For knitting an html, do not have this backslash.
---&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;As part of my fellowship at &lt;a href=&#34;https://blog.insightdatascience.com/&#34;&gt;Insight Data Science&lt;/a&gt;, I worked on a 2 week consulting project with an external company. Their product was a SaaS application enabling team collaboration on shared files, and has a 30-day free trial model. Multiple users can be associated with an account, and users can have different roles that enable different privileges within the service. The overall goal was to identify free trial accounts that would convert to subscription-based paying customers.&lt;/p&gt;
&lt;p&gt;At the request of the client, some of the information in this post has been masked so as to not reveal any confidential information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;business-need&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Business Need&lt;/h1&gt;
&lt;p&gt;The survival of any company hinges on it’s ability to acquire new users. However, the majority of free trial users for my client failed to convert to customers. This leaves much opportunity to increase their userbase.&lt;/p&gt;
&lt;div id=&#34;project-goals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Project Goals&lt;/h2&gt;
&lt;p&gt;As a consultant, I helped with two major goals focused on issuing nudges to users in order to increase user acquisition:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Identify, as early as possible, patterns in user behavior that indicate whether a free trial user is likely to become a customer after the trial.&lt;/li&gt;
&lt;li&gt;Leverage those patterns to identify outreach strategies for users that might otherwise be unlikely to continue using the service..&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-sources-and-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data sources and processing&lt;/h1&gt;
&lt;!---(exclude clearbit)---&gt;
&lt;p&gt;The original dataset consisted of account activity and product performance data for all accounts over a one year period. The dataset contained a large number of accounts, including some that were not germane to the project goals. Since the primary goal focused on behavior during the free trial, I excluded any accounts that were never on a free trial during the data collection period. In exploratory analysis, I found a large number of accounts that showed little to no activity. This prompted me to believe there are two types of trial accounts that do not convert to paying customers – 1) accounts that were created then never engaged with the product, and 2) accounts that engaged with the product, then failed to find the product valuable and decided to stop engagement. Since the strategies to address these accounts might differ, I decided to exclude any account without a minimal degree of engagement with the product and focus on the &lt;span class=&#34;math inline&#34;&gt;\(2^{nd}\)&lt;/span&gt; type of nonconversion. The criteria for a minimal level of engagement was chosen based on dependencies between product features, and decided during discussion with the client. Finally, some product features were not available during the full duration of data collection, as the product evolved over time. Since there was a large sample size, the easiest way to make the analyses pertinent for all features, and to keep accounts comparable to one another, was to exclude data collected before all product features were available.&lt;/p&gt;
&lt;div id=&#34;feature-engineering-and-exploration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Engineering and Exploration&lt;/h2&gt;
&lt;p&gt;The original account activity data was in terms of counts of each possible action (e.g. workspaces created) on each date during data collection. To make the accounts easily comparable regardless of observation dates, I created a variable for account age (days since the account was created). Also, rather than working with daily counts I computed cumulative sums, which represented an account’s total usage aggregated over all its users’ activity of a product feature up to a particular age of their account. Finally, to mitigate confounding of the counts of activity by the number of users, I divided the cumulative sums by the number of users. These were features that I focused on – total usage of each product activity per user up to a particular day since the creation of the account.&lt;/p&gt;
&lt;p&gt;The time of conversion could be many months after a trial had ended, as it may take time for a user associated with an organization to gain approval to purchase a subscription, or it may take time for a large organization to negotiate a price with my client. To focus on classifying free users as &lt;em&gt;potential&lt;/em&gt; customers based on activity, I created a variable that indicated whether an account &lt;em&gt;ever&lt;/em&gt; ended up as a customer. I then conceptualized the problem as a classification problem where, based on account activity over time since the account’s creation, I estimated the probability that the account would ever convert to a customer.&lt;/p&gt;
&lt;p&gt;I initially thought that the accounts that converted would differ in terms of the distribution of these features (cumulative activity per user), relative to those that did not convert. Hence, in exploration, I focused on probing this intuition by plotting the median and &lt;span class=&#34;math inline&#34;&gt;\(20^{th}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(80^{th}\)&lt;/span&gt; quantiles of cross sections of these features across account age, split by users who ended up converting versus those who did not. An example, focusing on the number of workspaces created per user, is shown in the figure below. Classification would be easiest and most interpretable if there were features that the converters clearly used more than those who never converted.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../csum_files_per_contributor.png&#34; alt=&#34;Median (solid line) and middle 60% (shading) of the distribution of cumulative workspaces created per user, split by those who ever paid (blue) and those that were on a free trial forever (red)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Median (solid line) and middle 60% (shading) of the distribution of cumulative workspaces created per user, split by those who ever paid (blue) and those that were on a free trial forever (red)&lt;/p&gt;
&lt;/div&gt;
&lt;!---
^ This one should be a plot of median and 20-60% quantiless
comments sometimes help the captions appear...
---&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling&lt;/h1&gt;
&lt;div id=&#34;considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Considerations&lt;/h2&gt;
&lt;p&gt;My intuition about differing distributions is natrually expressed in &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_discriminant_analysis&#34;&gt;linear discriminant analysis&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Quadratic_classifier&#34;&gt;quadratic discriminant analysis&lt;/a&gt;. However, these algorithms hinge on an assumption that the features are Gaussian distributed, which was clearly not the case – values were strictly non-negative, and distributions were skewed such that there were lots of values around 0 and some values far from the mean. Because of these violations, I also considered a &lt;a href=&#34;https://en.wikipedia.org/wiki/Support-vector_machine&#34;&gt;support vector classifier&lt;/a&gt; with radial basis functions, and tree based algorithms – &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;random forests&lt;/a&gt;, and &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;gradient boosting decision trees&lt;/a&gt; (GBT). An advantage of the tree-based approaches is their robustness to any distribution of the features, and any functional relationship between the features and the probability of an account continuing product use after the free trial.&lt;/p&gt;
&lt;p&gt;Another issue was class imbalance, since the majority of accounts did not continue. I used &lt;a href=&#34;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&#34;&gt;SMOTE&lt;/a&gt;, an algorithm that generates synthetic data from the underrepresented class (continuing customers), to address the class imbalance when fitting the model in training sets. I also considered metrics beyond accuracy, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&#34;&gt;AUC&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/F1_score&#34;&gt;F1 score&lt;/a&gt;, when selecting a final model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;initial-performance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initial Performance&lt;/h2&gt;
&lt;p&gt;To quickly hone in on a model, I assessed the algorithms mentioned above with default hyperparameter values using &lt;a href=&#34;https://scikit-learn.org/stable/index.html&#34;&gt;scikit learn&lt;/a&gt; in Python. I focused on classification accuracy in a test set, and the tree-based algorithms outperformed the others by approximately 10%. This outperformance is likely due to the non-standard distributions of the features.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hyperparameter-tuning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hyperparameter Tuning&lt;/h2&gt;
&lt;p&gt;To assess performance across hyperparameters for the tree-based algorithms, I created &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html&#34;&gt;validation curves&lt;/a&gt; with 5-fold cross validation and a set of hyperparameter values. The performance of both tree based algorithms was mostly stable in terms of &lt;a href=&#34;https://en.wikipedia.org/wiki/F1_score&#34;&gt;F1 scores&lt;/a&gt;, except for poor performance when there were very few (&lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;10\)&lt;/span&gt;) estimators. To mitigate potential overfitting, I increased the minimum number of observations in a leaf to 10, but left all other hyperparameters at their default values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting Procedure&lt;/h2&gt;
&lt;p&gt;I fit the random forests and GBT algorithms to cross-sections of the data at 7, 14, and 30 days since account creation. For each cross section, I split the data into 5 folds, and used &lt;a href=&#34;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&#34;&gt;SMOTE&lt;/a&gt; when training the algorithms within each fold to account for the class imbalance. In each fold, I logged the feature importances and performance metrics from both algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;actionable-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Actionable Results&lt;/h1&gt;
&lt;p&gt;I used the mean feature importances across folds from the GBT algorithm to identify the features that differentiated between successful accounts and accounts with users that may have needed more onboarding. Since these features seperate continuing accounts from those that did not continue after the free trial, these are the features to prioritize when considering interventions to add value to the user’s experience.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../feature_importance.png&#34; alt=&#34;Mean (bar length) and standard deviation (black error bars) of feature importance evaluated across the 5 folds for the gradient boosting trees algorithm. Feature names have been obscured at the client’s request.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Mean (bar length) and standard deviation (black error bars) of feature importance evaluated across the 5 folds for the gradient boosting trees algorithm. Feature names have been obscured at the client’s request.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To understand the form of the relationship between these features and continued engagement after the trial, I binned accounts based on product feature usage and plotted the proportion of accounts that continued using the product after the free trial across these bins. Figures like this, paired with data on an individual account activity, could help in personalizing user outreach to focus on aligning users’ feature usage with that of users in more successful accounts. For example, &lt;strong&gt;based on the figure below, if an account has fewer than 1 workspace per user, their experience might be improved by resources that make workspace creation easier to understand or engage with&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../ConvertByFeature.png&#34; alt=&#34;Proportion of accounts that convert as a function of workspaces per user. There seems to be a bump in conversion rates from around 20% to around 40% above around 1 workspace per user.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Proportion of accounts that convert as a function of workspaces per user. There seems to be a bump in conversion rates from around 20% to around 40% above around 1 workspace per user.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To identify struggling accounts for outreach, I applied the GBT algorithm to all data and focused on the confusion matrix, shown below. Different actions could be taken with respect to accounts in each quadrant. Accounts in the &lt;em&gt;bottom right quadrant&lt;/em&gt; are currently customers, and were identified as such by the algorithm. These are the accounts with users who were successful in identifying value in the product and establishing an ongoing relationship with my client. Accounts in the &lt;em&gt;top right quadrant&lt;/em&gt; are those who have not yet become customers, but are engaging with the product in ways that are similar to those who have become customers. Users in these accounts have likely identified valuable aspects of the product, and could be contacted by a customer success team to help them find a subscription plan that suits their needs.&lt;/p&gt;
&lt;p&gt;Accounts on the &lt;em&gt;top left&lt;/em&gt; and &lt;em&gt;bottom left quadrants&lt;/em&gt; were identified as accounts that would not continue with the product after the free trial – &lt;strong&gt;these are the accounts that may benefit from outreach&lt;/strong&gt; that demonstrates the value this product can add to their workflows. Accounts in the &lt;em&gt;top left quadrant&lt;/em&gt; were correctly identified as accounts that would not continue with the product. Users in these accounts may have failed to identify valuable aspects of the product, and may have had a better experience if they had been contacted early by customer support or had access to educational resources that could have helped them use the product. Accounts in the &lt;em&gt;bottom left quadrant&lt;/em&gt; are those that became paying customers, but were incorrectly identified as accounts that would not continue with the product based on their activity in the first 7 days. Misclassifying these accounts has essentially no harm, as it would only encourage efforts to improve their experience early in their trial.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../confusion_matrix.png&#34; alt=&#34;Confusion matrix based on all data, for identifying who to contact.&#34; /&gt; &lt;!--- comments help caption output ---&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;I began with an intuition that accounts that converted would have a different distribution of product feature usage per user, relative to those that did not convert, when feature usage was assessed at cross sections based on account age. To explore this, I looked at the quantiles of distributions across feature usage. Focusing on this with formal models, I found that tree-based algorithms could perform well across accuracy, precision, recall, and F1 scores, in identifying the accounts that converted, based on daily snapshots of aggregate feature usage.&lt;/p&gt;
&lt;p&gt;To derive insights from this data and analysis, I focused on confusion matrices which identified free trial accounts that acted as if they would continue product usage, and accounts that were currently unlikely to continue with the product (those in need of educational resources, and/or contact from support teams). I focused on the features found to be important by the tree based algorithms in order to identify specific features to target when reaching out to users in struggling accounts. To discover how feature usage relates to a propensity to continue using the product, I looked at the proportion of accounts that continued after trial across levels of engagement with the important product features.&lt;/p&gt;
&lt;p&gt;Overall, my work provided valuable tools for identifying accounts to connect with for long term relationships or for onboarding and educational assistance, as well as feature usage patterns indicative of success with the product.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hypothesis Testing as Classifier Evaluation</title>
      <link>/MacStrelioff/unlisted/hypothesesaremodels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/hypothesesaremodels/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Here I breed ideas from hypothesis testing, with those from the machine learning community on evaluating classifiers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis Testing&lt;/h1&gt;
&lt;p&gt;Hypothesis testing is based on a notion of accepting or rejecting a hypothesis based on data from an experiment.&lt;/p&gt;
&lt;p&gt;accept, reject&lt;/p&gt;
&lt;p&gt;Type I error –&lt;/p&gt;
&lt;p&gt;Type II error –&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; power&lt;/p&gt;
&lt;p&gt;base rates&lt;/p&gt;
&lt;p&gt;false discovery rate&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-evaluation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Classification Evaluation&lt;/h1&gt;
&lt;p&gt;Calssification evaluation is based on selecting a model based on performance in out-of-sample performance. Many performance measures, that I’ll cover throughout, as they relate to concepts from hypothesis testing.&lt;/p&gt;
&lt;p&gt;positive (true), negative (false) true positive, true negative, false positive, false negative, …&lt;/p&gt;
&lt;p&gt;false discovery rates probably related to one of – accuracy, precision, recall, sensitivity, specificity&lt;/p&gt;
&lt;p&gt;class imbalance (base rates)&lt;/p&gt;
&lt;p&gt;F1 score&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hybrid-ideas&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hybrid Ideas&lt;/h1&gt;
&lt;p&gt;Relate &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, and false discovery rate to accuracy, precision, recall.&lt;/p&gt;
&lt;p&gt;sensitivity and specificity&lt;/p&gt;
&lt;p&gt;Signal detection – hit, miss, false alarm?&lt;/p&gt;
&lt;p&gt;Resources with more detail on each metric; &lt;a href=&#34;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&#34;&gt;Most ML metrics&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Insight</title>
      <link>/MacStrelioff/unlisted/insight/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/insight/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview:&lt;/h1&gt;
&lt;p&gt;Made this doc to document my journey through the Insight Data Science program.&lt;/p&gt;
&lt;p&gt;1-3 work on project and professional skills. Week 4 work on demo. Weeks 5-8 deliver demo, work on interview preparation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;professional-skills&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Professional Skills&lt;/h1&gt;
&lt;div id=&#34;mindset-day-1-632019&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mindset (Day 1: 6/3/2019)&lt;/h2&gt;
&lt;p&gt;Imposter syndrome&lt;/p&gt;
&lt;p&gt;Fail quickly and iterate&lt;/p&gt;
&lt;p&gt;Open mindedness – new problem space, …&lt;/p&gt;
&lt;p&gt;Adaptability, identify weakenesses and adapt&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;applying&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applying&lt;/h2&gt;
&lt;p&gt;Identify strengths, and demonstrantions of value to a company&lt;/p&gt;
&lt;div id=&#34;linkedin&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LinkedIn&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/insightdatascience.com/professional-skills-hub-sv-19b/resumes/linkedin?authuser=0&#34;&gt;workshop&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resume&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Resume&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/insightdatascience.com/professional-skills-hub-sv-19b/resumes&#34;&gt;hub page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LEhTuQ4kvUU&amp;amp;feature=youtu.be&#34;&gt;workshop lecture&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Resume is a marketing tool to get an interview. Resumes should be understood in 15-45 seconds.&lt;/p&gt;
&lt;p&gt;Audience:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Hiring managers&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Wants: quickly hire, culture fit, compliments skillsets on team&lt;/li&gt;
&lt;li&gt;Dislike: lack of detail, ‘fluff’, verbose&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Recruiter&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Wants: Pass high quality&lt;/li&gt;
&lt;li&gt;Dislikes: Poor writing or grammer, “creative” resumes that take time to orient to, verbose&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Convey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Technical fit for role&lt;/li&gt;
&lt;li&gt;Potential for learning and growth&lt;/li&gt;
&lt;li&gt;Unique professional value (differentiation)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Components:&lt;/p&gt;
&lt;p&gt;Order from most to least impactful. Focus on content relevant for next role.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Header, contact information. Location that is local to the company being applied to. LinkedIn should also include anything not on resume.&lt;/li&gt;
&lt;li&gt;Skills, core competencies for recruiter to check off required skills. Sort into meaningful clusters.&lt;/li&gt;
&lt;li&gt;Experience, for hiring manager. Include evidence of the skills. Talk about publications here, and emphasize their impact or value. Start with past-tense verb. Situation, Task, Action, Result.&lt;/li&gt;
&lt;li&gt;Education, also mainly for recruiters. Could include a section on specific courses, but would be more impactful as projects in the experience section.&lt;/li&gt;
&lt;li&gt;NO SUMMARY, experience is more valuable. Can put one on LinkedIn. Avoid making too specific (pidgenhole) or general (cliche).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Include numbers to quantify impacts, numbers can be salient relative to text.&lt;/li&gt;
&lt;li&gt;Include URLs, for those who read on paper.&lt;/li&gt;
&lt;li&gt;Consistent puncuation, … .&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pitch&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pitch&lt;/h3&gt;
&lt;p&gt;A pitch is a quick and compelling story used to start conversations.&lt;/p&gt;
&lt;p&gt;10 second:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Hi I’m (). I got my PhD in () where I used () to ()”&lt;/li&gt;
&lt;li&gt;“Hi I’m Mac. I got my PhD in Cognitive Science at UC Irvine, where I used reinforcement learning algorithms as models of how people learn about and interact with a new technology.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;30 second:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Appeals to imagination and empathy to facilitate recall and decision-making&lt;/li&gt;
&lt;li&gt;Differentaites you as a leader rather than hopeful employee&lt;/li&gt;
&lt;li&gt;Creates a story from past experiences that clearly leads to the target job&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;demos&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Demos&lt;/h3&gt;
&lt;p&gt;A demo is a presentation of work to a hiring committee.&lt;/p&gt;
&lt;p&gt;Kinds of questions to anticipate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Very techinical questions&lt;/li&gt;
&lt;li&gt;Business sense&lt;/li&gt;
&lt;li&gt;Thought process – diagnostics, model selection, validation or definition of success&lt;/li&gt;
&lt;li&gt;Alternative approaches, and their strengths and weaknesses&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;opportunities-networking-and-company-visits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Opportunities: Networking and Company Visits&lt;/h2&gt;
&lt;p&gt;At Insight, data science hiring teams frequently visit. During the first week, we had visits from Square, Lab 41, and App Annie.&lt;/p&gt;
&lt;p&gt;Attempt:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask questions to assess fit: size, hierarchy, business model / monetization&lt;/li&gt;
&lt;li&gt;Ask about experiences – “Tell me about a time when someone …”. This leads to an opportunity to make them feel heard and important, or to foster a sense of connection around a similar shared experience.&lt;/li&gt;
&lt;li&gt;Demonstrating curiosity and engagement, to foster a connection and sense of trust.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Avoid:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Questions” purely intended to demonstrate your own knowledge. Instead, introduce yourself after the talk.&lt;/li&gt;
&lt;li&gt;Trying to prove you are smarter than the conterparty, which can make them view you as a threat. Instead cooperate with them to foster trust.&lt;/li&gt;
&lt;li&gt;Implying that their company is inferior to a competitor, this can make them feel defensive and mitigate the potential for a trusting connection.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;profiling-an-opportunity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Profiling an opportunity&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Project lifecycles and autonomy?&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How do data scientists get projects?&lt;/li&gt;
&lt;li&gt;What is the hierarchy?&lt;/li&gt;
&lt;li&gt;How often are check-ups with the team or managers?&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Who are the colleagues?&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Team composition – team size and roles?&lt;/li&gt;
&lt;li&gt;Background and expertise of team members?&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Work culture&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Company sponsored activities?&lt;/li&gt;
&lt;li&gt;Leanring or mentoring opportunities?&lt;/li&gt;
&lt;li&gt;Work hours / work from home policies?&lt;/li&gt;
&lt;li&gt;Vacation policies?&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Next steps&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Interview and onboarding process?&lt;/li&gt;
&lt;li&gt;Current projects?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;project&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Project&lt;/h1&gt;
&lt;div id=&#34;ideation-521-65&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ideation (5/21-6/5)&lt;/h2&gt;
&lt;div id=&#34;prediction-market&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction Market&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sentiment Analysis&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c&#34;&gt;tools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e&#34; class=&#34;uri&#34;&gt;https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://sentdex.com/sentiment-analysis/&#34; class=&#34;uri&#34;&gt;http://sentdex.com/sentiment-analysis/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stats-website&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stats Website&lt;/h3&gt;
&lt;p&gt;Shiny app that generates practice problems and contains lessons&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bandit-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bandit Analysis&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://fastml.com/ab-testing-with-bayesian-bandits-in-google-analytics/&#34;&gt;good example&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d&#34; class=&#34;uri&#34;&gt;https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.insightdatascience.com/visualizing-machine-learning-thresholds-to-make-better-business-decisions-4ab07f823415&#34;&gt;good dashboarding&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;car-price-recommender&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Car price recommender&lt;/h3&gt;
&lt;p&gt;(this has already been done on fb…)&lt;/p&gt;
&lt;p&gt;Sync data from Facebook, Amazon, and Kelly Blue Book to find distributions over car prices, to empower buyers and sellers in social markets.&lt;/p&gt;
&lt;p&gt;I wanted to sell a car and didn’t know what it was worth.&lt;/p&gt;
&lt;p&gt;Learn the KBB API, setup a shiny app that allows one to enter KBB info, and query KBB and FB and Amazon for KBB price, and amazon and fb postings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kaggle-competition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kaggle competition?&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings&#34;&gt;Air bnb new user bookings&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model a user’s behavior from landing on site to their first booking.&lt;/li&gt;
&lt;li&gt;Look for ways to improve bookings? – book in fewer searches, book in less time, …&lt;/li&gt;
&lt;li&gt;Do all NDF locations have a missing ‘date first booked’ variable?&lt;/li&gt;
&lt;li&gt;Does a missing ‘date_first_booked’ mean that the person did not book?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/ga-customer-revenue-prediction/data&#34;&gt;Google store revenue&lt;/a&gt; - few items - makeup items? - predict probability of purchasing an item&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/facebook-v-predicting-check-ins/data&#34;&gt;Facebook checkin prediction&lt;/a&gt; - fake data…&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection&lt;/h2&gt;
&lt;p&gt;Data scientists make data products.&lt;/p&gt;
&lt;p&gt;Assess value&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What is the product?&lt;/li&gt;
&lt;li&gt;Why is this useful to a specific company?&lt;/li&gt;
&lt;li&gt;Why is it useful to users?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assess feasibility&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What data is needed for this product?&lt;/li&gt;
&lt;li&gt;How much data is there, where does it come from?&lt;/li&gt;
&lt;li&gt;What technical methods are used?&lt;/li&gt;
&lt;li&gt;How could AI/ML improve the product?&lt;/li&gt;
&lt;li&gt;Are there any limits on data access or ethical constraints?&lt;/li&gt;
&lt;li&gt;How could it be monetized?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assess potential&lt;/p&gt;
&lt;ol start=&#34;10&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How might a company expand the product?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-principles-for-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Guiding principles for analysis&lt;/h2&gt;
&lt;p&gt;This wasn’t taught at insight, so I’m drawing on my training in statistics and model building here.&lt;/p&gt;
&lt;div id=&#34;thinking-about-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thinking about Variables&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Think of the variables and their structure, before seeing data; confounds, precision, neusance, …&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;thinking-about-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thinking about Models&lt;/h3&gt;
&lt;p&gt;Estimand&lt;/p&gt;
&lt;p&gt;interpretation&lt;/p&gt;
&lt;p&gt;validation (CV)&lt;/p&gt;
&lt;p&gt;efficiency – computational, memory load&lt;/p&gt;
&lt;p&gt;deployment&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;consulting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Consulting&lt;/h2&gt;
&lt;p&gt;I applied for, and was awarded, a consulting project. This section describes notes on professional consulting relationships.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;populate from consulting class…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Constant communication to clarify goals and stay on track. It can be tempting or habitual to conduct an extravagent analysis without regard for its usefulness to the cliant – avoid this waste of time by continually checking in, presenting work, and clarifying the client’s goals and desired outcomes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minimum-viable-product-mvp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Minimum Viable Product (MVP)&lt;/h2&gt;
&lt;p&gt;Working model and actionable insights&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-deliverable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final Deliverable&lt;/h2&gt;
&lt;p&gt;Blog post describing the project&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;presentation-or-demo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Presentation or Demo&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;context&lt;/li&gt;
&lt;li&gt;need&lt;/li&gt;
&lt;li&gt;vision (‘stretch goal’)&lt;/li&gt;
&lt;li&gt;outcome&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Demonstrate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Value of the solution&lt;/li&gt;
&lt;li&gt;Value of you – 1-2 years of salary, benefits, …&lt;/li&gt;
&lt;li&gt;Reasoning throughout the project.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;business-value-statement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Business Value Statement&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/insightdatascience.com/professional-skills-hub-sv-19b/communication/business-value-statement&#34;&gt;hub page&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Summarize project and its use case&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;“I’m using [data] to develop [deliverable] that [outcome] so that [client] can [solution to problem]”&lt;/p&gt;
&lt;p&gt;Mine was: “I’m using daily user activity during a free trial to estimate the probability that the user will convert to a paid user, so that my client can decide when and how to nudge free users that are unlikely to convert”&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Quantify impact;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Statistics that give the scope of the problem&lt;/li&gt;
&lt;li&gt;What is the market, or how many potential customers exist?&lt;/li&gt;
&lt;li&gt;What resources, and how much, will the solution save?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“The fate of a start-up hinges on user acquisition.”&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Broaden use case. Convey the abstract or general challenge that was solved.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Combine for final pitch&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My value statement for this project was:&lt;/p&gt;
&lt;p&gt;“The fate of a start-up depends on user acquisition. I’m using data on user behavior during a free trial to estimate the probability that a free user will convert to a paid user, and determine which product features drive this conversion. My analysis will guide decisions about when and how to nudge the free users that are unlikely to convert, so that my client can acquire more paying users”&lt;/p&gt;
&lt;p&gt;Things to consider;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Concise description of project&lt;/li&gt;
&lt;li&gt;Who are the potential users, how many exist?&lt;/li&gt;
&lt;li&gt;What problem is solved, evidence that it is a real problem&lt;/li&gt;
&lt;li&gt;What solutions already exist, how is yours better? How would this solution save time, money, emotional headache, … ?&lt;/li&gt;
&lt;li&gt;After watching the demo, what skills would the audiance know you can offer on Day 1? How will your demo make these skills explicit?&lt;/li&gt;
&lt;li&gt;Given the technical challenges of executing the project, who is going to be most excited to interview you? How does your project relate to the challenges they face? (specific companies, types of teams, industries, … )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Product: I’m conducting analyses on user demographics, and making a model to predict user conversion after a free trial.&lt;/p&gt;
&lt;p&gt;Potential users: My client. Various teams will use the demographic analyses.&lt;/p&gt;
&lt;p&gt;Problem: Want to know what features or experiences with the product are driving sales.&lt;/p&gt;
&lt;p&gt;competitors: none here.&lt;/p&gt;
&lt;p&gt;After demo: Summary stat skills for demographics, root cause analysis for finding features that drive conversion, feature engineering for the predictive model, ML model building and fitting. Interpreting data for a general audience.&lt;/p&gt;
&lt;p&gt;Who will like the presentation?: Collaborative teams, since I delivered a little for mutltiple teams. Teams that rely on data scientists for business decisions, since I worked actionably product insights.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interview-preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interview Preparation&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Metrics Through YouTube Creator</title>
      <link>/MacStrelioff/unlisted/metricsthroughyoutubecreator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/metricsthroughyoutubecreator/</guid>
      <description>


&lt;div id=&#34;what-are-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are Metrics?&lt;/h2&gt;
&lt;p&gt;Metrics are how the data scientist communicates with project managers.&lt;/p&gt;
&lt;p&gt;Two categories; success and tracking.&lt;/p&gt;
&lt;p&gt;Goals are defined in terms of metrics, and user experience is understood through metrics.&lt;/p&gt;
&lt;div id=&#34;success-metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success Metrics&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;tracking-metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tracking Metrics&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;metrics-for-goals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metrics for Goals&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;metrics-for-insights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metrics for Insights&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;communicating-metrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Communicating Metrics&lt;/h2&gt;
&lt;p&gt;Idea from:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/MfP-P8EHGBo&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Add metrics from google analytics, and youtube creator.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Online Autoclicker</title>
      <link>/MacStrelioff/unlisted/autoclicker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/autoclicker/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Madean autoclicker to play cookie clicker…&lt;/p&gt;
&lt;div id=&#34;cookie-clicker-game..&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cookie clicker game..&lt;/h2&gt;
&lt;p&gt;talk through code..&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probability and Information</title>
      <link>/MacStrelioff/unlisted/its-a-null-world-after-all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/its-a-null-world-after-all/</guid>
      <description>


&lt;p&gt;Here’s a series of questions about hypothesis testing, parils of p-hacking, and an argument against a publication requirement for graduation..&lt;/p&gt;
&lt;p&gt;What is a p-value?&lt;/p&gt;
&lt;p&gt;What is power?&lt;/p&gt;
&lt;p&gt;Assume all hypotheses in a field are false. Using z-tests or t-tests, testing at the &lt;span class=&#34;math inline&#34;&gt;\(\alpha=.05\)&lt;/span&gt; level, …&lt;/p&gt;
&lt;p&gt;How many experiments, on average, would a graduate student have to run in order to find three significant effects?&lt;/p&gt;
&lt;p&gt;What kind of distribution is this?&lt;/p&gt;
&lt;p&gt;How would you find the probability that a graduate student who runs 10 experiments finds three or more significant effects?&lt;/p&gt;
&lt;p&gt;From now on, assume students run 10 experiments during graduate school.&lt;/p&gt;
&lt;p&gt;If the person tests two hypotheses per experiment?&lt;/p&gt;
&lt;p&gt;What if there are some scientsits who test two hypotheses, and some who test one. Also assume someone needs three papers to graduate. What proportion of graduating scientists would only test one hypothses per experiment?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probability and Information</title>
      <link>/MacStrelioff/unlisted/probabilityandinformation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/probabilityandinformation/</guid>
      <description>


&lt;div id=&#34;probability-theory-and-information-theory&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Probability Theory and Information Theory&lt;/h1&gt;
&lt;div id=&#34;types-of-probabilities-and-their-computations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Types of Probabilities and Their Computations&lt;/h2&gt;
&lt;p&gt;motivation? Many important variables (future income, GDP, who I’ll marry, whether I’ll get sick, what disease I have given some symptoms, …)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;randomness-event-spaces-and-probability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Randomness, Event Spaces, and Probability&lt;/h2&gt;
&lt;p&gt;motivating example.. Imagine that a vaccine against a dangerous disease recently becomes available. However, there is a small risk that the vaccine will produce a condition just as bad as the disease itself. You do not know whether you will get the disease, or whether, given the vaccine, you will contract the side effect. Should you get the vaccine or abstain and live with a higher risk of disease?&lt;/p&gt;
Knowledge of random variables, and related concepts, can help in this and many other situations. Specifically;

&lt;p&gt;%Knowing the probabilities of each event in the event space under conditions where you take or abstain from the vaccine could determine your decision.&lt;/p&gt;
&lt;p&gt;A random variable random variable is a variable with an unknown value, but known possible values. An event is an observed value of a random variable. An event space contains all possible values of the random variable. Probabilities are values assigned to the events in an event space (i.e. the possible values of a random variable) and represent how likely each event is relative to other events in the event space.&lt;/p&gt;
Here &lt;span class=&#34;math inline&#34;&gt;\(p(e)\)&lt;/span&gt; will be used to represent the probability of an event. Some special probabilities include the probability of any event in the event space, which is 1. And the probability of any event other than &lt;span class=&#34;math inline&#34;&gt;\(p(e)\)&lt;/span&gt;, known as the compliment of &lt;span class=&#34;math inline&#34;&gt;\(p(e)\)&lt;/span&gt;, denoted with &lt;span class=&#34;math inline&#34;&gt;\(p(\neg e)\)&lt;/span&gt;, is found by;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(\neg e) = 1 - p(e)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;For example, the value observed when a six sided die is rolled could be considered a random variable with possible values of 1 through 6. Rolling the die could be an experiment, and, if a 5 lands face up, then 5 would be the outcome.&lt;/p&gt;
&lt;p&gt;Probabilities are often thought of with reference to experiments and outcomes. In this context, an experiment is any opportunity to observe a relevant outcome. Outcomes are the consequences of an experiment, and one is typically interested in outcomes of a particular kind. For example, if coin flips are thought of as an experiment, then the outcomes of this experiment could be heads or tails. In this context, probabilities assign numbers to the outcomes (e.g. heads, or tails) of an experiment (e.g. coin toss).&lt;/p&gt;
&lt;p&gt;see &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_interpretations&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Probability_interpretations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;define event define event space as all the events that could happen probability = events of interest / all possible events&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joint-events-and-the-multiplication-rule&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joint Events and the Multiplication Rule&lt;/h2&gt;
&lt;p&gt;A joint event refers to two or more events occurring together. They are colloquially talked about as one event ‘and’ another event occurring together. More formally, joint events are called intersections (represented with the &lt;span class=&#34;math inline&#34;&gt;\(\cap\)&lt;/span&gt; symbol) between events. For events &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;, the probability of their joint event will be represented with &lt;span class=&#34;math inline&#34;&gt;\(p(e_1 \cap e_2)\)&lt;/span&gt;. The probability of an intersection of events is the same regardless of which event is considered first;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} \label{joint reflexivity}
p(e_1 \cap e_2) = p(e_2 \cap e_1)
\end{equation}\]&lt;/span&gt;
For two events, &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;, their intersection is found by;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} \label{joint probability definition}
p(e_1 \&amp;amp; e_2) = p(e_1\cap e_2) = p(e_1|e_2)p(e_2)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p(e_1|e_2)\)&lt;/span&gt; is a conditional probability, discussed in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-events-and-bayes-rule&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional Events and Bayes’ Rule&lt;/h2&gt;
&lt;p&gt;Conditional events refer to one event, &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt;, after another event, &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;, is known. &lt;span class=&#34;math inline&#34;&gt;\(p(e_1|e_2)\)&lt;/span&gt; represents the probability of &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; given, or after knowing, &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;. These can be defined by rearranging the multiplication rule (equation ) as follows;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(e_1|e_2)p(e_2)=p(e_1\cap e_2) \rightarrow p(e_1|e_2)= \frac{p(e_1\cap e_2)}{p(e_2)}
\end{equation}\]&lt;/span&gt;
Noting that, by the reflexively of joint events (equation ) and the definition of the multiplication rule (equation ), &lt;span class=&#34;math inline&#34;&gt;\(p(e_1 \cap e_2) = p(e_2 \cap e_1)= p(e_2 | e_1) p(e_1)\)&lt;/span&gt;, and so the above equation becomes;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} \label{Bayes Theorem}
p(e_1|e_2)=\frac{p(e_2 | e_1) p(e_1)}{p(e_2)}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Equation  is known as Bayes Theorem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unions-of-events-and-the-addition-rule&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Unions of Events and the Addition Rule&lt;/h2&gt;
&lt;p&gt;A union (represented with the &lt;span class=&#34;math inline&#34;&gt;\(\cup\)&lt;/span&gt; symbol) of events refers to at least one of multiple events occurring. For events, &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;, their union would include the probability that &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; occurs, the probability that or &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; occurs, and the probability that both &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; occur. Colloquially this is talked about the probability of &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; ‘or’ &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt;. Formally this is expressed and computed as;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} \label{addition rule}
p(e_1\text{ or } e_2) = p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1 \cap e_2)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(p(e_1 \cap e_2)\)&lt;/span&gt; is a joint probability, defined in Equation .&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;independent-events&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Independent Events&lt;/h2&gt;
Independence is a common assumption in many statistical techniques. Statisticians assume independence primarily because it simplifies the computation of certain probabilities. Events are said to be independent if knowing one event does not change the probability of the other event. Formally, if events &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are independent, this would mean that;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} \label{independent definition}
p(e_1|e_2) = p(e_1) \text{ and } p(e_2|e_1) = p(e_2)
\end{equation}\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are independent, then their joint probability, defined in in Equation , simplifies as so;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} \label{product rule with independence}
p(e_1 \cap e_2) = p(e_1|e_2)p(e_2) = p(e_1)p(e_2)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Where the last equality is only true if &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are independent (i.e. &lt;span class=&#34;math inline&#34;&gt;\(p(e_1|e_2)=p(e_1)\)&lt;/span&gt;).&lt;/p&gt;
This then simplifies the addition rule in Equation  so that;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1|e_2)p(e_2) = p(e_1) + p(e_2) - p(e_1)p(e_2)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Where again, the last part of this equality is only true if &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mutually-exclusive-events&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mutually Exclusive Events&lt;/h2&gt;
&lt;p&gt;Events &lt;span class=&#34;math inline&#34;&gt;\(e_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_2\)&lt;/span&gt; are said to be mutually exclusive if the occurrence of either event precludes the occurrence of the other event. Formally mutual exclusivity means that;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} \label{mutually exclusive definition}
p(e_1 | e_2) = 0 \text{ and } p(e_2 | e_1) = 0
\end{equation}\]&lt;/span&gt;
If two events are mutually exclusive, then the probability of their joint event is;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(e_1 \cap e_2) = p(e_1|e_2)p(e_2) = 0*p(e_2) = 0
\end{equation}\]&lt;/span&gt;
And the probability of their union simplifies to;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1 \cap e_2) = p(e_1) + p(e_2) - 0 = p(e_1) + p(e_2)
\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;probability-functions-and-parameter-estimation-via-likelihoods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Probability Functions and Parameter Estimation via Likelihoods&lt;/h1&gt;
&lt;p&gt;The general concepts of parameterized probability functions and likelihoods are introduced here. For specific distributions, see Chapter .&lt;/p&gt;
&lt;p&gt;events can be thought of as values in spaces.. probability functions assign probabilities to data, assuming that parameters are known likelihoods assign probabilities to parameters, assuming the data are known or observed&lt;/p&gt;
&lt;div id=&#34;domain-of-a-function-as-an-event-space&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Domain of a Function as an Event Space&lt;/h2&gt;
&lt;p&gt;In Section , probabilities were introduced with respect to events. If these events are a value, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, from a large set of possible values, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, then a probability function, denoted &lt;span class=&#34;math inline&#34;&gt;\(f(x|\theta)\)&lt;/span&gt;, can be used to assign probabilities to the values &lt;span class=&#34;math inline&#34;&gt;\(x \in X\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pdfs-or-pmfs-and-cdfs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PDFs or PMFs, and CDFs&lt;/h2&gt;
&lt;p&gt;Probability functions are referred to as probability density functions (PDFs) if &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; takes continuous values, and probability mass functions (PMFs) if &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; takes discrete values. Regardless of the type of variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is, a probability function that assignes probabilities to values &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; that depend on a known parameter or parameter vector, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, is denoted &lt;span class=&#34;math inline&#34;&gt;\(f(x|\theta)\)&lt;/span&gt;. The cumulative distribution function (CDF), denoted &lt;span class=&#34;math inline&#34;&gt;\(F(x|\theta)\)&lt;/span&gt;, represents the total probability assigned to values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; less than a particular value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The CDF is related to the PDF or PMF through integration from the left side. All of this can be summarized as;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(X \leq x |\theta) = F(x|\theta) = \int_{-\infty}^x f(x|\theta) = \int_{-\infty}^x p(X=x|\theta)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Note that integrals are replaced with summation when &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; takes discrete values.&lt;/p&gt;
&lt;p&gt;A technical difficulty when considering values in a continuous domain is that the probability of any single value is 0. To sidestep this issue, one can consider values within a tiny set of values around &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, that is, &lt;span class=&#34;math inline&#34;&gt;\(\{x:x\in x\pm \Delta\}\)&lt;/span&gt;, for an arbitrarily small &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of being based on observations, these assign probabilities to a range of values, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, in the domain of the function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-and-log-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood and Log Likelihood&lt;/h2&gt;
&lt;p&gt;The likelihood, denoted &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}(\theta|x)\)&lt;/span&gt;, assigns probabilities to values of a parameter or parameter vector, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, when the events or data, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; are treated as known. When a number, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, of observations are made, and independence between the observations is assumed, the likelihood can be expressed as;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\mathcal{L}(\theta | x_1,x_2,...,x_n) = p(\theta | x_1,x_2,...,x_n) = p(\theta|x_1) p(\theta|x_2) ... p(\theta|x_n) = \prod_{i=1}^n p(\theta|x_i)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Likelihoods can be difficult to work with analytically if they are complicated functions, and numerically if they are very small numbers. Because of this, log likelihoods are often used in place of likelihoods. A log likelihood is defined as;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\ell(\theta | x_1,x_2,...,x_n) = log(\mathcal{L}(\theta | x_1,x_2,...,x_n)) = \sum_{i=1}^n p(\theta | x_i)
\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;score-function-and-fisher-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Score Function and Fisher Information&lt;/h2&gt;
&lt;p&gt;Usually statisticians are interested in the most likely parameters, that is, the parameter values that maximize the likelihood. One way to find these parameters is to find the derivative of the log likelihood and solve for the parameter values that make this equal to zero. The score function, &lt;span class=&#34;math inline&#34;&gt;\(U(\theta)\)&lt;/span&gt;, is the derivative of the log likelihood. Specifically;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
U(\theta) = \frac{d}{d\theta}\ell(\theta| x_1,x_2,...,x_n)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Statisticians are also concerned with the uncertainty in parameter estimates. One statistic used to assess this is Fisher Information, &lt;span class=&#34;math inline&#34;&gt;\(I(\theta)\)&lt;/span&gt;, defined as the negative expectation of the second derivative of the log likelihood;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
I(\theta) = -E\bigg(\frac{d^2}{d^2\theta}\ell (\theta | x_1,x_2,...,x_n)\bigg) = \int_{-\infty}^{\infty} \frac{d^2}{d^2\theta}\ell (\theta | x_1,x_2,...,x_n) f(x|\theta) d\theta
\end{equation}\]&lt;/span&gt;
&lt;p&gt;The inverse Fisher Information, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{I(\theta)}\)&lt;/span&gt;, is the variance of the sampling distribution for parameter estimates obtained by maximizing the log likelihood. As a second derivative, Fisher Information also represents the peakedness of the likelihood around the maximum likelihoods estimate. A large &lt;span class=&#34;math inline&#34;&gt;\(I(\theta)\)&lt;/span&gt; indicates a steeper peak around the maximum likelihood estimate, which is interpreted as a more informative sample.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;information-and-entropy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Information and Entropy&lt;/h1&gt;
&lt;p&gt;see &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantities_of_information&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Quantities_of_information&lt;/a&gt; see &lt;a href=&#34;https://en.wikipedia.org/wiki/Information_theory&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Information_theory&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;surprise-self-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Surprise (Self Information)&lt;/h2&gt;
&lt;p&gt;For an event, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, surprise (also called information or self-information) is defined to be the function satisfying these properties;&lt;/p&gt;

&lt;p&gt;The function satisfying these properties is:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{InformationDfn}
I(e)=-log(p(e))=log\left(\frac{1}{p(e)} \right)
\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;entropy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Entropy&lt;/h2&gt;
&lt;p&gt;Entropy is the expected information;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{EntropyDfn}
H(E)=E(I(E)) = E(-log(p(E)))  = \int_E p(e)I(e) = -\int_E p(e)log(p(e))
\end{equation}\]&lt;/span&gt;
&lt;p&gt;entropy is the expected information&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joint-entropy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joint Entropy&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-entropy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional Entropy&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;relative-entropy-divergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relative Entropy (Divergence)&lt;/h2&gt;
&lt;p&gt;KL divergence&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mutual-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;Mutual information quantifies the information obtained about one random variable through the observation of another. can be expressed as average divergance? self information is a special case of mutual information, the mutual information of a variable and itself. Given the definition of Information in Equation , Mutual Information is defined as:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{MutualInformationDfn}
I(E_1;E_2)=\int_{E_2} \int_{E_1} p(e_a,e_2)log\left(\frac{p(e_1,e_2)}{p(e_1)p(e_2)}  \right)
\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recommending: Searching, Sorting, Ranking</title>
      <link>/MacStrelioff/unlisted/searchsortrank/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/searchsortrank/</guid>
      <description>


&lt;div id=&#34;recommendation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recommendation&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Recommender_system&#34;&gt;recommendation algorithms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data science methods play a major role in discovering recommendations for users.&lt;/p&gt;
&lt;p&gt;Econ, compliments and substitutes (competitors).&lt;/p&gt;
&lt;p&gt;Types of recommendation; compliments (these go together), competitors (you might also like…), temporal (might buy this again in the future). Horizontal and vertical focus&lt;/p&gt;
&lt;p&gt;Temporal involves forecasting, which is a topic for another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sort-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sort Algorithms&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://www.geeksforgeeks.org/sorting-algorithms/&#34;&gt;sort algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;search-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Search Algorithms&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Category:Search_algorithms&#34;&gt;search algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-algorithms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking Algorithms&lt;/h2&gt;
&lt;p&gt;Lots of &lt;a href=&#34;https://en.wikipedia.org/wiki/Ranking_(information_retrieval)&#34;&gt;ranking algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Shiny Apps</title>
      <link>/MacStrelioff/data-science/shiny_apps_rps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/data-science/shiny_apps_rps/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#shiny-apps&#34;&gt;Shiny apps&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ui-user-interface&#34;&gt;UI: User Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#server&#34;&gt;Server&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#components&#34;&gt;Components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rock-paper-scissors-agent-logic&#34;&gt;Rock Paper Scissors Agent Logic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hosting&#34;&gt;Hosting&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hosting-locally&#34;&gt;Hosting locally&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hosting-online&#34;&gt;Hosting online&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Here I document what I learned, and what resources I found helpful, while I was making my first Shiny app. Shiny apps are an easy way to make web apps from RStudio, with a syntax geared towards online dashboards. As a graduate student in an experimental psychology lab, I wondered if shiny apps could be used to host online experiments. My experiments were essentially simple games – there would be buttons that participants could press, and feedback that they would see based on their actions. I decided to make a rock-paper-scissors app to gain experience with Shiny apps and probe their utility as tools for hosting experiments. You can check out the app &lt;a href=&#34;https://macstrelioff.shinyapps.io/rockpaperscissorsagent/&#34;&gt;here (https://macstrelioff.shinyapps.io/rockpaperscissorsagent/)&lt;/a&gt; though I only have 25 hours a month of free hosting, so it may be down occasionally.&lt;/p&gt;
&lt;p&gt;The source code for this app can be found on my GitHub &lt;a href=&#34;https://github.com/MacStrelioff/RockPaperScissors&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-apps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Shiny apps&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://shiny.rstudio.com/&#34;&gt;Shiny apps&lt;/a&gt; are primarily a tool for dashboarding. A dashboard is a tool that a data professional could create in order to communicate insights and actionable results to decision-makers. Ideally these apps enable decison-makers to easily interact with the data in a way that streamlines their decision making process. Many examples, with code, can be seen in the &lt;a href=&#34;http://shiny.rstudio.com/gallery/&#34;&gt;shiny app gallary&lt;/a&gt; – this was the primary resource I turned to while putting together my app.&lt;/p&gt;
&lt;p&gt;Shiny apps are comprised of a user interface (UI) component and a server component; a common layout might look like this;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(shiny)

ui &amp;lt;- fluidPage()

server &amp;lt;- function(input, output){}

shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;ui&lt;/code&gt; will contain functions that control the layout of the page and the names of interactive components like buttons and sliders. The &lt;code&gt;server&lt;/code&gt; defines how the page changes in response to events and user actions, and the final line &lt;code&gt;shinyApp(ui=ui,server=server)&lt;/code&gt; runs the app. For hosting on shinyapps.io (described below), the script that runs the app must be called &lt;code&gt;App.R&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;ui-user-interface&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UI: User Interface&lt;/h2&gt;
&lt;p&gt;My code for the user interface is shown below and broken down in this section;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ui &amp;lt;- fluidPage(
  # Application title
  titlePanel(&amp;quot;Rock Paper Scissors!&amp;quot;),
  # figure
  fluidRow(
    column(width=5,
           plotOutput(&amp;quot;distPlot&amp;quot;)
    )
  ),
  # buttons
  fluidRow(
    column(width=5,offest=2,
           actionButton(&amp;quot;rock&amp;quot;,&amp;quot;Rock&amp;quot;),
           actionButton(&amp;quot;paper&amp;quot;,&amp;quot;Paper&amp;quot;),
           actionButton(&amp;quot;scissors&amp;quot;,&amp;quot;Scissors&amp;quot;)
    )
  ),
  fluidRow(width=5,offset=5,
           textOutput(&amp;quot;result&amp;quot;),br(),
           p(&amp;quot;Source code available at: https://github.com/MacStrelioff/RockPaperScissors&amp;quot;)
           )
  
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;fluidPage()&lt;/code&gt; function is named after a type of layout that adjusts to the dimensions of a browser. Functions within &lt;code&gt;fluidPage()&lt;/code&gt; add elements to the webpage. &lt;code&gt;titlePanel(&amp;quot;TITLE&amp;quot;)&lt;/code&gt; controls the large title at the top of a page. &lt;code&gt;fluidRow()&lt;/code&gt; adds rows of elements to the page, the length of which are determined by the &lt;code&gt;column()&lt;/code&gt; function. I add the figure with;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;column(width=5,
       plotOutput(&amp;quot;distPlot&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;plotOutput(&amp;quot;OUTPUT_NAME&amp;quot;)&lt;/code&gt; takes an output from the &lt;code&gt;server&lt;/code&gt; function, described in the next section, and plots it. More information on reactive output can be found &lt;a href=&#34;http://shiny.rstudio.com/tutorial/written-tutorial/lesson4/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next I create the row of buttons with;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# buttons
fluidRow(
  column(width=5,offest=2,
         actionButton(&amp;quot;rock&amp;quot;,&amp;quot;Rock&amp;quot;),
         actionButton(&amp;quot;paper&amp;quot;,&amp;quot;Paper&amp;quot;),
         actionButton(&amp;quot;scissors&amp;quot;,&amp;quot;Scissors&amp;quot;)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;offset&lt;/code&gt; controls spacing from a previous &lt;code&gt;column&lt;/code&gt; call (not used here since everything is in one &lt;code&gt;column()&lt;/code&gt; call). Each &lt;code&gt;actionButton()&lt;/code&gt; call draws one of the buttons – the first argument is the variable name of this button, which is used as input for the server, and the second argument is the text that appears on the button. An overview of the many kinds of interactive elements you can add is avilable &lt;a href=&#34;http://shiny.rstudio.com/tutorial/written-tutorial/lesson3/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally I add some text to describe game events and point interested users to the source code, with;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fluidRow(width=5,offset=5,
         textOutput(&amp;quot;result&amp;quot;),br(),
         p(&amp;quot;Source code available at: https://github.com/MacStrelioff/RockPaperScissors&amp;quot;)
         )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;textOutput(&amp;quot;OUTPUT_NAME&amp;quot;)&lt;/code&gt; takes the text ouput variable &lt;code&gt;OUTPUT_NAME&lt;/code&gt; from the server and displays it. &lt;code&gt;br()&lt;/code&gt;, named after a line break in HTML (&lt;code&gt;&amp;lt;br&amp;gt;&lt;/code&gt;) adds a line break. And &lt;code&gt;p()&lt;/code&gt; adds static text, again named after a paragraph tag (&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt;) in HTML.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;server&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Server&lt;/h2&gt;
&lt;p&gt;The server defines how page elements interact with user input and server events.&lt;/p&gt;
&lt;div id=&#34;components&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Components&lt;/h3&gt;
&lt;p&gt;My code for the server is shown below and broken down in this section with an emphasis on the functions used. In the code below, I replaced opponent logic with &lt;code&gt;...&lt;/code&gt; to keep the emphasis here on understanding server functions. This logic is revealed and detailed in the next section.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server &amp;lt;- function(input, output) {
  
  # initialize variables (runs once when app visited)
  values &amp;lt;- reactiveValues()
  values$round =0; # track round
  values$opp_actions = c() # track opponent actions
  values$score =0; # track score
  values$scores=0; # track score history for feedback
  values$grams = data.frame(&amp;#39;rrrrr&amp;#39;=rep(0,3)) # initialize to store gram counts
  values$a = &amp;quot;init&amp;quot;;
  values$as = c(&amp;quot;r&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;) # possible actions

  observeEvent(input$rock | input$paper | input$scissors,{
      # if any action taken (done to block the first run when these are all NULL-&amp;gt;0)
      if(input$rock | input$paper | input$scissors){
        # increment round
        values$round  = values$round+1;
        # policy -- code to greedily pick best action
        ## if fewer than 5 actions taken, draw uniformly
        if(length(values$opp_actions)&amp;lt;5){
          values$a=sample(values$as,1)
          } else{ # if at least 5 actions taken
          nobs = length(values$opp_actions)
          ngram = paste(values$opp_actions[(nobs-4):nobs],collapse = &amp;quot;&amp;quot;)
          #cat(&amp;quot;\n&amp;quot;,ngram)
          # if this pattern not observed before, initialize it and choose randomly
          if(!any(names(values$grams)==ngram)){
            values$grams[ngram]=rep(0,3)
            values$a=sample(values$as,1)
          } else { # if at least 5 actions taken, and this pattern has been seen before, 
            pred = values$as[which.max(values$grams[ngram][[1]])]
            values$a=switch(pred,&amp;quot;r&amp;quot;=&amp;quot;p&amp;quot;,&amp;quot;p&amp;quot;=&amp;quot;s&amp;quot;,&amp;quot;s&amp;quot;=&amp;quot;r&amp;quot;)
          }
          #cat(&amp;quot;\n&amp;quot;,names(values$grams))
          #cat(&amp;quot;\n&amp;quot;,values$grams[ngram][[1]])
        }
        
        # get opponent action and outcome
        if(input$rock    -sum(values$opp_actions==&amp;quot;r&amp;quot;)==1){
          opp_action=&amp;quot;r&amp;quot;
          dscore = switch(values$a,&amp;quot;r&amp;quot;=0,&amp;quot;p&amp;quot;=-1,&amp;quot;s&amp;quot;=1)
          }
        if(input$paper   -sum(values$opp_actions==&amp;quot;p&amp;quot;)==1){
          opp_action=&amp;quot;p&amp;quot;
          dscore = switch(values$a,&amp;quot;r&amp;quot;=1,&amp;quot;p&amp;quot;=0,&amp;quot;s&amp;quot;=-1)
        }
        if(input$scissors-sum(values$opp_actions==&amp;quot;s&amp;quot;)==1){
          opp_action=&amp;quot;s&amp;quot;
          dscore = switch(values$a,&amp;quot;r&amp;quot;=-1,&amp;quot;p&amp;quot;=1,&amp;quot;s&amp;quot;=0)
        }
        
        # evaluate outcome
        values$score  = values$score+dscore
        values$scores = c(values$scores,values$score);
        
        # update opponent model 
        values$opp_actions = c(values$opp_actions,opp_action);
        
        if(length(values$opp_actions)&amp;gt;5){
          if(any(names(values$grams)==ngram)){
            values$grams[ngram][[1]]=values$grams[ngram][[1]]+(values$as==opp_action)
          }
        }
        
      }
    
    # use strings to code, then just take last 5 strings and use as the key for the dictionary of 5-grams...
    output$distPlot &amp;lt;- renderPlot({
      try({
      x = seq(0,values$round);
      y = values$scores;
      cat(&amp;quot;\n round:&amp;quot;,values$round, &amp;quot;, score:&amp;quot;,values$score,&amp;quot;, len(x): &amp;quot;,length(x),&amp;quot; len(y):&amp;quot;,length(y),&amp;quot;, opp_act:&amp;quot;,values$opp_actions,
          &amp;quot;\n a: &amp;quot;,values$a,
          sep=&amp;quot;&amp;quot;)
      # draw the histogram with the specified number of bins
      plot(x,y,type=&amp;quot;l&amp;quot;,xlab = &amp;quot;Rounds&amp;quot;,ylab=&amp;quot;Score&amp;quot;,main=&amp;quot;Cumulative Score&amp;quot;)
      })
    })
    })
  
  output$result = renderText({
    paste(&amp;quot;Opponent chose: &amp;quot;,switch(values$a,&amp;quot;r&amp;quot;=&amp;quot;Rock&amp;quot;,&amp;quot;p&amp;quot;=&amp;quot;Paper&amp;quot;,&amp;quot;s&amp;quot;=&amp;quot;Scissors&amp;quot;,&amp;quot;init&amp;quot;=&amp;quot;Nothing yet, ...&amp;quot;))
  })
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first chunk here, shown below, uses the &lt;code&gt;reactiveValues()&lt;/code&gt; function to create a named list of variables that can be updated throughout the app session.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# initialize variables (runs once when app visited)
values &amp;lt;- reactiveValues()
values$round =0; # track round
values$opp_actions = c() # track opponent actions
values$score =0; # track score
values$scores=0; # track score history for feedback
values$grams = data.frame(&amp;#39;rrrrr&amp;#39;=rep(0,3)) # initialize to store gram counts
values$a = &amp;quot;init&amp;quot;;
values$as = c(&amp;quot;r&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;) # possible actions&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next section uses &lt;code&gt;observeEvent(LOGICAL)&lt;/code&gt; to check if an event has occurred. Here the events are a &lt;code&gt;TRUE&lt;/code&gt; value from any of the buttons, which would represent that the button had been pressed. The variables &lt;code&gt;input$NAME&lt;/code&gt; represent the button whose label is &lt;code&gt;NAME&lt;/code&gt;. The second argument is a script to run when an event is detected. Note that the values for variables that reflect button presses, here &lt;code&gt;input$rock&lt;/code&gt;, &lt;code&gt;input$paper&lt;/code&gt;, &lt;code&gt;input$scissors&lt;/code&gt;, are initialized as &lt;code&gt;NULL&lt;/code&gt;, so the server will first run once with these values as &lt;code&gt;NULL&lt;/code&gt;. To avoid the game from starting on this run, I included a conditional that required one of their values to be &lt;code&gt;TRUE&lt;/code&gt;, which would indicate that the player clicked one of the buttons. The rest of this block is replaced with &lt;code&gt;...&lt;/code&gt; because it is game logic that is described below, however, it heavily relies on access to the reactive values stored in the &lt;code&gt;values&lt;/code&gt; structure.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  observeEvent(input$rock | input$paper | input$scissors,{
      # if any action taken (done to block the first run when these are all NULL-&amp;gt;0)
      if(input$rock | input$paper | input$scissors){
          ...
        }
      }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last two chunks use the &lt;code&gt;renderPlot()&lt;/code&gt; function to produce the &lt;code&gt;distPlot&lt;/code&gt; variable, stored in the &lt;code&gt;output&lt;/code&gt; structure, which is referenced in the &lt;code&gt;ui&lt;/code&gt; when drawing the figure. This code occasionally crashed when button presses happened rapidly, so I wrapped it in a &lt;code&gt;try()&lt;/code&gt; block. The &lt;code&gt;cat()&lt;/code&gt; function was used to print values to the console while debugging. Finally, I used the &lt;code&gt;renderText()&lt;/code&gt; function to assign textual feedback on the agent’s actions to the output variable &lt;code&gt;result&lt;/code&gt; which is referenced in the &lt;code&gt;ui&lt;/code&gt; when displaying the text that is rendered here.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output$distPlot &amp;lt;- renderPlot({
  try({
  x = seq(0,values$round);
  y = values$scores;
  cat(&amp;quot;\n round:&amp;quot;,values$round, &amp;quot;, score:&amp;quot;,values$score,&amp;quot;, len(x): &amp;quot;,length(x),&amp;quot; len(y):&amp;quot;,length(y),&amp;quot;, opp_act:&amp;quot;,values$opp_actions,
      &amp;quot;\n a: &amp;quot;,values$a,
      sep=&amp;quot;&amp;quot;)
  # draw the histogram with the specified number of bins
  plot(x,y,type=&amp;quot;l&amp;quot;,xlab = &amp;quot;Rounds&amp;quot;,ylab=&amp;quot;Score&amp;quot;,main=&amp;quot;Cumulative Score&amp;quot;)
  })
})
})
  
output$result = renderText({
  paste(&amp;quot;Opponent chose: &amp;quot;,switch(values$a,&amp;quot;r&amp;quot;=&amp;quot;Rock&amp;quot;,&amp;quot;p&amp;quot;=&amp;quot;Paper&amp;quot;,&amp;quot;s&amp;quot;=&amp;quot;Scissors&amp;quot;,&amp;quot;init&amp;quot;=&amp;quot;Nothing yet, ...&amp;quot;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rock-paper-scissors-agent-logic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rock Paper Scissors Agent Logic&lt;/h3&gt;
&lt;p&gt;Here I return to the logic inside the &lt;code&gt;observeEvent()&lt;/code&gt; call that implements the game. This code is run whenever a user chooses an action. First I increment the round, which is stored in the &lt;code&gt;reactiveValue&lt;/code&gt; structure, &lt;code&gt;values&lt;/code&gt;;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# increment round
values$round  = values$round+1;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then implemented the agent’s policy. In a reinforcement learning context, a policy is an agent’s probability distribution over actions. The actions here are stored in the reactive value &lt;code&gt;as&lt;/code&gt; which consists of “r”,“p”, and “s”, respectively representing the actions Rock, Paper, or Scissors. The policy I implemented here depends on the last 5 actions that a user takes. The entire history of a user’s actions, excluding the current action, is stored in the reactive value &lt;code&gt;opp_actions&lt;/code&gt; which is a string consisting of the charasters “r”, “p”, or “s”. For example, if &lt;code&gt;opp_actions&lt;/code&gt; is “rrps”, it would mean that the user chose rock twice, then paper, then scissors, before picking the current action which is not yet part of the action history. The agent’s action is stored in the reactive value &lt;code&gt;a&lt;/code&gt;. On the first 5 rounds, the agent picks uniformly from the actions Rock, Paper, Scissors;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if(length(values$opp_actions)&amp;lt;5){
  values$a=sample(values$as,1)
  } &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After 5 rounds, the agent uses 5-grams (sequences of the last 5 user actions), to pick an action that will beat the most likely player action based on the previous times the player has emitted this sequence of 5 actions. To implement this, I first get the number of observed actions, &lt;code&gt;nobs&lt;/code&gt;, and then get a string that represents the last 5 actions the user has taken, &lt;code&gt;ngram&lt;/code&gt;. If theis sequence has not been observed, the agent creates an instance of the sequence in its memory of the users’s 5-grams (&lt;code&gt;grams&lt;/code&gt;), initializes counts of the uses subsequent Rock, Paper, Scissors actions to 0, and finally uniformly samples an action. Otherwise, if this sequence has been observed before, the agent predicts what the user will choose on this round (&lt;code&gt;pred&lt;/code&gt;) by finding the action that the player most frequently chose in the past after an identical sequence of 5 actions, and then picks the action &lt;code&gt;a&lt;/code&gt; that would beat what it expects the player to chose. The code that implements this process is shown below;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  else{ # if at least 5 actions taken
  nobs = length(values$opp_actions)
  ngram = paste(values$opp_actions[(nobs-4):nobs],collapse = &amp;quot;&amp;quot;)
  # if this pattern not observed before, initialize it and choose randomly
  if(!any(names(values$grams)==ngram)){
    values$grams[ngram]=rep(0,3)
    values$a=sample(values$as,1)
  } else { # if at least 5 actions taken, and this pattern has been seen before, 
    pred = values$as[which.max(values$grams[ngram][[1]])]
    values$a=switch(pred,&amp;quot;r&amp;quot;=&amp;quot;p&amp;quot;,&amp;quot;p&amp;quot;=&amp;quot;s&amp;quot;,&amp;quot;s&amp;quot;=&amp;quot;r&amp;quot;)
  }
  #cat(&amp;quot;\n&amp;quot;,names(values$grams))
  #cat(&amp;quot;\n&amp;quot;,values$grams[ngram][[1]])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The game environment then processes the user’s action and the agent’s action, and determines an outcome of +1 if the user won, 0 if the agent and user tied, and -1 if the agent won;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# get opponent action and outcome
if(input$rock    -sum(values$opp_actions==&amp;quot;r&amp;quot;)==1){
  opp_action=&amp;quot;r&amp;quot;
  dscore = switch(values$a,&amp;quot;r&amp;quot;=0,&amp;quot;p&amp;quot;=-1,&amp;quot;s&amp;quot;=1)
  }
if(input$paper   -sum(values$opp_actions==&amp;quot;p&amp;quot;)==1){
  opp_action=&amp;quot;p&amp;quot;
  dscore = switch(values$a,&amp;quot;r&amp;quot;=1,&amp;quot;p&amp;quot;=0,&amp;quot;s&amp;quot;=-1)
}
if(input$scissors-sum(values$opp_actions==&amp;quot;s&amp;quot;)==1){
  opp_action=&amp;quot;s&amp;quot;
  dscore = switch(values$a,&amp;quot;r&amp;quot;=-1,&amp;quot;p&amp;quot;=1,&amp;quot;s&amp;quot;=0)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The outcome value is added to the cumulative score, &lt;code&gt;score&lt;/code&gt;, and the score is appended to the &lt;code&gt;scores&lt;/code&gt; variables which stores the score on every round for plotting.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# evaluate outcome
values$score  = values$score+dscore
values$scores = c(values$scores,values$score);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the agent updates its model of the user by appending the current action to the user’s action history, &lt;code&gt;opp_actions&lt;/code&gt;, and updating it’s memory of user behavior stored in &lt;code&gt;grams&lt;/code&gt; by incrementing the count of the user’s action associated with the 5-gram for this round, &lt;code&gt;ngram&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# update opponent model 
values$opp_actions = c(values$opp_actions,opp_action);

if(length(values$opp_actions)&amp;gt;5){
  if(any(names(values$grams)==ngram)){
    values$grams[ngram][[1]]=values$grams[ngram][[1]]+(values$as==opp_action)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hosting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hosting&lt;/h2&gt;
&lt;div id=&#34;hosting-locally&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hosting locally&lt;/h3&gt;
&lt;p&gt;Running the function &lt;code&gt;shinyApp(ui = ui, server = server)&lt;/code&gt; from RStudio will run the application on a local host. This is great for debugging, but not great for making the app avilable to users on other computers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hosting-online&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hosting online&lt;/h3&gt;
&lt;p&gt;RStudio supports many ways of &lt;a href=&#34;https://shiny.rstudio.com/deploy/&#34;&gt;hosting a shiny app&lt;/a&gt;, but the one that worked best for me was to host through the free plan at &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinnyapps.io&lt;/a&gt;. A detailed walkthrough on deploying shiny apps can be found &lt;a href=&#34;https://docs.rstudio.com/shinyapps.io/getting-started.html#deploying-applications&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is also possible to host a Shiny app through Amazon Web Services. More resources on that can be found &lt;a href=&#34;https://medium.com/@CharlesBordet/how-to-deploy-a-shiny-app-on-aws-part-1-4893d0a7432f&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.charlesbordet.com/en/shiny-aws-1/&#34;&gt;here&lt;/a&gt;, or &lt;a href=&#34;https://stackoverflow.com/questions/47725234/understanding-the-scalability-of-rshiny-apps-hosted-on-shinyserver&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why I Like Bayesian Statistics</title>
      <link>/MacStrelioff/unlisted/why_bayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/MacStrelioff/unlisted/why_bayes/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Madean autoclicker to play cookie clicker…&lt;/p&gt;
&lt;div id=&#34;coherent&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coherent&lt;/h2&gt;
&lt;p&gt;Examples where frequentist stats is incoherent?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flexible&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Flexible&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;easy to model complex processes&lt;/li&gt;
&lt;li&gt;easy to test non-standard hypotheses, e.g. “does every”, or “two groups higher, one group lower”&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;integration-with-decision-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integration with decision theory&lt;/h2&gt;
&lt;p&gt;Thompson sampling example&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
