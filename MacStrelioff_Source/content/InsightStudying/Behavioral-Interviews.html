---
title: "Behavioral Interviewing"
author: "Mac Strelioff"
date: "2019-07-12 09:00:20"
math: true
menu:
  InsightStudying:
    parent: Foundations
    weight: 99
linktitle: Behavioral Interviewing
---



<p>Tell me about yourself, your academic research, why data science?; 30 second elevator pitch vs. 2 minute phone screen/on-site</p>
<p>Why this company? what value can you add? Tell me about a time when…</p>
<p>STAR chart (Situation, Task, Action, Result)</p>
<p>Mock interviews: Practice, practice, practice</p>
<div id="common-behavioral-questions" class="section level1">
<h1>Common Behavioral Questions</h1>
<div id="tell-me-a-little-about-your-background." class="section level2">
<h2>“Tell me a little about your background.”</h2>
<p>PhD projects can include:</p>
<div id="child-bandits" class="section level3">
<h3>Child bandits:</h3>
<p>Bandit tasks are a common paradigm for studying individual level decision making and making claims about the way people make decisions. But in lots of contexts, the results you get when you addregate over trials can be different from the actual result on any trial. So I developed a model to infer on a trial by trial level, what decision making strategy kids were using to choose between two ‘bandits’ that gave out stickers. – I have an interest in using more customized models and Bayesian inference to overcome issues that arise from aggregating data at too high a level. If you had averaged over trials, then there may have been evidence for one or another model, but the dynamic changes in decision making strategies would have gone unnoticed.</p>
<ul>
<li>Similar approaches could be useful for dynamic experimental designs, OR in the context of Netflix, for personalization. If we could dynamically infer something like the strategy or emotional state of a user, then we could use that inference to personalize our recommended content or things like cover art or other aesthetic aspects of Netflix.</li>
</ul>
</div>
<div id="predictit" class="section level3">
<h3>PredictIt</h3>
<p>prediction markets are used kind of like filtering, where we aggregate over individual opinions to get a more accurate concensus opinion. But, individual opinions have systematic biases, so would the aggregation of these also have biases? To answer this, I partnered with a large prediction market provider and found evidence of mispricings consistent with individual level biases – miscalibration and also subadativity (superaditivity)?…</p>
</div>
<div id="rl" class="section level3">
<h3>RL</h3>
<ul>
<li>RL.</li>
</ul>
</div>
</div>
<div id="tell-me-about-the-project-you-worked-on-at-insight." class="section level2">
<h2>“Tell me about the project you worked on at Insight.”</h2>
<p>Generally, the answer can show clear communication of a problem, an approach, challenges, solutions, and relevance to the company or role.</p>
</div>
<div id="why-are-you-interested-in-a-career-as-a-data-scientist-or-data-engineer" class="section level2">
<h2>“Why are you interested in a career as a data scientist or data engineer?”</h2>
</div>
<div id="tell-me-about-your-researchtalk-to-me-about-your-last-role." class="section level2">
<h2>“Tell me about your research/talk to me about your last role.”</h2>
</div>
<div id="why-are-you-interested-in-joining-this-company" class="section level2">
<h2>“Why are you interested in joining this company?”</h2>
</div>
</div>
