---
title: "Machine Learning"
author: "Mac Strelioff"
date: "`r Sys.time()`"
math: true
output:
  blogdown::html_page:
    toc: true
menu:
  InsightStudying:
    parent: Foundations
    weight: 25
linktitle: Machine Learning
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(entropy)

# set seed for reproducability
set.seed(10201991)
```

# Considerations

## Bias-variance trade-off

## Overfitting and Underfitting

Cross Validation; Classification

Assess by comparing a performance metric on training and testing data.

## Cost Functions and Metrics

Modeling: Validation metrics, 

metrics (precision, recall, F1)





R squared, RMSE

Likelihood

regularisation (ridge, lasso), 

## Feature Engineering

### Seperability

### Assumptions about form of relationship

Process: feature selection, data cleaning / imputation with common pitfalls, model training, bias-variance trade-off

# Unsupervised

## Clustering

### k-means

### Agglomerative

### Devisive

## Matrix Decomposition

### PCA Principle Components Analysis

### SVD Singular Value Decomposition

## Text-based

### LDA Latent Direchilote Analysis

### LSA Latent Semantic Analysis

# Supervised

## Regression

### Linear

### Support Vector Machine (SVM)

## Classification

### Naive Bayes

### Discriminant Analysis

#### LDA

When to use: 

- 

Pros: 

- 

Cons: 

- 

#### QDA

When to use: 

- 

Pros: 

- 

Cons: 

- 

### KNN

When to use: 

- 

Pros: 

- 

Cons: 

- 

### Logistic Regression

When to use: 

- 

Pros: 

- 

Cons: 

- 

# Ensemble Methods

boosting, bagging

XGBoost, NLP (bag of words, vector-space models, sentiment analysis), Rec systems, Collab filtering, Optimization, XGBoost

## Trees and Forests

Gini Impurity:
$$
\begin{aligned}
m:&\text{ Region} \\
k:& \text{ Class} \\
G =& \Sigma_k \hat{p}_{m,k}(1-\hat{p}_{m,k})
\\=& 1 - \Sigma_k \hat{p}_{m,k}^2
\end{aligned}
$$

Decrease in Impurity
$$
\begin{aligned}
t:&\text{ Node} \\
N_t:&\text{ Samples at node }t \\
s_t:&\text{ Split }t \text{ into } t_L,t_R\\
\Delta i(s_t,t)=& i(t) - \frac{N_L}{N_t} i(t_L) - \frac{N_R}{N_t} i(t_R)
\end{aligned}
$$

Mean Decrease in Impurity: 
$$
\begin{aligned}
T:& \text{Trees} \\
N_T:& \text{Number of trees} \\
t:&\text{ Nodes} \\
s_t:&\text{ Split }t \text{ into } t_L,t_R\\
v(s_t):&\text{Variable split on at } s_t \\
p(t):& \text{Proportion of total samples at node } t \\
Imp(X_m)=&\frac{1}{N_T} \Sigma_{T}\Sigma_{t\in T:v(s_t)\in X_m} p(t) \Delta i(s_t,t)
\end{aligned}
$$


[Good blog on analysis of feature importance](https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e)

Also see [SHAP](https://github.com/slundberg/shap) for interpreting predictions from tree models!


Benefits of trees over OLS

https://www.quora.com/How-does-a-random-forest-fix-regression-problems-non-normality-heteroscedasticity-multicollinearity-outliers-missing-values-and-categorical-variables

### Regression

### Classification

Deep learning, Rec systems, Content and collaborative filtering, Optimization, Probabalistic programming

#### Assessment

Confusion matrix, F1, precision-recall curve

ROC, AUC

# Open ended Template

## Raw data structure? 

- Table of information about consumer
- Table of transaction history


## Define target variable

e.g. Fraud would be if a consumer calls and labels a transaction as fraud. 

## Preprocession, 

- EDA - distributions, correlations

## Feature engineering

## Performance metrics and how they apply

- Precision
- Recall 
- ... 

## Cautions: 

- Don't mention models you aren't familiar with

## Imagine rolling the model out 

- What issues might arise? 
- What actions do you take in production? 


## Model explainability

### Shapely additive

### Locally linear

- eg [here](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)

### Limitations

Limitations described a little in the FLowcast paper [here](https://flowcast.ai/Flowcast_whitepaper_-_Explainability.pdf)

- biased for collinear features?






