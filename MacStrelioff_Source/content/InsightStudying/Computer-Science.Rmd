---
title: "Computer Science"
author: "Mac Strelioff"
date: "`r Sys.time()`"
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
menu:
  InsightStudying:
    parent: Foundations
    weight: 15
linktitle: Computer Science
math: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(reticulate)

# setup python usage
# NOTE: python vars persist across chunks when knitting, but not in RStudio. 
knitr::opts_chunk$set(comment = ">>>")
#py_discover_config() # to see versions of Python
use_python("/anaconda3/bin/python")

# set seed for reproducability
set.seed(10201991)
```

# Foundations

Data type manipulation (char, string, numeric, binary, ascii) regular expressions; 

## Complexity

The worst-case complexity of an algorithm is represented with big-O notation. Big-O notation is adapted from mathematics where $O(f(n))$ is used to represent the terms that remain relevant when taking a limit of the computations required by the algorithm, $f(n)$, as $n$ approaches $\infty$. The same notation is used for time complexity (number of operations) and for space complexity (memory requirements). 

### Computational Complexity 

Here I'll check the number of computations required by algorithms of different complexities.
```{python}
ns_to_test = [0,1,10,20]
```

#### Constant Time: O(1)

$O(1)$ represents comstant time complexity -- a component of an algorithm that is only performed once, regardless of the input size.

```{python}
def constant_example(n):
    num_ops = 1      # 1 operation
    num_ops +=1      # 1 operation
    return num_ops   # total: 2 operations


for n in ns_to_test:
    out=constant_example(n)
    print('f({}): {}'.format(n,out))
```

#### Logrithmic Time: O(log(n))

Very slowly increases in computational demand. It can result from splitting the input on each recusrive call.

```{python}
def log_example(n,num_ops=0):
    # 1 operation per call
    # if n>1, half n and recursively call again
    if n>=1:
        n/=2; num_ops += 1
        num_ops += log_example(n)
    return num_ops
        
for n in ns_to_test:
    out=log_example(n)
    print('f({}): {}'.format(n,out))
```

#### Polynomial Time: O(n), O(n^2), ...

[good hackerrank video with explanation of polynomial complexity for a recursive solution to fibbinochi](https://www.youtube.com/watch?v=P8Xa2BitN3I&list=PLI1t_8YX-ApvMthLj56t1Rf-Buio5Y8KL&index=11)

Each for loop scales complexity by a factor of $n$, so one loop would be linear ($O(n)$), two loops would be quadratic $O(n^2)$, and three loops would be cubic ($O(n^3)$), and so on. The example below is a quadratic time example. 

```{python}
def linear_example(n): 
    num_ops = 0         # initialize counter
    for i in range(n):  # n times, perform...
        num_ops +=1     # 1 operation (n*1)
    return num_ops      # 1+n*1 = 1+n operations, O(n)


for n in ns_to_test:
    out=linear_example(n)
    print('f({}): {}'.format(n,out))

def quadratic_example(n):
    num_ops = 0             # initialize counter
    for i in range(n):      # n times
        for j in range(n):  # n times (n*n)
            num_ops+=1      # 1 operation (1*n*n)
    return num_ops          # total 1 + 1*n + 1*n*n, O(n^2)


for n in ns_to_test:
    out=quadratic_example(n)
    print('f({}): {}'.format(n,out))
```

#### Exponential Time: $O(x^n)$

With exponential time, the number of operations increases by a constant *factor* whith the length of the input. An example of this is in recursion, where a function iteratively calls itself $n$ times for each element of the input.

In the example below, I call the function twice within each call. The complexity is then a geometric series, and the closed for solution for the number of operations can be found with;

$$ \sum_{i=1}^N ar^{i-1} = \frac{a(1-r^{N})}{1-r} $$

Where $r=2$, $a=2$, $n$ is looped over, and we start the index at 0 ($i-1$) instead of 1, so: 

$$ \sum_{i=0}^n 2*2^{i} = \frac{(1-2^{n+1})}{1-2} = \frac{(1-2^{n+1})}{-1}=-(1-2^{n+1}) = 2^{n+1}-1 $$

```{python}
def exponential_example(n,num_ops=0):
    num_ops += 1 # 1 operation per call
    # recursively called twice for each n>0 (2**n times)    
    if n > 0:
        n-=1
        num_ops += exponential_example(n) # 2**(n)
        num_ops += exponential_example(n) # 2**(n)
    return num_ops # 
    
for n in ns_to_test:
    out=exponential_example(n)
    print('f({}): {}'.format(n,out))
```

## Data Structures

basic structures (tuples, list, arrays, dict), list comprehensions, 

Good descriptions of [lists](https://runestone.academy/runestone/books/published/pythonds/AlgorithmAnalysis/Lists.html), [dicts](https://runestone.academy/runestone/books/published/pythonds/AlgorithmAnalysis/Dictionaries.html), and [why some operations are much faster in sets or dict keys](https://stackoverflow.com/questions/8929284/what-makes-sets-faster-than-lists).

In depth description of [hash tables](https://en.wikipedia.org/wiki/Hash_table?fbclid=IwAR1N5-jAoM9e6iY58CDP9MAwycenvOXXJwmkpa0eBVDKed3RBs9uVBm9Kkc). 

Data structures (hash, stack, queue, tree, linked list); 

# Sorting

sorting (bubble, selection, insertion, merge, quick)

Sorting and graph algos (depth-first and breadth-first seach)

sorting (heap, tim)

Lots of [sort algorithms](https://www.geeksforgeeks.org/sorting-algorithms/)

e.g. 'find median in a large dataset' -- sort half of it.

# Searching

Lots of [search algorithms](https://en.wikipedia.org/wiki/Category:Search_algorithms)

## Linear Search: Binary Search

[good hackerrank video](https://www.youtube.com/watch?v=P3YID7liBug)

- requires sorting before applying the search

## Graph Search

Graphs are a collection of nodes and edges (connections between nodes). In the code below, I use dictionaries to represent graphs -- keys represent nodes, and the list of elements represents all of the nodes that the parent node is connected to. 

```{python}
# examples
Examples = []

# Example 1: something where DFS would do equally well?
tmp_graph = {1: [2,4], 2: [3], 3: [5], 4: [5,7], 5: [6,7], 6: [], 7: []}
Examples.append(tmp_graph)

# Example 2: something where DFS will run faster
tmp_graph = {1:[2,3,4,5],2:[6,10,8,9],3:[],4:[],5:[],6:[7],7:[],8:[],9:[],10:[]}
Examples.append(tmp_graph)

# Example 3: something where BFS will run faster (swapped 3 and 10)
tmp_graph = {1:[2,7,4,5],2:[6,10,8,9],3:[],4:[],5:[],6:[3],7:[],8:[],9:[],10:[]}
Examples.append(tmp_graph)
```


### Breadth First (BFS) and queues

Breadth Frist Search (BFS) queues (FILO) nodes based on their distance from an origin node. This can be implemented with a list (queue) by searching the nodes at the head of the list and appending encountered nodes to the tail of the list. 

```{python}
# breadth first search: just a queue
def BFS_queue(graph,start,end):
    queue = [start] 
    visited = set()
    # build and search through queue
    while queue:
        visiting = queue[0]
        # if this was visited already, continue
        if visiting in visited: continue
        # mark node as visited
        visited.add(visiting)
        # if this is the end, return 
        if visiting == end: return True,visited
        # otherwise, append children to search queue
        for child in graph[visiting]: queue.append(child)
        queue.remove(visiting) # remove the visited node
    # if end was never found, return False
    return False,visited

# breadth first search: For loop over each layer
def BFS(graph,start,end):
    queue = [start] 
    visited = set()
    # build and search through queue
    while queue:
        for visiting in queue:
            # if this was visited already, continue
            if visiting in visited: continue
            # mark node as visited
            visited.add(visiting)
            # if this is the end, return 
            if visiting == end: return True,visited
            # otherwise, append children to search queue
            for child in graph[visiting]:
                queue.append(child)
    # if end was never found, return False
    return False,visited
```

### Depth First (DFS) 

Depth First search can be implemented with recursively (below) or with a stack (LIFO). 
[good hackerrank video](https://www.youtube.com/watch?v=zaBhtODEL0w)

- DFS can use a dictionary or set to 

```{python}
# Depth first search with recursion
def DFS(graph,start,end):
    visited = set()
    return _DFS(graph,start,end,visited),visited
    
def _DFS(graph,start,end,visited):
    # if this was visited already, return False
    if start in visited: return False
    # mark node as visited
    visited.add(start)
    # if this is the target node, return True
    if start == end: return True
    # DFS through the children of this node
    for child in graph[start]:
        if _DFS(graph,child,end,visited):
            return True
    return False
```

### DFS vs BFS

```{python}


for example in Examples:
    print('DFS  path: ', DFS(example,1,7))
    print('BFSq path: ', BFS(example,1,7))
    print('BFS  path: ', BFS(example,1,7),'\n')
```


# Ranking Algorithms

Lots of [ranking algorithms](https://en.wikipedia.org/wiki/Ranking_(information_retrieval))

# Recommendation

Lots of [recommendation algorithms](https://en.wikipedia.org/wiki/Recommender_system)


<!---
Data science methods play a major role in discovering recommendations for users. 

Econ, compliments and substitutes (competitors). 

Types of recommendation; compliments (these go together), competitors (you might also like...), temporal (might buy this again in the future). Horizontal and vertical focus 

Temporal involves forecasting, which is a topic for another post. 
--->

# Algorithms

lambda functions;

```{python}

nums=[1,1,2,2,1]
val=1

count = 0
for i in range(len(nums)):
  if nums[i] != val:
    nums[count] = nums[i]
    count += 1

print(count)
print(nums)
```


# TODO / Other Topics:  

## recursion; 

## Dijkstra's algorithm, 

## dynamic programming (knapsack problem, Fibonacci sequence)

## Functional programming

# Tools

## Git and GitHub

git for version control (resolving merge conflicts)

## SQL 

### General Structure of SQL querys

[Order of execution for clauses below](https://sqlbolt.com/lesson/select_queries_order_of_execution)

```{, eval=FALSE, echo=TRUE}
SELECT (column names to select) [AS (name for selection)]

FROM (table to select from)

WHERE (logical condition on rows)
  column = (value or subquery),
  OR column IN (list or subquery)
  
GROUP BY 

HAVING (constraint on grouped rows)
  
LIMIT n [OFFSET m]

```

The `SELECT` clause can contain aggregations, and subqueries. 

The `FROM` clause can contain joins.

The `WHERE` clause can also contain sub queries.

### Aggregation functions

The `GROUP BY` clause can contain `GROUPING SETS`, such as a `ROLLUP()` and `CUBE()` functions. These hierarchically group the rows passed to the aggregation functions. `ROLLUP()` hierarchically groups by arguments, starting left to right. `CUBE()` passes all permutations of its arguments to the aggregation functions. 

[Grouping sets are described a little here](https://pgexercises.com/questions/aggregates/fachoursbymonth3.html).

### Window functions

[Orver() explained here](https://pgexercises.com/questions/aggregates/countmembers.html). Some more detailed information [here](http://www.sqlservertutorial.net/sql-server-window-functions/).

Window functions include `OVER()` and `ROW_NUMBER()`. They take two optional arguments, `PARTITION BY` and `ORDER BY`. 

# Resources

- SQL training resources;
- [pgexercises](https://pgexercises.com/)
- [Udacity course](https://classroom.udacity.com/courses/ud198)
- [sqlbolt](https://sqlbolt.com/lesson/select_queries_introduction)
- [Mode Analytics](https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/) HackerRank (basic select, aggregation)
- [Hacker Rank](https://www.hackerrank.com/domains/sql)
- [W3School](https://www.w3schools.com/sql/exercise.asp?filename=exercise_select1)

- Python and other coding languages:
- [HackerRank](https://www.hackerrank.com/dashboard)
- [LeetCode](https://leetcode.com/)













