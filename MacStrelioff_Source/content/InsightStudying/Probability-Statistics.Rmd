---
title: "Probability and Statistics"
author: "Mac Strelioff"
date: "`r Sys.time()`"
math: true
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
menu:
  InsightStudying:
    parent: Foundations
    weight: 20
linktitle: Probability and Statistics
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(entropy)

# set seed for reproducability
set.seed(10201991)
```


<!--
Fermi problems (order of magnitude)
--> 

<!--
- Make YouTube videos on each!
-->

<!--
# What is a probability, and how does that relate to statistics? 

- Probabilities are commonly thought of in three ways; 

1. Purely mathematical
2. Magnitudes of belief
3. Relative frequencies of infinate events

I use probability theory to quantify beliefs and make decisions. 

Logic, about truths of the universe. 
By there is uncertainty in what we know, and uncertainty in our measurements and observations. Statistics as a field is concerned with specifying our knowledge of the laws of the universe and the uncertainty around them. They do this by applying probability theory. 

Probability theory generalizes logic to situations where we aren't certain. And logic emerges from probability theory when we know things with certainty. 

Hypothesis testing

Statisticians use probability theory to decide on what is and isn't true
-->

# Probability Foundations

<!--
Events, event spaces, 
combinatorics (permutations and combinations)
-->

A random variable random variable is a variable with an unknown value, but known possible values. An event is an observed value of a random variable. An event space contains all possible values of the random variable. Probabilities are values assigned to the events in an event space (i.e. the possible values of a random variable) and represent how likely each event is relative to other events in the event space.

Here $p(e)$ will be used to represent the probability of an event. Some special probabilities include the probability of any event in the event space, which is 1. And the probability of any event other than $p(e)$, known as the compliment of $p(e)$, denoted with $p(\neg e)$, is found by; 

$$
p(\neg e) = 1 - p(e)
$$

## Probability Rules

A joint event refers to two or more events occurring together. They are colloquially talked about as one event 'and' another event occurring together. More formally, joint events are called intersections (represented with the $\cap$ symbol) between events. For events $e_1$ and $e_2$, the probability of their joint event will be represented with $p(e_1 \cap e_2)$. The probability of an intersection of events is the same regardless of which event is considered first; 

$$
p(e_1 \cap e_2) = p(e_2 \cap e_1)
$$

For two events, $e_1$ and $e_2$, their intersection is found by;
$$
p(e_1 \& e_2) = p(e_1\cap e_2) = p(e_1|e_2)p(e_2)
$$
where $p(e_1|e_2)$ is a conditional probability, discussed in the next section. 


Conditional events refer to one event, $e_1$, after another event, $e_2$, is known. $p(e_1|e_2)$ represents the probability of $e_1$ given, or after knowing, $e_2$. These can be defined by rearranging the multiplication rule as follows; 
$$
p(e_1|e_2)p(e_2)=p(e_1\cap e_2) \Rightarrow p(e_1|e_2)= \frac{p(e_1\cap e_2)}{p(e_2)}
$$

Noting that, by the reflexively of joint events and the definition of the multiplication rule, $p(e_1 \cap e_2) = p(e_2 \cap e_1)= p(e_2 | e_1) p(e_1)$, and so the above equation becomes; 
$$
p(e_1|e_2)=\frac{p(e_2 | e_1) p(e_1)}{p(e_2)}
$$
This equation is known as Bayes' Theorem or Bayes' Rule.

A union (represented with the $\cup$ symbol) of events refers to at least one of multiple events occurring. For events, $e_1$ and $e_2$, their union would include the probability that $e_1$ occurs, the probability that or $e_2$ occurs, and the probability that both $e_1$ and $e_2$ occur. Colloquially this is talked about the probability of $e_1$ 'or' $e_2$. Formally this is expressed and computed as; 
$$
p(e_1\text{ or } e_2) = p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1 \cap e_2)
$$

Where $p(e_1 \cap e_2)$ is a joint probability.

Independence is a common assumption in many statistical techniques. Statisticians assume independence primarily because it simplifies the computation of certain probabilities. Events are said to be independent if knowing one event does not change the probability of the other event. Formally, if events $e_1$ and $e_2$ are independent, this would mean that; 
$$
\begin{aligned}
p(e_1|e_2) &= p(e_1) \\
p(e_2|e_1) &= p(e_2)
\end{aligned}
$$

If $e_1$ and $e_2$ are independent, then their joint probability simplifies as so; 
$$
p(e_1 \cap e_2) = p(e_1|e_2)p(e_2) = p(e_1)p(e_2)
$$
Where the last equality is only true if $e_1$ and $e_2$ are independent (i.e. $p(e_1|e_2)=p(e_1)$). 

Then the probability of their union simplifies to;
$$
\begin{aligned}
p(e_1 \cup e_2) &= p(e_1) + p(e_2) - p(e_1|e_2)p(e_2) \\
&= p(e_1) + p(e_2) - p(e_1)p(e_2)
\end{aligned}
$$
Where again, the last part of this equality is only true if $e_1$ and $e_2$ are independent.

Events $e_1$ and $e_2$ are said to be mutually exclusive if the occurrence of either event precludes the occurrence of the other event. Formally mutual exclusivity means that; 
$$
p(e_1 | e_2) = 0 \text{ and } p(e_2 | e_1) = 0
$$

If two events are mutually exclusive, then the probability of their joint event is; 
$$
\begin{aligned}
p(e_1 \cap e_2) &= p(e_1|e_2)p(e_2) \\
&= 0*p(e_2) \\
&= 0
\end{aligned}
$$

And the probability of their union simplifies to;
$$
\begin{aligned}
p(e_1 \cup e_2) &= p(e_1) + p(e_2) - p(e_1 \cap e_2) \\
&= p(e_1) + p(e_2) - 0 \\
&=p(e_1) + p(e_2)
\end{aligned}
$$

<!--
## Practice Problems and Walkthroughs

- link to my YouTube videos on these things

Practice problems on the amoeba [here](https://www.quora.com/Bobo-the-amoeba-has-a-25-25-and-50-chance-of-producing-0-1-or-2-offspring-respectively-Each-of-Bobos-descendants-also-have-the-same-probabilities-What-is-the-probability-that-Bobos-lineage-dies-out) and  [here](https://www.quora.com/An-amoeba-has-a-75-chance-of-splitting-in-two-and-25-chance-of-dying-Is-there-an-intuitive-reason-why-the-probability-of-extinction-is-not-1).
-->

## Probability Functions

<!--
Many machine learning algorithms are different ways of estimating f(x)
-->

Here I'll describe the properties and use cases for probability functions and cumulative probability functions. Conventionally, use $f(x)$ is used represent a probability function, and $F(x)$ to represent a cumulative probability function. $f(x)$ is commonly called a probability mass function (pmf) if $x$ is discrete, and a probability density function (pdf) if $x$ is continuous. $f(x)$ and $F(x)$ are related through integration: 

$$
\begin{aligned}
f(x) &= \frac{d}{dx}F(x) \\
F(x) &= \int_{-\infty}^{\infty} f(x) d_{x}
\end{aligned}
$$

For discrete variables, the probability function represents the probability that $X$ takes a specific value $x$: $f(x) = p(X=x)$. For continuous variables, the probability that the variable $X$ takes any particular value is technically $0$; $p(X=x)=0$. However, $p(x)$ is still related to probability functions $f(x)$ through areas. For an arbitrarily small $\epsilon$ the probability that $X$ takes a value between $x-\epsilon$ and $x+\epsilon$ is the area of the distribution over that range; 

$$
\begin{aligned}
p( x - \epsilon< x < x+\epsilon) &= F(x+\epsilon) - F(x-\epsilon) \\
&= \int_{-\infty}^{(x-+\epsilon)}f(x)d_{x} - \int_{-\infty}^{(x-\epsilon)}f(x)d_{x}
\end{aligned}
$$


Probability functions have to satisfy a few conditions: 

1. 
2. 
3. 

<!--
Transformations of variables!
-->

<!--
CDF transformation to convert from uniform into any random variable with a CDF
-->

<!--
Link to YouTube video on the circle problem
- circle problem
-->

<!--
### Probability Distributions
Make YouTube videos on each. 
- 1) Walk through properties from Wikipedia!
- 2) Walk through some simulations in Python!
-->

<!--
## Central limit theorem

Sum of any random variable converges to normal distribution. 
- show for binomial case, to justify Z test
-->

## Expectation

<!--
expectation, variance, conditional or iterated expectations, law of total variance, variance of multiple variables
-->

For any functions of a randome variable, $g(x)$, the expectation of that function is; 

$$
E_{x}(g(x))=\int_{-\infty}^{\infty}g(x)f(x)d_{x}
$$

For discrete random variables, $E(x)=\sum_xxp(x)$. For continuous random variables, $f(x)dx$ is conceptually $p(x)$ since it is the area under an infinately small segment of the probability function if $f(x)$ represents the height and $dx$ represents an infinately small width. 

Expectation is a linear operator, so for constants $a$, $b$, and $c$ and random variables $x$ and $y$; $E(ax+by+c) = aE(x)+bE(y)+c$.

Expectation for joint random variables

$$
E_{x,y}(g(x,y))=\int_{s=x}\int_{t=y}g(s,t)f(s,t)d_{s}d_{t}
$$

Marginalizing a distribution from a joint probability function ($f(x,y)$) or a conditional probability function ($f(x|y)$):
$$
\begin{aligned}
f_x(x)&=\int_yf_{x,y}(x,y)d_{y}\\
&=\int_y f_{x|y}(x|y)f_y(y)d_{y}\\
&=E_y(f(x|y))
\end{aligned}
$$

[Law of total expectation, and rough proof](https://en.wikipedia.org/wiki/Law_of_total_expectation)

$$
\begin{aligned}
E_{x}(x)&=E_{y}(E_{x|y}(x|y))\\
&=\int_y\int_{x}xf_{x|y}(x|y)d_{x}f(y)d_{y} \\
&=\int_{x}x\int_{y}f_{x|y}(x|y)f(y)d_{y}d_{x} \\
&=\int_{x}xf(x)d_{x}\\
&=E_{x}(x)
\end{aligned}
$$

The last steps reflect that the internal integral is just marginalizing across y; $f_x(x)=\int_y f_{x|y}(x|y)f_y(y)d_{y}$.

<!--
Linear regression as a conditional expectation
Could show a simple figure with Y~1 and Y~X as example of expectation versus conditional expectation

Conditional expectation 
$$
E(g(x)|Z)=\int_{-\infty}^{\infty}p(x|Z)g(x)dx
$$

in linear regression, the expectation of Y is the linear combination of X and B, since those are deterministic, and the only non-deterministic part, epsilon, has expectation 0. 

- So the property that expectation propogates through sums leads to this result..
--->


## Variance

Covariance and variance

$$
\begin{aligned}
Cov(X,Y)&=E_{x,y}((X-E_{x}(X))(Y-E_{y}(Y)))\\
Var(X)&=Cov(X,X)=E_{x}((X-E_{x}(X))^2) \\
&= E(X^2)-E(X)^2
\end{aligned}
$$

The [Bias-Variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) decomposes the theoretical variance obtained when estimating a function $g(x)$ with a model $\hat{g}(x)$ into three components that can be used to motivated changes to the model. Specifically, the Bias-Variance tradeoff is a decomposition of the expected prediction error; $g(x)-\hat{g}(x)$. To see it's relation to variance, first rearrange the variance equation above:

$$
\begin{aligned}
Var(X)= E(X^2)-E(X)^2 \\
\Rightarrow E(X^2) = Var(X)+E(X)^2 \\
\end{aligned}
$$

Then define the random variable $X$ to be the errors of the model; 

$$
\begin{aligned}
E((g(x)-\hat{g}(x))^2) &= Var((g(x)-\hat{g}(x))^2)+E((g(x)-\hat{g}(x)))^2 \\
&= (g(x)-E(\hat{g}(x)))^2 + E((E(\hat{g}(x))-\hat{g}(x))^2) + Var(g(x))\\
&= Bias(\hat{g}(x))^2 + Var(\hat{g}(x)) + Var(g(x))
\end{aligned}
$$

Here bias represents the squared error between the true function $g(x)$ and the expected model $E(\hat{g}(x))$. Variance represents the squared error between the expected model and the obtained model. And $Var(g(x))$ represents the true noise or irreducible error in the process $g(x)$. 

Variance of linear combinations of random variables: 

$$
Var(aX+bY+c)=a^2Var(X)+b^2Var(Y)+2abCov(X,Y)
$$

Variance is also crucial for designing experiments, for example if $a$ or $b$ have different signs and $Cov(X,Y)>0$, then the variance between is reduced because the covariance is subtracted off. This is a justification for within-participant designs, where $X$ represents the first measurement, $Y$ represents a future measurement, $a=-1$ and $b=1$ to reflect a difference comparing the second and first measurements ($Y-X$), and since the measurements come from the same person here, it is reasonable to assume they are correlated ($Cov(X,Y)>0$). In this case, $Var(X,Y)=Var(X)+Var(Y)-Cov(X,Y)$, which is not larger than the variance that would be obtained if the measurements were from independent samples. 

<!--
Confidence intervals, prediction intervals
-->

<!--
Variance of a function:
$$
\begin{aligned}
Var(g(x)) &= E((g(x)-E(g(x)))^2)\\
&\approx \left(\frac{d}{dx}g(E(x))\right)^2var(x) 
\end{aligned}
$$
-->

[Law of total variance](https://en.wikipedia.org/wiki/Law_of_total_variance): The variance of a variable $X$ can be decomposed into the variation in the variable that remains when $Y$ is known or irreducible error ($E(Var(X|Y))$), and the variation that carries through from the uncertainty in the model estimation $Y$ ($Var(E(X|Y)$).
$$
\begin{aligned}
Var(X)&=E(Var(X|Y))+Var(E(X|Y))
\end{aligned}
$$

This arises in hierarchical models and prediction intervals. 

$$
\begin{aligned}
x               &\sim N(\mu,\sigma)\\
E(x)            &=\mu\\
Var(x)          &=\sigma^2\\
\hat{\mu}_n     &\sim N\left(\mu,\frac{\sigma}{\sqrt{n}}\right)\\
E(\hat{\mu}_n)  &=\mu\\
Var(\hat{\mu}_n)&=\frac{\sigma^2}{n}\\
E(x_{n+1})      &= E(E(x_{n+1}|\hat{\mu}_n))= E(\hat{\mu}_n)=\mu \\
Var(x_{n+1})    &=E(Var(x_{n+1})) + Var(E(x_{n+1}|\mu_{n})\\
                &=\sigma^2+\frac{\sigma^2}{n}\\
(100-\alpha)PI: & E(x_{n+1}|\hat{\mu}_n) \pm T_\alpha \sqrt{Var(x_{n+1})}\\
\end{aligned}
$$

<!--
Expectation and variance are the foundation for decision making
- value function v(x) that assigns values to the outcomes x
- If you know the distribution and value function, then you can directly compare choices based on their expected value
-->

<!--
## Models as conditional expectations

Models output an expectation that depends on the model, its structure, its features, and in Bayesian settings it's priors
-->

## Information Theory

Information or Surprise is a measure of how unexpected an event was: 
$$
I(x)=-log(f(x))=log\left(\frac{1}{f(x)} \right)
$$

Entropy is the expected surprise -- higher entropy relates to more uncertainty: 
$$
H(X)=E(I(x)) = E(-log(f(x)))  = \int_X f(x)I(x)dx = -\int_X f(x)log(f(x))d_x
$$

K-L Divergance, or relative entropy, is the expected distance between distributions with respect to one of the distributions: 
$$
\begin{aligned}
D_{KL}(f_{x}(x);f_{y}(x)) &= \int_{X} f_{x}(x)log\left(\frac{f_{y}(x)}{f_{x}(x)} \right)d_x\\
&= \int_X f_{x}(x)(log(f_{y}(x))-log(f_{x}(x))d_x \\
\end{aligned}
$$

Divergance is an important concept in machine learning, because it can be a loss function when we are estimating a distribution $f(x)$ with a model $\hat{f}(x)$. From a Bayesian perspective, Divergance between a prior and posterior measures the information gained by observing the data. Bayesian experimental design focuses on collecting data that maximizes the divergance between the posterior and prior, i.e. the most informative data.

Mutual Information is the divergance from the joint distribution $f_{x,y}(x,y)$ to the joint when independence is assumed $f_x(x)f_y(y)$ -- i.e. a measure of non-independence;
$$
I(x;y)=\int_{y} \int_{x} f_{x,y}(x,y)log\left(\frac{f(x,y)}{f(x)f(y)}  \right)d_yd_x
$$

# Causal Inference

The ultimate goal of most statistical work is to discover the causes of some outcome so that the outcome can be controlled. Hence, it is important to understand when causality can be inferred. 

From this perspective, there is a treatment assignment $T$ and an outcome of interest that depends on the treatment $Y(T)$. Ideally, we would be able to compute the expected treatment effeoct for any individual $E(Y(T=1)-Y(T=0))$, but it is impossible to assign an individual to both variants of the treatment under the exact same conditions -- e.g. one must be done at a later time than the other. Causal inference approaches specify the assumptions, models, and designs neeed to make causal statements. 

## Common Methods

Some summarized in this [Medium Post](https://towardsdatascience.com/causal-inference-using-difference-in-differences-causal-impact-and-synthetic-control-f8639c408268)

### Diff in Diff

Contexts and example ..



Assumptions: 

- The trend in the control group is an appropriate proxy for the trend in the treatment group, had the treatment not occurred.

Model:

$$
\begin{aligned}
I_{T}&:\text{Indicator for treatment group} \\
I_{Post}&:\text{Indicator for post treatment} \\
Y &= \beta_0 + \beta_1 I_{T} + \beta_2 I_{Post} + \beta_3 I_{T} I_{Post}
\end{aligned}
$$


### Causal Impact

Causal impact was developed by Google, docs [here](https://google.github.io/CausalImpact/CausalImpact.html).

Context: 

Assumptions: 

Model: 

- Uses Bayesian structural time series to construct a counterfactual group -- an estimate of what the time series would have looked like if the treatment had not been assigned. The method is described in [this video](https://www.youtube.com/watch?v=GTgZfCltMm8).
- 




### Synthetic Controls

advertising and exposure to an ad?

### IV Analysis

particular way of interpreting a regression?
- maybe the analysis Day talked about at Pintrest?


### Regression Discontinuity

Yelp example


# Experiment Design


## Foundations

- Hypotheses
- Treatment and control conditions
- assumptions (including CI assumptions, SUTVA, ...?)

## Concerns

- network effects: front-end, it could be from different interactions, back-end it could be from changes in algorithm behavior for units in C based on different behavior from units in T -- e.g. if a new recommender for units in T influences them to watch more of x, then x may also get recommended more to those in C. 
- learning effects (change aversion, novelty seeking)
- early adopters
- Non-compliance: Those assicned to treatment may not actually experience the treatment. 
- Crossover: Those assigned to control might gain access to the treatment. 
- Treatment inhomogenaity: Some user segments might respond differently than others.

## Treatment Units

What is the unit at which we assign treatments? 

Ideally independent individuals, but online it is hard to know who is visiting a webpage, or crucial to keep experiences comparable across devices. Also, in networks, individuals are not independent and it is important to keep user experience consistent across connected individuals. 

Proxies for individuals include;

- User ID or account -- most clearly tied to a user, but 
- cookies -- device and browser specific, so these could differ across a user's browsers or devices.
- IP address (device request return address) -- device specific, so a user's experience might differ across devices.

In networks, loosely connected clusters of individuals can be used as experimental units (see unofficial google data science blog post on this).

- a solution to SUTVA violations, but decreases sample size and power dramatically
- cluster-based, stratified, serial, balanced, ... 
- propensity scores and matching

## Generative Model and Adjustment Variables

- Specify the hypothesized generative process
- Confounds
- Precision
- Neusance

## Common Designs

A/B testing

A/A testing to estimate variation

Variance reduction designs (paired designs, matching, ...)

Bandits for limited data or maximizing an objective

## Assignment Mechanism

- maps samples (xi,yi) into treatment or control conditions.
- randomized control trials ideal, not always possible
- other options, ...?
- test assignment validity, could use propensity scores or maybe chi square.

## Duration

- learning effects: initial exploration or novelty seeking
- learning effects: initial change aversion
- power analysis for sample size
- time to run to mitigate 

<!---
# Experiment Design

## Objective, demand, and value

Objective is based on a metric (increase clickthrough or revenue per user). 

Demand -- if adding a new user feature, how can demand for the feature be assessed? 

value = benefit - cost. 

Expected value helps with decisions about the size and duration of an experiment

## Constructs, metrics, and scoping

Once an objective is clear, the details of what can be measured need to be flushed out. 

Metrics (fill in from udamy course section on metrics)

Netflix metrics -- streaming hours, retention (users staying on platform), viewing for a title (e.g. effected by artwork -- but may be at the detrement of general viewing?) 

### User flow and target metrics

Think of the sequence of actions a user might take on the site. Experiments can target transtition probabilities between any user/platform states. 

- number of clicks
- time on page
- ... 

### Invariants

Metrics that shouldn't change or differ across groups. For example, demographic variables should be the same across groups if the randomization or balancing worked properly. Also, many application performance metrics and business metrics should be unchanged, or monitored just in case they change. 

### Confounding variables

Mitigated by randomization. 

### Precision variables

Can also highlight features that one would want to match treatment and control on.

Examples: 

- number of posts
- number of followers
- visibility / impressions

## Metric Validation

User expreience research, retrospective analyses of past data or log files, ... . 

## Conditions

Define the experimental manipulations. 

## Treatment Units

What is the unit at which we assign treatments? 

Ideally independent individuals, but online it is hard to know who is visiting a webpage, or crucial to keep experiences comparable across devices. Also, in networks, individuals are not independent and it is important to keep user experience consistent across connected individuals. 

Proxies for individuals include;

- User ID or account -- most clearly tied to a user, but 
- cookies -- device and browser specific, so these could differ across a user's browsers or devices.
- IP address (device request return address) -- device specific, so a user's experience might differ across devices.

In networks, loosely connected clusters of individuals can be used as experimental units (see unofficial google data science blog post on this).

## Assignment Mechanism 

The assignment mechanism samples members from a population and assignes them to conditions. 

How are units assigned to treatments?

Randomized control trials are the ideal, but many issues arise in online experimentation settings. 

(look up desirable properties form causal inference notes)

### Population 

What group is being sampled from?

### Cohorts

Random sample, cluster-based, stratified, serial, balanced, ... 

### Synthetic Control Groups

Propensity matching

### Limitations

- Non-compliance: Those assicned to treatment may not actually experience the treatment. 
- Crossover: Those assigned to control might gain access to the treatment. 
- Treatment inhomogenaity: Some user segments might respond differently than others.

# Implementation

Batched, or real-time

## Size

power, sample size

## Duration

- learning effects: initial exploration or novelty seeking
- learning effects: initial change aversion

Solution: Consider running experiment past any initial observed effect.

### Temporal variation

### Optional stopping

Why it's an issue for frequentists

Optamizely using a threshold on FDR from likelihood ratio tests in frequentist setting

Bayesian justifications

p-values versus likelihood ratios

## Estimands

- Average treatment effect
- Treatment on treated

## Effiencicy or Variance Reduction

- look up variance reduction in A/B tests, a common method might be to incorporate precision variables like demographic data or user data such as device or browser. 

e.g. with a two sample t-test, $Y_t - Y_{t-1} = \beta_0$, versus a model $Y_t = \beta_0 + \beta_1 Y_{t-1}$

- get table for different tests of means from soc sci 10 notes.

## Checking assignment integrety

- Check demographics across buckets, if randomization worked then demographics should be approximately equally represented in the buckets. 

--->

## Integrety Checks

- check that those assigned to treatment received it
- check that stratifications were implemented correctly
- Check demographics across buckets, if randomization worked then demographics should be approximately equally represented in the buckets. 


## Maximum Likelihood versus Bayes' Estimators

likelihood is one term in Bayes' Theorem

# GLMs

Again about the variance -- this time it takes a form other than normal.

likelihood, score, fisher information, robust veriance, ...?


# Common Tests

## t-test

- different variance formulations. 
- one-sample
- two independent sampels
- two dependent samples

## ANOVA

## Linear Model

## Z test for proportions

Binary data

## Chi-Square

## Logistic Regression

## Likelihood Ratio and Bayes Factors

## always valid p-values (ck optimizely white paper)

# Different Estimands For CI

- Average treatment effect
- Treatment on treated

# more on variance

## Emperical Variance Estimation

bootstrapping

A/A testing

## Mixed Effects Models

- Maime's consulting project? Other use cases?

## Effiencicy or Variance Reduction

- look up variance reduction in A/B tests, a common method might be to incorporate precision variables like demographic data or user data such as device or browser. 

e.g. with a two sample t-test, $Y_t - Y_{t-1} = \beta_0$, versus a model $Y_t = \beta_0 + \beta_1 Y_{t-1}$

- get table for different tests of means from soc sci 10 notes.


## GLMs
<!--
models for when the errors aren't assumed to be normally distributed
--->

Stats: descriptive, R-squared, chi-squared;  

Probability: distributions, CLT, sampling distributions, p-value, 

Stats: k-s, Q-Q plot, hypothesis testing, experimentation; 

Probability: Bayes, bootstrap

Probability: maximum likelihood estimation; time series analysis (ARIMA models), granger causality

Dynamic experimentation (multi-armed bandits)

Unit testing; EDA visualization (seaborn, Plotly, Bokeh)


# Causal Inference Methods

Good medium blog post [here](https://towardsdatascience.com/causal-inference-using-difference-in-differences-causal-impact-and-synthetic-control-f8639c408268)

For each mention the assumptions and the model.

## Difference in Differences

## Causal Impact

## Synthetic Controls

## Propensity Score Matching

## Fixed Effects Regression

## Instrumental variables

## Regression Discontinuity





