---
title: "Behavioral Interviewing"
author: "Mac Strelioff"
date: "`r Sys.time()`"
math: true
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
menu:
  InsightStudying:
    parent: Foundations
    weight: 99
linktitle: Behavioral Interviewing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(entropy)

# set seed for reproducability
set.seed(10201991)
```

# Principles

- Interviews are conversations, not tests
- Use stories to establish a connection and demonstrate fit for a role and company


Tell me about yourself, your academic research, why data science?; 30 second elevator pitch vs. 2 minute phone screen/on-site

Why this company? what value can you add? Tell me about a time when… 

STAR chart (Situation, Task, Action, Result)

Mock interviews: Practice, practice, practice 

# Common Behavioral Questions

## "Tell me a little about your background."

This questions gives an opportunity to demonstrate claer communication, curiosity, enthusiasm, transferrable skills, and value adds. 

Template: 

- Problem: Concepts and context
- Curiosity: Open question
- Value: Why should people care
- Project Outline: Methods, results

Example answer: 

- Problem: Wisdom of the crowds, like the social science equivalent of ensemble learners. However, many fallacies and biases in probabilistic reasoning hove been demonstrated in lab settings. 
- Context: Ryan and I.. I was like a statistician/behavioral economist, and he was a programmer. So I'd lead algorithm developemnt and he would code and deploy.
- Curiosity: Would prices in prediction markets be good estimators of real probabilities, since they incentivise and aggregate individual estimates, or would they show the same biases as individual probabilistic judgments, since they are aggregating over biased individuals? 
- Value: Finding systematic biases could result in profitable trading strategies.
- Project Outline: 

prediction markets are used kind of like filtering, where we aggregate over individual opinions to get a more accurate concensus opinion. But, individual opinions have systematic biases, so would the aggregation of these also have biases? To answer this, I partnered with a large prediction market provider and found evidence of mispricings consistent with individual level biases -- miscalibration and also subadativity (superaditivity)?... 

Evaluation:

- Communication: Clearly explain prediction market mechanics, wisdom of the crowds, and an individual level fallacy. 
- Enthusasism: Interest in markets and complex problems. 
- Transferrable skills: Mention some of the models used.
- Value adds: Realized prediction markets are biased close to expiration -- their signal is weaker than previously thought. Also able to profit literally by rolling out trading algorithms. 


### Child bandits: 

Bandit tasks are a common paradigm for studying individual level decision making and making claims about the way people make decisions. But in lots of contexts, the results you get when you addregate over trials can be different from the actual result on any trial. So I developed a model to infer on a trial by trial level, what decision making strategy kids were using to choose between two 'bandits' that gave out stickers. -- I have an interest in using more customized models and Bayesian inference to overcome issues that arise from aggregating data at too high a level. If you had averaged over trials, then there may have been evidence for one or another model, but the dynamic changes in decision making strategies would have gone unnoticed. 

- Similar approaches could be useful for dynamic experimental designs, OR in the context of Netflix, for personalization. If we could dynamically infer something like the strategy or emotional state of a user, then we could use that inference to personalize our recommended content or things like cover art or other aesthetic aspects of Netflix. 


## "Tell me about the project you worked on at Insight."

This question gives an opportunity to demonstrate what I'd be capable of on day 1. Clarity 

Template: 

- Purpose: Client acquired users through a free trial. They wanted to 1) identify, early on, who would become a subscriber, and 2) How successful users were using the product. 
- Methods: I usually think about a generative process. Here I thought about LDA or QDA. Later thought about other algorithms because this was primarily a prediction problem. 
- Challenges: Non-normal features (could break LDA, QDA)

Generally, the answer can show clear communication of a problem, an approach, challenges, solutions, and relevance to the company or role.

At Insight I worked on a consulting project for an external company. This company's user funnel was something like -- visitors come to their website, are offered a trial, and some portion of trialers become subscribers. Problem was, around 80% of trialers weren't becoming subscribers. 

How could I have used a mixed effects model here? (See Maime's blog?)

Evaluation: 

- Clarity: 
- Relevance: Do the skills add value to the company? 
- Logic: Were sensible solutions tried in a sensible order? 

## "Why are you interested in a career as a data scientist?"

This is an opportunity to show that becoming a data sceientist at the company is the logical next career move, and to demonstrate that you know what to expect in the new role. 

Outline of topics that may resonate with a hiring manager: 

- Skills, Technical: Tools, technologies, methods you have used.
- Skills, Conceptual: Your approach to understanding problems and navigating to solutions
- Interests: What are you eager to do in this next phase of your professional life?
- Interests, Broad: Knowledge of problems tackled by data professionals in this industry
- Interests, Specific: Knowledge of technical challenges that will be faced by someone in this role
- Growth, Skills: What you’re excited to learn in the next few years
- Growth, Trajectory: Relate this job to an ultimate career trajectory


Example answer: 




Evaluation: 

- Educated Enthusasiam: 
- Professional Goals: 
- Niche area: 


## "Tell me about your research / talk to me about your last role."




Evaluation: 

- Skills: 
- Relevance: Show how previous work relates to the responsibilities of the current role. 
- Clarity: 



## "Why are you interested in joining this company?"

This is an opportunity to demonstrate knowledge of and interest in the current role. 


Evaluation: 

- Industry knowledge: 
- Company knowledge: 
- Potential for impact: 

## "Talk about a time you were on a team that struggled with effective communication." 


- Situation: Where / when did this happen? What were you working on, and with whom?
- Task: What were you responsible for? What challenges did you face? 
- Action: How did you decide the appropriate response to the situation? How did you implement it?
- Result: How do you know your solution was successful? 
- Takeaways: What did you learn from this situation? What would you do the same or differently next time?


Evaluation: 

- Knowledge of company values around communication: 
- Your principles for communication: 
- Awareness of problem: 
- Initiative: 
- Lessons Learned: 


## "Tell me about a time you made a mistake at work." 



- Situation: Where/when did this happen? What were you working on, and with whom?
- Task: What were you responsible for? What challenges did you face? 
- Action: How did you decide the appropriate response to the situation? How did you implement it?
- Result: How do you know your solution was successful? 
- Takeaways: What did you learn from this situation? What would you do the same or differently next time?


Evaluation: 

- Your principles for mistakes: 
- Knowledge of company values around rapid iteration, learning through failure, and troubleshooting
- Judgment: How did you explain the mistake (misunderstanding, incomplete information, technical errors)? Was it due to carelessness. 
- Responsibility: Did you make excuses, or make solutions? 
- Wisdom: Are you likely to avoid a similar mistake in the future? Can lessons here apply more broadly? 

## "Talk about a situation that required you to explain your technical work to someone outside your industry or field."



Evaluation: 

- Ability to communicate with stakeholders: 
- Impact: Understanding of what the other party needed to know
- Adaptability: Techniques (analogy, visualization, ...) used to simply communicate ideas. How did you decide what to include. Did you clarify that the explanation made sense. 
- Attitude: Will you patiently explain your ideas? 


## "Talk about a time you were asked to manage an important project." 

Evaluation: 

- Knowledge of company values on leadership
- Trust: Why were you chosen to lead?
- Respect and responsibility: 
- Delegation: 
- People management: 
- Executive decision-making: 
- Team dynamics: 
- Potential: Strengths and weaknesses in leading.
- Interest in management roles: 



# Deep-Dives

[Deep Dive guide](https://sites.google.com/insightdatascience.com/interviewstrategies/interviews/interview-logistics/deep-dives?authuser=0)


























