---
title: "Behavioral Interviewing"
author: "Mac Strelioff"
date: "`r Sys.time()`"
math: true
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
menu:
  InsightStudying:
    parent: Foundations
    weight: 99
linktitle: Behavioral Interviewing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(entropy)

# set seed for reproducability
set.seed(10201991)
```

# Principles

- Interviews are conversations, not tests
- Use stories to establish a connection and demonstrate fit for a role and company


Tell me about yourself, your academic research, why data science?; 30 second elevator pitch vs. 2 minute phone screen/on-site

Why this company? what value can you add? Tell me about a time whenâ€¦ 

STAR chart (Situation, Task, Action, Result)

Mock interviews: Practice, practice, practice 

# Common Behavioral Questions

## "Tell me a little about your background."

This questions gives an opportunity to demonstrate claer communication, curiosity, enthusiasm, transferrable skills, and value adds. 

Template: 

- Problem: Concepts and context
- Curiosity: Open question
- Value: Why should people care
- Project Outline: Methods, results

PhD projects can include: 

### PredictIt 

- Problem: Wisdom of the crowds, like the social science equivalent of ensemble learners. However, many fallacies and biases in probabilistic reasoning hove been demonstrated in lab settings. 
- Context: Ryan and I.. I was like a statistician/behavioral economist, and he was a programmer. So I'd lead algorithm developemnt and he would code and deploy.
- Curiosity: Would prices in prediction markets be good estimators of real probabilities, since they incentivise and aggregate individual estimates, or would they show the same biases as individual probabilistic judgments, since they are aggregating over biased individuals? 
- Value: Finding systematic biases could result in profitable trading strategies.
- Project Outline: 

prediction markets are used kind of like filtering, where we aggregate over individual opinions to get a more accurate concensus opinion. But, individual opinions have systematic biases, so would the aggregation of these also have biases? To answer this, I partnered with a large prediction market provider and found evidence of mispricings consistent with individual level biases -- miscalibration and also subadativity (superaditivity)?... 

Evaluation:

- Communication: Clearly explain prediction market mechanics, wisdom of the crowds, and an individual level fallacy. 
- Transferrable skills: Mention some of the models used. Interest in markets and complex problems. 
- Value adds: Realized prediction markets are biased close to expiration -- their signal is weaker than previously thought. Also able to profit literally by rolling out trading algorithms. 


### Child bandits: 

Bandit tasks are a common paradigm for studying individual level decision making and making claims about the way people make decisions. But in lots of contexts, the results you get when you addregate over trials can be different from the actual result on any trial. So I developed a model to infer on a trial by trial level, what decision making strategy kids were using to choose between two 'bandits' that gave out stickers. -- I have an interest in using more customized models and Bayesian inference to overcome issues that arise from aggregating data at too high a level. If you had averaged over trials, then there may have been evidence for one or another model, but the dynamic changes in decision making strategies would have gone unnoticed. 

- Similar approaches could be useful for dynamic experimental designs, OR in the context of Netflix, for personalization. If we could dynamically infer something like the strategy or emotional state of a user, then we could use that inference to personalize our recommended content or things like cover art or other aesthetic aspects of Netflix. 

### RL

- RL. 

### sff


## "Tell me about the project you worked on at Insight."




Generally, the answer can show clear communication of a problem, an approach, challenges, solutions, and relevance to the company or role.

At Insight I worked on a consulting project for an external company. This company's user funnel was something like -- visitors come to their website, are offered a trial, and some portion of trialers become subscribers. Problem was, around 80% of trialers weren't becoming subscribers. 


## "Why are you interested in a career as a data scientist or data engineer?"

## "Tell me about your research/talk to me about your last role."

## "Why are you interested in joining this company?"



























