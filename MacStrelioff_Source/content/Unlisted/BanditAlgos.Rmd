---
title: "Bandit Algos for Estimation, Hypothesis Testing, and Decision Making"
author: "Mac Strelioff"
date: "3/20/2019"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(entropy)

# set seed for reproducability
set.seed(10201991)
```

# TODO:
1. redo in Python, make an agent that uses each strategy as a method. Build the agent throughout the script. 


# Bandit Algorithms

Much of this is from [this blog](https://sudeepraja.github.io/Bandits/).

Here I evaluate different algorithms for bandit problems similar to those anticipated in industry or testing settings. Criteria of consideration are; 


## Problem Formalization

Bandit tasks can be cast as a Markov Decision Process. 

$$
\begin{aligned}
t &\in \{1,...,T\} \\
a_t &\in \mathcal{A} \\
s_t &\in \mathcal{S} \\
r_t &= R(s_{t+1}|a_t,s_t) = v(s_{t+1}|a_t,s_t) \\
T(s_{t+1}|a_t,s_t) &= p(s_{t+1}|a_t,s_t) \\
\pi(s_t)&=p(a_t|s_t)
\end{aligned}
$$

$\pi$ is referred to as a policy or decision rule. Mathematically, it is a probability distribution over actions -- a funciton that maps from states to actions. 

The decision maker's goal here is to learn a policy ($\pi$) that is optimal for some criteria. A variety of possible criteria are discussed and evaluated below. This objective is considered when chooseing the value function $v(s_{t+1}|a_t,s_t)$. 

Objective is to maximize 
$$
E\left(\sum_t r_t\right)=\sum_tE(r_t)
$$

In a typical testing scenario, the policy replaces the assignment mechanism. Hence, I treat assignment mechanisms as policies here.


## Use Cases and Evaluation Criteria

Bandit problems arise across many theoretical and applied fields. Everything from industry A/B testing, medical clinical trials, experimental lab studies, and toy problems for reinforcement learning algorithms can be cast as a bandit task. These different domains generally have different goals. A/B tests are conducted to find evidence for an advantage of one version of a product over another while emphasizing classical statistical objectives like minimizing type I error rates or false discovery rates. The goal of clinical experiments is to quickly discover the best treatment so that patient lives can be improved or saved. In simulation settings, bandit problems have been used to benchmark a variety of algorithms in terms of regret. Here I conduct similar benchmarks, while also evaluating standard 'best proctices' in terms of type I error rates and false discovery rates. 


\begin{enumerate}
\item Type I Error rates
\item False discovery rates
\item Reward value estimation
\item Expected regret
\end{enumerate}


### Type I Error

Among situations where there is no difference, how often is a difference detected?

Type I error occurs when a null hypothesis is reongly rejected -- so when a difference in outcomes of arms is detected when none actually exists. 

### False Discovery Rate

Among detected differences, how many are real? 

A false discovery occurs when an arm is selected but is not the actual optimal arm.

I'll consider this on a trial-by-trial level as well as the result of the overall experiment.

### Regret
Regret is the difference between the reward that would have been obtained had the optimal action been chosen, and the reward that was actually obtained from the chosen action. 

$$
\begin{aligned}
E(Regret) &= \sum_t E(r^*_{t}-r_t)
\end{aligned}
$$


## Uniform Policy 

This corresponds to a simple random sample type of assignment mechanism -- the gold standard for causal inference from experimental data. 

$$
\begin{aligned}
\pi(s_t) &= \frac{1}{|\mathcal{A}|} \\
E(Regret) &= TE(r^*_t) - \sum_t E(r_t|\pi) \\
&= T(E(r^*_t) - E(r_t)) \\
\end{aligned}
$$

The expected regret on each trial is the difference between the maximal reward and mean reward across actions. 


## Greedy Policies

### $\epsilon$-Greedy Policy

### Softmax Policy

Might be good for parameter estimation while also reducing regret.


```{r bandits:softmax}
na=5      # Number of arms
as=1:na   # action IDs
beta = 5
R = runif(na) # arm probabilities
#R[1]=1; R[2]=.5; R[3]=0
T = 1500   # Number of trials
q=rep(0,na) 

pis = c()
qs = c()
ats = c()
rts = c()
regret = c()
regrett= 0;

for(ti in 1:T){
# select action
pi = exp(beta*q)/sum(exp(beta*q))
at = rmultinom(n=1,size=as,prob=pi)
at = which(at==1)
# observe outcome  
rt = rbinom(1,size=1,prob=R[at])
# save stats
pis = rbind(pis,pi)
ats = c(ats,at)
rts = c(rts,rt)
# update action values
q[at] = q[at] + .2 * (rt - q[at])
qs = rbind(qs,q)
# regret
regrett= regrett+max(R)-R[at]
regret = c(regret,regrett)
}
```


```{r}

plot(regret,type="l")

cols = rainbow(na,s=1,v=.7)
for(ai in as){
if( ai>1){par(new=TRUE)}
plot(qs[,ai],type="l",ylim=c(0,1),col=cols[ai],
     main="Estimation",
     ylab="Q-value",
     xlab="Trial")
abline(h=R[ai],col=cols[ai],lty=2)
}

for(ai in as){
if( ai>1){par(new=TRUE)}
plot(pis[,ai],type="l",ylim=c(0,1),col=cols[ai],
     main="Action Probabilities",
     ylab="Policy",
     xlab="Trial")
#abline(h=R[ai],col=cols[ai],lty=2)
}

ent = 0
ps=c()
for(ai in as){
p = sum(ats==ai)/length(ats)
ent = ent + p*(-log2(p))
ps=c(ps,p)
}
c(ent,ps)
c(entropy.empirical(table(ats),unit="log2"),freqs.empirical(table(ats)))
```



## Upper Confidence Bound (UCB) Policy



## Thompson Sampling Policy

```{r bandits:thompson}
#na=5      # Number of arms
#as=1:na   # action IDs
#beta = 5
# keeps same as those above
# R = runif(na) # arm probabilities
#T = 5000   # Number of trials
#priors: rows: actions, col1:alpha, col2:beta
prior = matrix(1,nrow=na,ncol=2)

aposts=c()
bposts=c()
thompsonSamples = c()
ats = c()
rts = c()
regret = c()
regrett= 0;

for(ti in 1:T){
# select action
thompsonSample=c()
for(ai in as){
thompsonSample = c(thompsonSample,rbeta(1,prior[ai,1],prior[ai,2]))
}
at = which.max(thompsonSample)
# observe outcome
rt = rbinom(1,size=1,prob=R[at])
# save stats
thompsonSamples = rbind(thompsonSamples,thompsonSample)
ats = c(ats,at)
rts = c(rts,rt)
# update beta distributions
prior[at,1]=prior[at,1]+rt
prior[at,2]=prior[at,2]+(1-rt)
# save posterior parameters
aposts=rbind(aposts,prior[,1])
bposts=rbind(bposts,prior[,2])
# regret
regrett= regrett+max(R)-R[at]
regret = c(regret,regrett)
}
```



```{r}
plot(regret,type="l",
     main="Regret = max E(reward) -  E(reward|choice)",
     ylab="Cumulative Regret",
     xlab="Trial")

# something might be wrong with rbeta(x,vec1,vec2)

cols = rainbow(na,s=1,v=.7)
for(ai in as){
if( ai>1){par(new=TRUE)}
plot(aposts[,ai]/(aposts[,ai]+bposts[,ai]),type="l",ylim=c(0,1),col=cols[ai],
     main="Reward Probability Estimation",
     ylab="Posterior Means",
     xlab="Trial")
abline(h=R[ai],col=cols[ai],lty=2)
}
legend(1100,.8,c("Estimate","True"),lty=c(1,2))

for(ai in as){
if( ai>1){par(new=TRUE)}
plot(thompsonSamples[,ai],type="l",ylim=c(0,1),col=cols[ai],
     main="Posterior Samples",
     #ylab=expression('Sampled Value -- Policy=argmax'['a']*'(sample'['a']*')'),
     ylab=expression('Sampled Value'),
     xlab="Trial")
#abline(h=R[ai],col=cols[ai],lty=2)
}

ent = 0
ps=c()
for(ai in as){
p = sum(ats==ai)/length(ats)
ent = ent + p*(-log2(p))
ps=c(ps,p)
}
c(ent,ps)
c(entropy.empirical(table(ats),unit="log2"),freqs.empirical(table(ats)))
```



# Policy Comparisons











































