---
title: "Hypothesis Testing as Classifier Evaluation"
author: "Mac Strelioff"
date: "5/13/2019"
output: html_document
math: true
---



<div id="overview" class="section level1">
<h1>Overview</h1>
<p>Here I breed ideas from hypothesis testing, with those from the machine learning community on evaluating classifiers.</p>
</div>
<div id="hypothesis-testing" class="section level1">
<h1>Hypothesis Testing</h1>
<p>Hypothesis testing is based on a notion of accepting or rejecting a hypothesis based on data from an experiment.</p>
<p>accept, reject</p>
<p>Type I error –</p>
<p>Type II error –</p>
<p><span class="math inline">\(\alpha\)</span> <span class="math inline">\(\beta\)</span> power</p>
<p>base rates</p>
<p>false discovery rate</p>
</div>
<div id="classification-evaluation" class="section level1">
<h1>Classification Evaluation</h1>
<p>Calssification evaluation is based on selecting a model based on performance in out-of-sample performance. Many performance measures, that I’ll cover throughout, as they relate to concepts from hypothesis testing.</p>
<p>positive (true), negative (false) true positive, true negative, false positive, false negative, …</p>
<p>false discovery rates probably related to one of – accuracy, precision, recall, sensitivity, specificity</p>
<p>class imbalance (base rates)</p>
<p>F1 score</p>
</div>
<div id="hybrid-ideas" class="section level1">
<h1>Hybrid Ideas</h1>
<p>Relate <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and false discovery rate to accuracy, precision, recall.</p>
<p>sensitivity and specificity</p>
<p>Signal detection – hit, miss, false alarm?</p>
<p>Resources with more detail on each metric; <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">Most ML metrics</a></p>
</div>
