---
title: "Probability and Information"
author: "Mac Strelioff"
date: "5/13/2019"
output: html_document
math: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability Theory and Information Theory

## Types of Probabilities and Their Computations

motivation? Many important variables (future income, GDP, who I'll marry, whether I'll get sick, what disease I have given some symptoms, ...)

## Randomness, Event Spaces, and Probability
 motivating example..
Imagine that a vaccine against a dangerous disease recently becomes available. However, there is a small risk that the vaccine will produce a condition just as bad as the disease itself. You do not know whether you will get the disease, or whether, given the vaccine, you will contract the side effect. Should you get the vaccine or abstain and live with a higher risk of disease?

 Knowledge of random variables, and related concepts, can help in this and many other situations. Specifically; 
\begin{itemize}
\item A random variable random variable is a variable with an unknown value, but known possible values. For example, your state of health.
\item An event is an observed value of a random variable. Feeling healthy right now is an event.
\item An event space contains all possible values of the random variable. In the example, the event space includes healthy, diseased, and affected by the side effect of the vaccine.
\item Probabilities are values assigned to the events in an event space (i.e. the possible values of a random variable) and represent how likely each event is relative to other events in the event space.
\end{itemize}

%Knowing the probabilities of each event in the event space under conditions where you take or abstain from the vaccine could determine your decision. 

A random variable random variable is a variable with an unknown value, but known possible values. An event is an observed value of a random variable. An event space contains all possible values of the random variable. Probabilities are values assigned to the events in an event space (i.e. the possible values of a random variable) and represent how likely each event is relative to other events in the event space.

Here $p(e)$ will be used to represent the probability of an event. Some special probabilities include the probability of any event in the event space, which is 1. And the probability of any event other than $p(e)$, known as the compliment of $p(e)$, denoted with $p(\neg e)$, is found by; 
\begin{equation}
p(\neg e) = 1 - p(e)
\end{equation}

For example, the value observed when a six sided die is rolled could be considered a random variable with possible values of 1 through 6. Rolling the die could be an experiment, and, if a 5 lands face up, then 5 would be the outcome. 

Probabilities are often thought of with reference to experiments and outcomes. In this context, an experiment is any opportunity to observe a relevant outcome. Outcomes are the consequences of an experiment, and one is typically interested in outcomes of a particular kind. For example, if coin flips are thought of as an experiment, then the outcomes of this experiment could be heads or tails. In this context, probabilities assign numbers to the outcomes (e.g. heads, or tails) of an experiment (e.g. coin toss). 

see https://en.wikipedia.org/wiki/Probability_interpretations

define event
define event space as all the events that could happen
probability = events of interest / all possible events

## Joint Events and the Multiplication Rule
A joint event refers to two or more events occurring together. They are colloquially talked about as one event 'and' another event occurring together. More formally, joint events are called intersections (represented with the $\cap$ symbol) between events. For events $e_1$ and $e_2$, the probability of their joint event will be represented with $p(e_1 \cap e_2)$. The probability of an intersection of events is the same regardless of which event is considered first; 

\begin{equation} \label{joint reflexivity}
p(e_1 \cap e_2) = p(e_2 \cap e_1)
\end{equation}

\noindent For two events, $e_1$ and $e_2$, their intersection is found by;
\begin{equation} \label{joint probability definition}
p(e_1 \& e_2) = p(e_1\cap e_2) = p(e_1|e_2)p(e_2)
\end{equation}

where $p(e_1|e_2)$ is a conditional probability, discussed in the next section. 

## Conditional Events and Bayes' Rule

Conditional events refer to one event, $e_1$, after another event, $e_2$, is known. $p(e_1|e_2)$ represents the probability of $e_1$ given, or after knowing, $e_2$. These can be defined by rearranging the multiplication rule (equation \ref{joint probability definition}) as follows; 

\begin{equation}
p(e_1|e_2)p(e_2)=p(e_1\cap e_2) \rightarrow p(e_1|e_2)= \frac{p(e_1\cap e_2)}{p(e_2)}
\end{equation}
Noting that, by the reflexively of joint events (equation \ref{joint reflexivity}) and the definition of the multiplication rule (equation \ref{joint probability definition}), $p(e_1 \cap e_2) = p(e_2 \cap e_1)= p(e_2 | e_1) p(e_1)$, and so the above equation becomes; 
\begin{equation} \label{Bayes Theorem}
p(e_1|e_2)=\frac{p(e_2 | e_1) p(e_1)}{p(e_2)}
\end{equation}

Equation \ref{Bayes Theorem} is known as Bayes Theorem.


## Unions of Events and the Addition Rule

A union (represented with the $\cup$ symbol) of events refers to at least one of multiple events occurring. For events, $e_1$ and $e_2$, their union would include the probability that $e_1$ occurs, the probability that or $e_2$ occurs, and the probability that both $e_1$ and $e_2$ occur. Colloquially this is talked about the probability of $e_1$ 'or' $e_2$. Formally this is expressed and computed as; 

\begin{equation} \label{addition rule}
p(e_1\text{ or } e_2) = p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1 \cap e_2)
\end{equation}

\noindent Where $p(e_1 \cap e_2)$ is a joint probability, defined in Equation \ref{joint probability definition}.

## Independent Events

Independence is a common assumption in many statistical techniques. Statisticians assume independence primarily because it simplifies the computation of certain probabilities. Events are said to be independent if knowing one event does not change the probability of the other event. Formally, if events $e_1$ and $e_2$ are independent, this would mean that; 
\begin{equation} \label{independent definition}
p(e_1|e_2) = p(e_1) \text{ and } p(e_2|e_1) = p(e_2)
\end{equation}

\noindent If $e_1$ and $e_2$ are independent, then their joint probability, defined in in Equation \ref{joint probability definition}, simplifies as so; 
\begin{equation} \label{product rule with independence}
p(e_1 \cap e_2) = p(e_1|e_2)p(e_2) = p(e_1)p(e_2)
\end{equation}
Where the last equality is only true if $e_1$ and $e_2$ are independent (i.e. $p(e_1|e_2)=p(e_1)$). 

\noindent This then simplifies the addition rule in Equation \ref{addition rule} so that;
\begin{equation}
p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1|e_2)p(e_2) = p(e_1) + p(e_2) - p(e_1)p(e_2)
\end{equation}
Where again, the last part of this equality is only true if $e_1$ and $e_2$ are independent.

## Mutually Exclusive Events

Events $e_1$ and $e_2$ are said to be mutually exclusive if the occurrence of either event precludes the occurrence of the other event. Formally mutual exclusivity means that; 

\begin{equation} \label{mutually exclusive definition}
p(e_1 | e_2) = 0 \text{ and } p(e_2 | e_1) = 0
\end{equation}

\noindent If two events are mutually exclusive, then the probability of their joint event is; 
\begin{equation}
p(e_1 \cap e_2) = p(e_1|e_2)p(e_2) = 0*p(e_2) = 0
\end{equation}

\noindent And the probability of their union simplifies to;
\begin{equation}
p(e_1 \cup e_2) = p(e_1) + p(e_2) - p(e_1 \cap e_2) = p(e_1) + p(e_2) - 0 = p(e_1) + p(e_2)
\end{equation}

# Probability Functions and Parameter Estimation via Likelihoods

The general concepts of parameterized probability functions and likelihoods are introduced here. For specific distributions, see Chapter \ref{common distribution chapter}.

events can be thought of as values in spaces.. 
probability functions assign probabilities to data, assuming that parameters are known
likelihoods assign probabilities to parameters, assuming the data are known or observed

## Domain of a Function as an Event Space
In Section \ref{Types of probability}, probabilities were introduced with respect to events. If these events are a value, $x$, from a large set of possible values, $X$, then a probability function, denoted $f(x|\theta)$, can be used to assign probabilities to the values $x \in X$.

## PDFs or PMFs, and CDFs

Probability functions are referred to as probability density functions (PDFs) if $x$ takes continuous values, and probability mass functions (PMFs) if $x$ takes discrete values. Regardless of the type of variable $x$ is, a probability function that assignes probabilities to values $x$ that depend on a known parameter or parameter vector, $\theta$, is denoted $f(x|\theta)$. The cumulative distribution function (CDF), denoted $F(x|\theta)$, represents the total probability assigned to values of $X$ less than a particular value $x$. The CDF is related to the PDF or PMF through integration from the left side. All of this can be summarized as;

\begin{equation}
p(X \leq x |\theta) = F(x|\theta) = \int_{-\infty}^x f(x|\theta) = \int_{-\infty}^x p(X=x|\theta)
\end{equation}
\noindent Note that integrals are replaced with summation when $x$ takes discrete values.

A technical difficulty when considering values in a continuous domain is that the probability of any single value is 0. To sidestep this issue, one can consider values within a tiny set of values around $x$, that is, $\{x:x\in x\pm \Delta\}$, for an arbitrarily small $\Delta$.

Instead of being based on observations, these assign probabilities to a range of values, $x$, in the domain of the function.

## Likelihood and Log Likelihood

The likelihood, denoted $\mathcal{L}(\theta|x)$, assigns probabilities to values of a parameter or parameter vector, $\theta$, when the events or data, $x$ are treated as known. When a number, $n$, of observations are made, and independence between the observations is assumed, the likelihood can be expressed as; 

\begin{equation}
\mathcal{L}(\theta | x_1,x_2,...,x_n) = p(\theta | x_1,x_2,...,x_n) = p(\theta|x_1) p(\theta|x_2) ... p(\theta|x_n) = \prod_{i=1}^n p(\theta|x_i)
\end{equation}
Likelihoods can be difficult to work with analytically if they are complicated functions, and numerically if they are very small numbers. Because of this, log likelihoods are often used in place of likelihoods. A log likelihood is defined as; 

\begin{equation}
\ell(\theta | x_1,x_2,...,x_n) = log(\mathcal{L}(\theta | x_1,x_2,...,x_n)) = \sum_{i=1}^n p(\theta | x_i)
\end{equation}

## Score Function and Fisher Information

Usually statisticians are interested in the most likely parameters, that is, the parameter values that maximize the likelihood. One way to find these parameters is to find the derivative of the log likelihood and solve for the parameter values that make this equal to zero. The score function, $U(\theta)$, is the derivative of the log likelihood. Specifically;

\begin{equation}
U(\theta) = \frac{d}{d\theta}\ell(\theta| x_1,x_2,...,x_n)
\end{equation}

Statisticians are also concerned with the uncertainty in parameter estimates. One statistic used to assess this is Fisher Information, $I(\theta)$, defined as the negative expectation of the second derivative of the log likelihood; 

\begin{equation}
I(\theta) = -E\bigg(\frac{d^2}{d^2\theta}\ell (\theta | x_1,x_2,...,x_n)\bigg) = \int_{-\infty}^{\infty} \frac{d^2}{d^2\theta}\ell (\theta | x_1,x_2,...,x_n) f(x|\theta) d\theta
\end{equation}

The inverse Fisher Information, $\frac{1}{I(\theta)}$, is the variance of the sampling distribution for parameter estimates obtained by maximizing the log likelihood. As a second derivative, Fisher Information also represents the peakedness of the likelihood around the maximum likelihoods estimate. A large $I(\theta)$ indicates a steeper peak around the maximum likelihood estimate, which is interpreted as a more informative sample.

# Information and Entropy
see https://en.wikipedia.org/wiki/Quantities_of_information
see https://en.wikipedia.org/wiki/Information_theory

## Surprise (Self Information)

For an event, $e$, surprise (also called information or self-information) is defined to be the function satisfying these properties; 

\begin{enumerate}
\item $I(e)=f(p(e))$: The information is only a function of the probability of $e$.
\item $p(e)=1 \Rightarrow I(e)=0$: There is no information in observing an event that is known with certainty.
\item $p(e) < 1 \Rightarrow I(e)>0$: Events with probability less than one have positive information.
\item $p(c)=p(a\cap b) = p(a)p(b) \Rightarrow I(c)=I(a\cap b)= I(a)+I(b)=I(p(a)p(b))$: For the joint of independent events, information maps from multiplication to addition.
\end{enumerate}

The function satisfying these properties is:

\begin{equation}\label{InformationDfn}
I(e)=-log(p(e))=log\left(\frac{1}{p(e)} \right)
\end{equation}

## Entropy

Entropy is the expected information;

\begin{equation}\label{EntropyDfn}
H(E)=E(I(E)) = E(-log(p(E)))  = \int_E p(e)I(e) = -\int_E p(e)log(p(e))
\end{equation}

entropy is the expected information

## Joint Entropy

## Conditional Entropy

## Relative Entropy (Divergence)

KL divergence

## Mutual Information
Mutual information quantifies the information obtained about one random variable through the observation of another. 
can be expressed as average divergance?
self information is a special case of mutual information, the mutual information of a variable and itself.
Given the definition of Information in Equation \ref{InformationDfn}, Mutual Information is defined as:

\begin{equation}\label{MutualInformationDfn}
I(E_1;E_2)=\int_{E_2} \int_{E_1} p(e_a,e_2)log\left(\frac{p(e_1,e_2)}{p(e_1)p(e_2)}  \right)
\end{equation}
