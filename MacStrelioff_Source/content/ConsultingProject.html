---
title: "Growing Smarter: Understanding User Acquisition"
author: "Mac Strelioff"
date: 
math: true
---



<div id="insight" class="section level1">
<h1>Insight</h1>
<p>Insight is a data science fellowship where Fellows work on a 2 week project. For my project, I consulted with Elastic Projects to understand user conversion on their product, <a href="https://www.abstract.com/">Abstract</a>. Abstract is a file sharing and versioning application, much like GitHub, with an emphasis on digital artwork.</p>
<p>Explain notion of user/org, contributor, viewer, and guest – this comes up later when justifying the per contributor features.</p>
<!--- borrow information from Abstract's website --->
</div>
<div id="business-value-and-project-goals" class="section level1">
<h1>Business Value and Project Goals</h1>
<p>The survival of any company hinges on it’s ability to acquire new users. One way that Elastic Projects acquires users is through a 30-day free trial of their product, <a href="https://www.abstract.com/">Abstract</a>. However, around 80% of free-trial users fail to convert to paying customers. This leaves much opportunity to increase their userbase.</p>
<div id="project-goals" class="section level2">
<h2>Project Goals</h2>
<p>As a consultant for Elastic Projects, I helped with two major goals focused on issuing nudges to users in order to increase the probability of conversion to a paid customer;</p>
<ol style="list-style-type: decimal">
<li>As early as possible, identify which users to nudge</li>
<li>Determine how to nude them.</li>
</ol>
</div>
<div id="original-beliefs" class="section level2">
<h2>Original Beliefs</h2>
<p>complex features were the source of Abstract’s value to users, and hence were the features driving user conversion.</p>
<!--- 
My work presented here helped Elastic Projects to identify ways to increase the proportion of users that subscribe for paid service by uncovering usage behavior most indicative of a propensity to convert. 
--->
<p>Question: Do product issues stop users from engaging with the app?</p>
</div>
</div>
<div id="thinking" class="section level1">
<h1>Thinking</h1>
<p>Payers and non-payers might differ in distributions of activity</p>
<p>Those who end up paying might differ in the distribution of these engagement metrics.</p>
<p>This lead to exploration that focused on these distributions</p>
<p>This lead to LDA and QDA as the first algorithms that I considered.</p>
</div>
<div id="data-sources-and-processing" class="section level1">
<h1>Data sources and processing</h1>
<!---(exclude clearbit)--->
<p>The original dataset consisted of user activity and product performance data for all users over a one year period. There were a large number of users included in the dataset. The dataset also included users that were not germain to the central problem of identifying which free-trial users to nudge. Since the primary goal was to identify free-trial users to nudge, I excluded any users who were never on a free trial during the data collection period. In exploratory analysis, I found a large number of users who never showed any activity. This prompted me to believe there are two types of trial users that do not convert – 1) those who make an account then never engage with the product, and 2) those who engage with the product, then decide to stop engagement. In conversation with Elastic Projects, it seemed that making a commit was a baseline for minimal engagement with the project, so, to focus the analysis on users who at least minimally engaged with the product, I also excluded users who never made a commit. Finally, some product features were not avilable during the full duration of data collection. Since there was a large sample size, the easiest way to make the analyses pertenant for all features was to exclude data collected before all product features were available.</p>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>The original user activity data was in terms of counts of an action (e.g. file edit) on each date. To make the users comparible to each other, I created a new variable for account age. Also, rather than working with daily counts, I computed cumulative sums which represented a user’s total usage of a product feature up to a particular age of their account. Finally, since multiple users (contributors) could be associated with an account, and since this is likely to be confounded with other measures of user activity because each contributor can take actions, I divided the cumulative sums by the number of contributors. Overall, the data features that I worked with were the total usage of each product feature per contributor up to a particular day of account age.</p>
<!---
feature engineering
account age
cumulative sums
sum per contributor
--->
<!---
* make a figure that shows a daily feature, and it's cumulative sum. 
--->
</div>
</div>
<div id="exploration" class="section level1">
<h1>Exploration</h1>
<p>Modeling</p>
<p>discriminant analyses, …</p>
</div>
<div id="modeling" class="section level1">
<h1>Modeling</h1>
<p>LDA, QDA, … .</p>
<p>regression trees, boosted trees, support vector classifier</p>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>My work provided evidence against the original intuition of complex features driving user conversion.</p>
<p>This lead to … at Elastic Projects.</p>
</div>
