---
title: "Growing Smarter: Understanding User Acquisition"
author: "Mac Strelioff"
date: 
math: true
---



<!---
NOTES: 
for online, add a backslish in image paths. 
For knitting an html, do not have this backslash.
--->
<div id="background" class="section level1">
<h1>Background</h1>
<p>As part of my fellowship at Insight Data Science, I worked on a 2 week consulting project with the company Elastic Projects. The overall goal was to understand user conversion on their product, <a href="https://www.abstract.com/">Abstract</a>, and identify users on a free trial who would convert to paying customers. Abstract is a file sharing and versioning application, much like GitHub, with an emphasis on digital artwork. Users can be individuals or organizations, and a user account can assign roles of ‘contributor’ or ‘viewer’ to individual users associated with the account. Contributors have privileges to upload and edit files, while viewers cannot.</p>
</div>
<div id="business-need" class="section level1">
<h1>Business Need</h1>
<p>The survival of any company hinges on it’s ability to acquire new users. One way that Elastic Projects acquires users is through a 30-day free trial of their product, <a href="https://www.abstract.com/">Abstract</a>. However, around 80% of free-trial users fail to convert to paying customers. This leaves much opportunity to increase their userbase.</p>
<div id="project-goals" class="section level2">
<h2>Project Goals</h2>
<p>As a consultant for Elastic Projects, I helped with two major goals focused on issuing nudges to users in order to increase user acquisition;</p>
<ol style="list-style-type: decimal">
<li>I used machine learning models to identify, as early as possible, which users were likely to convert.</li>
<li>I interpreted model metrics and performed descriptive analyses to determine which features to focus on when reaching out to users that were currently unlikely to convert.</li>
</ol>
</div>
<div id="original-beliefs" class="section level2">
<h2>Original Beliefs</h2>
<p>complex features were the source of Abstract’s value to users, and hence were the features driving user conversion.</p>
<!--- 
My work presented here helped Elastic Projects to identify ways to increase the proportion of users that subscribe for paid service by uncovering usage behavior most indicative of a propensity to convert. 
--->
<p>Question: Do product issues stop users from engaging with the app?</p>
<!---
# Thinking

Payers and non-payers might differ in distributions of activity

Those who end up paying might differ in the distribution of these engagement metrics.

This lead to exploration that focused on these distributions

This lead to LDA and QDA as the first algorithms that I considered.
--->
</div>
</div>
<div id="data-sources-and-processing" class="section level1">
<h1>Data sources and processing</h1>
<!---(exclude clearbit)--->
<p>The original dataset consisted of user activity and product performance data for all users over a one year period. There were a large number of users included in the dataset. The dataset also included users that were not germain to the central problem of identifying which free-trial users to nudge. Since the primary goal was to identify free-trial users to nudge, I excluded any users who were never on a free trial during the data collection period. In exploratory analysis, I found a large number of users who never showed any activity. This prompted me to believe there are two types of trial users that do not convert – 1) those who make an account then never engage with the product, and 2) those who engage with the product, then decide to stop engagement. In conversation with Elastic Projects, it seemed that making a commit was a baseline for minimal engagement with the project, so, to focus the analysis on users who at least minimally engaged with the product, I also excluded users who never made a commit. Finally, some product features were not avilable during the full duration of data collection. Since there was a large sample size, the easiest way to make the analyses pertenant for all features was to exclude data collected before all product features were available.</p>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>The original user activity data was in terms of counts of an action (e.g. file edit) on each date. To make the users comparible to each other, I created a new variable for account age. Also, rather than working with daily counts, I computed cumulative sums which represented a user’s total usage of a product feature up to a particular age of their account. Finally, since multiple users (contributors) could be associated with an account, and since this is likely to be confounded with other measures of user activity because each contributor can take actions, I divided the cumulative sums by the number of contributors. Overall, the data features that I used were the total usage of each product feature per contributor up to a particular day of account age.</p>
<p>I initially thought that the distribution of these features would differ across user who never converted, relative to those who did convert. Also, the time of conversion could be many months after after a trial ends, as it may take time for a user associated with an organization to gain approval to purchase a subscription, or it may take time for a large organization to negotiate a price with Elastic Projects. To focus on classifying users as potential paying users based on activity, and to mitigate the possible lags in becoming a paying user, I created a variable that indicated whether an individual ever ended up a paying customer. I then conceptualized the problem as a classification problem where, based on a user’s activity up to a particular day, I estimated the probability that the user would ever be a paying customer.</p>
<!---
feature engineering
account age
cumulative sums
sum per contributor
--->
<!---
* make a figure that shows a daily feature, and it's cumulative sum. 
--->
</div>
</div>
<div id="exploration" class="section level1">
<h1>Exploration</h1>
<p>I visualized the data using boxplots split by converters vs non converters, across days. And by plotting quantiles of feature usage across days, split by those users who converted versus those who did not.</p>
<div class="figure">
<img src="csum_files_per_contributor.png" alt="Distribution of cumulative files imported per contributor, split by those who ever paid (blue) and those that were on a free trial forever (red)" />
<p class="caption">Distribution of cumulative files imported per contributor, split by those who ever paid (blue) and those that were on a free trial forever (red)</p>
</div>
</div>
<div id="modeling" class="section level1">
<h1>Modeling</h1>
<div id="considerations" class="section level2">
<h2>Considerations</h2>
<p>My intuition about differing distributions is natrually expressed in linear discriminant, and quadratic discriminant models. However, these algorithms hinge on an assumption that the features are Gaussian distributed, which they clearly were not (values were strictly non-negative, and distributions were skewed – lots of values around 0 and some values far from the mean). Because of this violation, I also considered a support vector classifier with radial basis functions, and tree based algorithms – random forests, and gradient boosted decision trees.</p>
<p>Class imbalance was another issue, since around 80% of users did not convert. I used SMOTE, an upsampling algorithm, to address the class imbalance.</p>
</div>
<div id="initial-performance" class="section level2">
<h2>Initial Performance</h2>
<p>To get a baseline level of performance, I assess the algorithms with default hyperparameter values using sci-kit learn in Python. I focused on accuracy in a test set here, and the tree-based algorithms outperformed the others by approximately 10%. This outperformance is likely due to the non-standard distributions of the features.</p>
</div>
<div id="parameter-tuning" class="section level2">
<h2>Parameter Tuning</h2>
<p>To assess performance across hyperparameters for the tree-based algorithms, I used grid searches with 5-fold cross validation. The performance of both tree based algorithms was mostly stable, except for poor performance for very low values.</p>
</div>
<div id="fitting-procedure" class="section level2">
<h2>Fitting procedure</h2>
<p>Since the algorithms performed comparably well across hyperparameter values, and because a proper grid search would have taken a long time to program, run, and evaluate, I did tuned parameters from marginal grid searches, then picked something that performed comparibly well.</p>
<p>5-fold cross validation based on features that performed well in the tuning above, with SMOTE performed within each fold.</p>
</div>
</div>
<div id="actionable-results" class="section level1">
<h1>Actionable Results</h1>
<p>To get actionable results, I applied the random forests algorithm using all data. This could help identify who to nudge. The erros that this would make are in the bottom left cell – users who intend to convert to paid users, that we identify as users who would not convert. The effect of this would be targeting onboarding resources toward these users, which may further increase their engagement.</p>
<ul>
<li>top left: Users who are on a free trial, that are classified as users unlikely to convert. Can target onboarding efforts toward them.</li>
<li>top right: Users on free trial, classified as users likely to convert – can target salies efforts toward them.</li>
<li>bottom left: Users who end up paying that are classified as users unlikely to pay – onboarding resources would be targeted toward them, which has minimal detremental effect, and could further increase their product engagement and hence their likelihood of converting sooner or staying with the product for longer.</li>
<li>bottom right: Users who end up paying, that the model correctly classifies ad potential paid users – Either sales efforts would be allocated to them, if not currently paid users, or these users would be left alone / not annoyed with onboarding resources or sales efforts.</li>
</ul>
<div class="figure">
<img src="confusion_matrix.png" alt="confusion matrix based on all data, for identifying who to nudge." />
<p class="caption">confusion matrix based on all data, for identifying who to nudge.</p>
</div>
<p>Important features from the cross validation procedure described in the previous section point to features to focus on when considering how to nudge users.</p>
<div class="figure">
<img src="feature_importance.png" alt="Mean (bar length) and standard deviation (black error bars) of feature importance evaluated across the 5 folds." />
<p class="caption">Mean (bar length) and standard deviation (black error bars) of feature importance evaluated across the 5 folds.</p>
</div>
<p>To understand the form of the relationship between these features and the proportion of users who converted, I binned users based on product feature usage and plotted the proportion of users across these bins.</p>
<ul>
<li>Focus on the important features. For users classified as unlikely to convert, identify which important features they are underutilizing, and target onboarding resources for those features.</li>
</ul>
<p>For example, if their projects per user is below 1, we could work with the organization to train contributors in project creation.</p>
<div class="figure">
<img src="projects_per_contributor.png" alt="Proportion of users that convert as a function of projects per contributor. There seems to be a bump in conversion rates from around 20% to around 40% above around 1 project per contributor." />
<p class="caption">Proportion of users that convert as a function of projects per contributor. There seems to be a bump in conversion rates from around 20% to around 40% above around 1 project per contributor.</p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<p>I began with an intuition that users who convert would have a different distribution of aggregate product feature usage, relative to those who did not convert, when feature usage was assessed at cross sections based on account age. To explore this, I looked at the quantiles of distributions across feature usage. Focusing on this with formal models, I found that tree-based algorithms could achieve a high accuracy in identifying the users that eventually converted, based on daily snapshots of aggregate feature usage.</p>
<p>To derive insights from this data, I focused on confusion matracies, which identified free users that acted like paid users (good targets for sales teams), and users who were currently unlikely to convert (good targets for onboarding or support teams). I focused on the features found to be important by the tree based algorithms in order to identify specific features to target when reaching out to users that were unlikely to convert. To discover how to nudge on these features, I looked at the proportion of users that converted across levels of engagement with the important product features.</p>
<p>Overall, my work provided valuable tools for identifying users to target with sales and onboarding resources, and found that engagement with core features of the product were more important than engagement with the more compelx features.</p>
</div>
