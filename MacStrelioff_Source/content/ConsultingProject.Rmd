---
title: 'Growing Smarter: Understanding User Acquisition'
author: "Mac Strelioff"
date: "06/27/2019"
output:
  word_document: default
  html_document:
    df_print: paged
math: yes
---

<!---
NOTES: 
for online, add a backslish in image paths. 
For knitting an html, do not have this backslash.
--->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

As part of my fellowship at [Insight Data Science](https://blog.insightdatascience.com/), I worked on a 2 week consulting project with an external company. Their product was a SaaS application enabling team collaboration on shared files, and has a 30-day free trial model. Users can be individuals or teams, and can have different roles that enable different levels of privilege in the service. The overall goal was to identify users during the free trial who would convert to paying customers.

# Business Need

The survival of any company hinges on itâ€™s ability to acquire new users. However, the majority of free-trial teams for my client fail to convert to paying customers. This leaves much opportunity to increase their userbase.

## Project Goals

As a consultant for my client, I helped with two major goals focused on issuing nudges to users in order to increase user acquisition;

1. Identify, as early as possible, patterns in user behavior that indicate whether a user is likely to become a customer after the trial.
2. Leverage those patterns to identify outreach strategies for users that were currently unlikely to convert.

# Data sources and processing
<!---(exclude clearbit)--->

The original dataset consisted of user activity and product performance data for all accounts over a one year period.
The dataset contained a large number of users, including some that were not germain to the project goals. 
Since the primary goal was to identify free-trial users to nudge, I excluded any users who were never on a free trial during the data collection period. 
In exploratory analysis, I found a large number of users who showed little to no activity. 
This prompted me to believe there are two types of trial users that do not convert -- 1) users who make an account then never engage with the product, and 2) users who engage with the product, fail to find the product valuable, and then decide to stop engagement. Since the strategies to address these users might differ, I decided to exclude anyone without a minimal degree of engagement with the product and focus on the $2^{nd}$ set.
The criteria for a minimal level of engagement was chosen based on dependencies between product features, and decided during discussion with the client.
Finally, some product features were not available during the full duration of data collection, as the product evolved over the course of the data collection. Since there was a large sample size, the easiest way to make the analyses pertinent for all features was to exclude data collected before all product features were available. 

## Feature Engineering and Exploration

The original user activity data, at an account level, was in terms of counts of an action (e.g. file edits) on each date. To make the users easily comparible to each other across dates, I created a new variable for account age (days since the account was created). Also, rather than working with daily counts, I computed cumulative sums which represented an account's total usage of a product feature up to a particular age of their account. Finally, since multiple users could be associated with an account, and since this is likely to be confounded with other measures of user activity because the counts were aggregated across users, I divided the cumulative sums by the number of users. After these changes, the features that I used were the total usage of each product feature per user up to a particular day since the creation of the account. 

The time of conversion could be many months after after a trial had ended, as it may take time for a user associated with an organization to gain approval to purchase a subscription, or it may take time for a large organization to negotiate a price with the client. To focus on classifying users as *potential* paying users based on activity, I created a variable that indicated whether an account ever ended up as a paying customer. I then conceptualized the problem as a classification problem where, based on aaccount activity up to a particular day, I estimated the probability that the account would ever be a paying customer. 

<!---
# Exploration
--->

I initially thought that the accounts that converted would differ in terms of the distribution of these features (cumulative activity per user), relative to those that did not convert. Hence, in exploration, I focused on probing this intuition by plotting the median and $20^{th}$ to $80^{th}$ quantiles of cross sections of these features across days since the account was created, split by users who ended up converting versus those who did not. Ideally, there would be features that the converters clearly used more than those who did not convert. 

![Distribution of cumulative files imported per user, split by those who ever paid (blue) and those that were on a free trial forever (red)](csum_files_per_contributor.png)


# Modeling

## Considerations 

My intuition about differing distributions is natrually expressed in [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) or [quadratic discriminant analysis](https://en.wikipedia.org/wiki/Quadratic_classifier). However, these algorithms hinge on an assumption that the features are Gaussian distributed, which they clearly were not -- values were strictly non-negative, and distributions were skewed such that there were lots of values around 0 and some values far from the mean. Because of this violation, I also considered a [support vector classifier](https://en.wikipedia.org/wiki/Support-vector_machine) with radial basis functions, and tree based algorithms -- [random forests](https://en.wikipedia.org/wiki/Random_forest), and [gradient boosting decision trees](https://en.wikipedia.org/wiki/Gradient_boosting) (GBT). An advantage of the tree-based approaches is their robustness to any distribution of the features, and any functional relationship between the features and the probability of a user converting.

Another issue was class imbalance, since the majority of users did not convert. I used [SMOTE](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis), an algorithm that generates synthetic data from the underrepresented class (paid users), to address the class imbalance when fitting the model in training sets. I also considered metrics beyond accuracy, such as [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and the [F1 score](https://en.wikipedia.org/wiki/F1_score), when selecting a final model.

## Initial Performance

To quickly hone in on a model, I assessed the algorithms mentioned above with default hyperparameter values using [scikit learn](https://scikit-learn.org/stable/index.html) in Python. I focused on classification accuracy in a test set, and the tree-based algorithms outperformed the others by approximately 10\%. This outperformance is likely due to the non-standard distributions of the features.

## Hyperparameter Tuning

To assess performance across hyperparameters for the tree-based algorithms, I created [validation curves](https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html) with 5-fold cross validation and a set of hyperparameter values. The performance of both tree based algorithms was mostly stable in terms of [F1 scores](https://en.wikipedia.org/wiki/F1_score), except for poor performance when there were very few ($<10$) estimators. To mitigate potential overfitting, I increased the minimum number of observations in a leaf to 10, but left all other hyperparameters at their default values. 

## Fitting Procedure 

I fit the random forests and GBT algorithms to cross-sections of the data at 7, 14, and 30 days since account creation. For each cross section, I split the data into 5 folds, and used [SMOTE](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) when training the algorithms within each fold to account for the class imbalance. In each fold, I logged the feature importances and performance metrics from both algorithms.

# Actionable Results

<!---
The algorithms performed comparably well in terms of F1 scores. 
--->

I used the mean feature importances across folds from the GBT algorithm to identify the features that differentiated between successful users and users who may have needed more onboarding. Since these features seperate continuing users from those that did not continue engagement after the free-trial, these are the features to prioritize when considering interventions to add value to the user's experience.

![Mean (bar length) and standard deviation (black error bars) of feature importance evaluated across the 5 folds for the gradient boosting trees algorithm.](feature_importance.png)

To understand the form of the relationship between these features and continued user engagement, I binned users based on product feature usage and plotted the proportion of users who continue using the product after the free trial across these bins. Figures like this, paired with data on an individual user's activity, could help in personalizing user outreach to focus on aligning the user's feature usage with that of more successful users. For example, 
**based on the figure below, if an account has fewer than 1 workspace per user, their experience might be improved by resources that make workspace creation easier to understand or engage with. **

![Proportion of accounts that convert as a function of workspaces per user There seems to be a bump in conversion rates from around 20\% to around 40\% above around 1 workspace per user.](projects_per_contributor.png)

To identify users to contact, I applied the GBT algorithm to all data and focused on the confusion matrix, shown below. Different actions could be taken with respect to users in each quadrant. Users in the *bottom right quadrant* are currently customers, and were identified as such by the algorithm. These are the users who were successful in identifying value in the product and establishing an ongoing relationship with my client. Users in the *top right quadrant* are those who have not yet become customers, but are engaging with the product in ways that are similar to those who have become customers. These users have likely identified valuable aspects of the product, and could be contacted by a customer success team to help them find a subscription plan that suits their needs. 

Users on the *top left* and *bottom left quadrants* were identified as users that would not continue with the product after the free-trial -- **these are the users that may benefit from outreach** that demonstrates the value this product can add to their workflows. Users in the *top left quadrant*, were correctly identified as users that would not continue with the product. These users may have failed to identify valuable aspects of the product, and may have had a better experience if they had been contacted early by customer support or had access to other educational resources that might have helped them use the product. Users in the *bottom left quadrant* are those who became paying customers, but were incorrectly identified as users that would not continue with the product based on their activity in the first 7 days. Misclassifying these users has essentially no harm, as it would only encourage efforts to improve their experience early in their trial.

![confusion matrix based on all data, for identifying who to nudge.](confusion_matrix.png)

## Limitations

causation in terms of the feature recommendation.. Removing users who didn't satisfy a minimum level of engagement might help with this slightly, by leaving only those who were looking for this kind of product in the sample.. 


Or users might understand the product and just realize that it does not suit their needs -- in this scenario, no amount of clarity or onboarding would help to convert the user. This information could still be useful; for example, if an advertising channel is found to bring in particularly this type of user, then ad funds could be better spent on other channels. 


# Summary

I began with an intuition that users who convert would have a different distribution of aggregate product feature usage, relative to those who did not convert, when feature usage was assessed at cross sections based on account age. To explore this, I looked at the quantiles of distributions across feature usage. Focusing on this with formal models, I found that tree-based algorithms could achieve a high accuracy in identifying the users that eventually converted, based on daily snapshots of aggregate feature usage. 

To derive insights from this data, I focused on confusion matrices, which identified free users that acted like paid users (good targets for sales teams), and users who were currently unlikely to convert (good targets for educational resources, and/or onboarding or support teams). I focused on the features found to be important by the tree based algorithms in order to identify specific features to target when reaching out to users that were unlikely to convert. To discover how to nudge on these features, I looked at the proportion of users that converted across levels of engagement with the important product features. 

Overall, my work provided valuable tools for identifying users to connect with for sales or onboarding assistance, and found that engagement with core features of the product was generally more important than engagement with more compelx features. 

